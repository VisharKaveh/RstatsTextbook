[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for the Language Sciences",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Data Analysis for the Language Sciences",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis textbook is intended as a hands-on introduction to data management, statistics, and data visualisation for students and researchers in the language sciences. It relies exclusively on freely accessible, open-source tools, focusing primarily on the programming language and environment R.\nIt is often claimed that learning R is ‚Äúnot for everyone‚Äù, or that it has ‚Äúa steep learning curve‚Äù. This textbook aims to prove that the opposite is true. There are many reasons why it is worth investing the time and effort to learn how to do research in R, and it is no more difficult than learning any other new skill. In fact, the results of a recent study suggests that language aptitude is a much stronger predictor of programming aptitude than numeracy (i.e., ‚Äúbeing good at numbers‚Äù) (Prat et al. 2020). So if you have successfully learnt a foreign language in the past, there is no reason why you shouldn‚Äôt succeed in learning a programming language!\n\nLearning R is like learning a foreign language. If you enjoy learning languages, then ‚ÄòR‚Äô is just another one. [‚Ä¶] You have to learn vocabulary, grammar and syntax. Similar to learning a new language, programming languages also have steep learning curves and require quite some commitment. (Dauber 2024)\n\nThe rationale for this textbook is based on my personal observations, in both teaching and consulting, that many ‚Äòintroductory‚Äô textbooks to statistics and/or R are not suitable for many humanities scholars, who typically have little to no prior programming experience and for whom the word ‚Äústatistics‚Äù often evokes little more than unpleasant memories of school mathematics. It is worth stressing that is not a matter of generation (I have observed this phenomenon across all age groups), intelligence (I have taught people far more intelligent than me), or an innate inability to deal with numbers and/or computers (although these are beliefs that, sadly, some have deeply internalised). Instead, I am convinced that, for many people, it is simply a matter of finding a sturdy, first stepping stone and gathering up the courage to step on it to begin this learning journey.\nThe aim of this textbook is by no means to replace any of the brilliant, existing textbooks aimed at imparting statistical literacy for linguistics research, but rather to provide a stepping stone to be able to access these wonderful resources.1",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Data Analysis for the Language Sciences",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThe target audience for this book are students and researchers in the language sciences, including (applied) linguistics, (first and second) language teaching, and language education research. All examples are taken from these research areas. Ultimately, however, this textbook may be of use to anyone who feels they could benefit from a maximally accessible stepping stone, whichever discipline they are coming from.\nThis textbook is intended to be read linearly, chapter by chapter. Apart from the first introductory chapter, all other chapters will require several hours of commitment. They include quiz questions and short practical tasks. Completing these tasks is essential to genuinely assimilate the textbook‚Äôs contents. That‚Äôs because the best way to learn new skills is to try things out so, with this in mind, let‚Äôs get cracking!\n\n\n\n\n\n\nFigure¬†1: Artwork encouraging beginner R learners by @allison_horst.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Data Analysis for the Language Sciences",
    "section": "About the author",
    "text": "About the author\nI started learning about statistics and R in 2017 when I realised that it would be important for me to conduct the kind of quantitative analyses that I wanted to do as part of my PhD in applied linguistics/English language teaching (Le Foll 2022). I had no previous experience in either and there were no such courses at my university. Even though I mostly learnt by myself, it would be incorrect to say that I am self-taught: I learnt from some of the resources listed in Appendix A, attended bootcamps and summer schools, read countless posts on StackOverflow and various blogs, and exchanged with like-minded people on social media (#Rstats, #dataviz, #TidyTuesday). This why it is fairer to say that I am community-taught.\nI now like to describe myself as an ‚Äúadvanced beginner‚Äù in R and statistics. I am not a programmer, nor a statistician, but rather an applied linguist and committed educator. I enjoy teaching data literacy, statistics, and data visualisation to current and future generations of linguists, language education scholars, and teachers. I teach regular methods courses at the University of Cologne that are attended not just by M.A.¬†and M.Ed. students, but also by some doctoral and post-doctoral researcher colleagues. I also teach workshops for both doctoral and post-doctoral researchers at other institutions on a freelance basis.\nThis textbook was partly designed on the basis of materials that I have developed for these courses and workshops. Publishing these materials is my way to contribute to the wonderful community of people who have helped me on my leaRning journey. ü§ó\n\n\n\nMe back in 2017, proudly presenting at my first international conference.2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Analysis for the Language Sciences",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis textbook has benefited greatly from the generous, critical feedback I have received from both novice and expert users of R throughout this project. Many thanks to my colleagues from the Digital Research Academy, Nick Bearman, Ben Golub, and Fritjof Lammers for their critical peer review and to my (former) students at the University of Cologne, Jan Hollmann, Rose H√∂rsting, Vijaya Lakshmi, Paula Raabe, Poppy Siahaan, Veronika Strobl, Clara Stumm, Katja Wiesner, and Isabel Zimmer, for their critical learner feedback.\nSpecial thanks also go out to the researchers whose works are used as case studies in this textbook, Sarah Schimke and Ewa DƒÖbrowska, and to Allison Horst whose beautiful and witty artworks illustrate many of the chapters of this textbook (e.g., Figure¬†1).\nIn addition, I would like to thank everyone who has contributed and continues to contribute to my own data analysis learning journey. At the risk of forgetting someone, I would like to extend special thanks to Vaclav Brezina, Guillaume Desagulier, Stephanie Evert, Stefan Gries, Dani√´l Lakens, Natalia Levshina, Luke Tudge, the RLadies Stack group, the R package developers and maintainers of all the packages I use, as well as the many generous contributors to Stack Overflow and to #Rstats on social media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Data Analysis for the Language Sciences",
    "section": "Get in touch! üì©",
    "text": "Get in touch! üì©\nIf (parts of) this textbook helped you on your leaRning journey or for your teaching, do drop me a line to let me know!\nIf you have any suggestions for improvements, I would also love to hear from you. ‚úâÔ∏è\n\n\n\n\n\nDauber, Daniel. 2024. R for non-programmers: A guide for social scientists. Open Education Resource. https://bookdown.org/daniel_dauber_io/r4np_book/.\n\n\nLe Foll, Elen. 2022. Textbook English: A corpus-based analysis of the language of EFL textbooks used in secondary schools in France, Germany and Spain. Osnabr√ºck University PhD thesis. https://doi.org/10.48693/278.\n\n\nPrat, Chantel S., Tara M. Madhyastha, Malayka J. Mottarella & Chu-Hsuan Kuo. 2020. Relating natural language aptitude to individual differences in learning programming languages. Scientific Reports. Nature 10(1). 3817. https://doi.org/10.1038/s41598-020-60661-8.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Data Analysis for the Language Sciences",
    "section": "",
    "text": "A (work-in-progress) list of next-step resources can be found in Appendix A.‚Ü©Ô∏é\nI chose this picture because I vividly remember two professors pointing out that I had written ‚Äúp¬†=¬†0.00‚Äù on my poster (which I had copied-and-pasted from the output of the statistics tool that I had used) and laughing among themselves (but well within earshot) at how stupid that was. Learning these skills certainly requires a lot of effort on the part of the learner, but it also requires an academic culture that strives to include rather than exclude. This textbook explicitly aims for an inclusive approach to teaching the basics of data literacy and I have included this photo as a reminder to always persevere, whether in the face of seemingly insurmountable error message or snarky remarks!‚Ü©Ô∏é",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html",
    "href": "1_OpenScholarship.html",
    "title": "1¬† Open ScholaRship",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn about the relevance of Open Scholarship in learning how to manage, manipulate, analyse, and visualise research data. In doing so, the following aspects of Open Scholarship will be introduced:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Schola`R`ship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#sec-OpenScience",
    "href": "1_OpenScholarship.html#sec-OpenScience",
    "title": "1¬† Open ScholaRship",
    "section": "1.1 Open Science",
    "text": "1.1 Open Science\nOpen Science is a major component of Open Scholarship and the two terms are frequently used synonymously. Open Scholarship, however, is broader in that it includes all kinds of knowledge, whereas Open Science focuses on what is conventionally considered ‚Äúscientific knowledge‚Äù. Open Science covers many different aspects including:\n\n\n\n\n\n\n\n\nOpen materials\nGiving free, unrestricted, public access to research materials in a way that allows others to replicate the results of published studies and to conduct new studies based on these existing materials.\nMaterials may include questionnaire items, all kinds of experimental stimuli, annotation schemes, inclusion and exclusion criteria, etc. (see Task 2 in Section 2.4).\n\n\n\nOpen data\nGiving free, unrestricted, public access to scientific data, whenever ethically and legally possible (see Berez-Kroeker et al. 2022).\nIn Section 2.1, we will see that studies in the language sciences can involve many different types of data including texts, tables, images, and videos.\n\n\n\nOpen code\nMaking computer code freely and publicly available with appropriate documentation to make research methods and data analyses transparent.\nOpen code can include source code for custom software and packages, code for stimuli generation, data collection and processing, statistical analysis, and data visualisation. Sharing code allows for collaborations, while sharing both code and data allows others to reproduce published results.\n\n\n\nOpen access\nGiving free, unrestricted, public access to scientific outputs, foremost publications. Contrary to a frequent misunderstanding, authors or their institutions do not necessarily have to pay article processing fees (APC) to publish their work in open access. Publishing open access can instead involve uploading a pre-copyedit version of a publication on a public repository (see Section 2.4) or publishing in a not-for-profit open access publication outlet (see section on Open Access in The Turing Way Community 2022).\n\n\n\n\nSharing research data allows us to reproduce the analyses reported in research publications based on the authors‚Äô original data and to test whether different analysis methods would have led to different conclusions. Sharing research materials and code means that we can replicate studies to check the robustness of published results and/or their generalisability across different populations. For example, if a journal article reports on the effectiveness of a new language teaching method based on a study conducted at a British university, we can test whether the same effect can be observed when replicating the study at a Nigerian university or an Indonesian secondary school.\nOpen Science advocates argue that scientific knowledge ‚Äú[should], where appropriate, be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour‚Äù (Parsons et al. 2022). This corresponds to an ideal that, although probably impossible to fully achieve, is nonetheless worth striving for at all times.\n\nOpen science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. (Parsons et al. 2022)\n\nTo conduct open science, a sound understanding of data management and of effective data analysis workflows is crucial. This textbook aims to provide a gentle, practical introduction to these foundational skills using examples from the language sciences. Published as an Open Educational Resource (see Section 1.3), it showcases linguistics and Second Language Acquisition (SLA) publications that include open data, open code and/or materials and teaches data analysis using exclusively open-source software and programming languages (see Section 1.2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Schola`R`ship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#sec-OpenSource",
    "href": "1_OpenScholarship.html#sec-OpenSource",
    "title": "1¬† Open ScholaRship",
    "section": "1.2 Open Source",
    "text": "1.2 Open Source\nIn line with its aim to provide an accessible introduction to statistics and data visualisation, this textbook relies exclusively on open-source software and programming languages, foremost LibreOffice Calc, R and RStudio. Open source refers to software whose source code is available under a license that grants anyone the rights to study, modify, and distribute the software to anyone and for any purpose. If we think of a software application as a cake, the source code is like its recipe. It contains the list of ingredients and the steps to bake the cake. Open source means that the recipe is publicly available. You can access it, read it, and use it to bake the cake. You can also modify it to add your own twist, such as adding a new ingredient or making it vegan, and share it with others. In the context of software, this allows many people to collaborate, make improvements, and share their versions, resulting in better and more diverse software.\nUsing open-source software in this introductory textbook means that anyone2 can download, install and use the required software at no cost. However, it is very important to note that not all free software (‚Äòfreeware‚Äô) is open source.\n\n\n\n\n\n\nQuiz time!\n\n\n\nThis quiz encourages you to do some quick internet searches to find out more about open-source software.\n1) Which of these is an open-source alternative to Microsoft Word?\n\n\n\n\nGoogle Docs\n\n\nLibreOffice Writer\n\n\nPages\n\n\n\n\n\n\n\n¬†\n2) Which of these is an open-source alternative to Microsoft Powerpoint?\n\n\n\n\nGoogle Slides\n\n\nLibreOffice Impress\n\n\nKeynote\n\n\n\n\n\n\n\n¬†\n3) Not only can software be open source, programming languages can, too. In fact, most modern programming languages are open source. In this book, we will focus on the open-source programming language R. Which of these is not an open-source programming language?\n\n\n\n\nJavaScript\n\n\nMATLAB\n\n\nPython\n\n\n\n\n\n\n\n¬†\n4) There are also many open-source operating systems. Which of these is an open-source alternative to the operating system Windows?\n\n\n\n\niOS\n\n\nMacOS\n\n\nUbuntu\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\nTask 1\n\n\n\nYour first practical task is to download and install the open-source software suite LibreOffice. This is important as we will use its spreadsheet editor, LibreOffice Calc, in the following two chapters.\n\n\nLibreOffice is available for Windows, Mac and Linux. You can download it from here: https://www.libreoffice.org/download/download-libreoffice/.\nDetailed installation instructions can be found here: https://www.libreoffice.org/get-help/install-howto/.\nOn the official LibreOffice website you can choose either:\n\nthe latest version for ‚Äútechnology enthusiast, early adopter or power user‚Äù\nor the ‚Äúslightly older‚Äù but more tested version.\n\nIn drafting this textbook, I used the latest version which, at the time, was version 24.2.2. The one that you download will be higher than that as the developers regularly publish updates. If you already have LibreOffice installed on your computer, now is a good time to check that your version is up-to-date.\nDetailed documentation is available in many different languages: https://documentation.libreoffice.org/en/english-documentation/\n\n\n\n\n\n\n\n\n\nGoing further\n\n\n\n\n\nIn this introductory textbook, we have simplified things considerably. To be considered open source, software distributions actually have to comply with ten criteria. You can read up on them here:\n\nhttps://opensource.org/osd\n\nTo find out more about the benefits of open-source software in the context of research, I recommend reading:\n\nhttps://book.the-turing-way.org/reproducible-research/open/open-source",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Schola`R`ship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#sec-OpenEducation",
    "href": "1_OpenScholarship.html#sec-OpenEducation",
    "title": "1¬† Open ScholaRship",
    "section": "1.3 Open Education",
    "text": "1.3 Open Education\nOpen Education is a key component of Open Scholarship (see Chapter 1). Open Education aims to stimulate collaborative teaching and learning and to provide high-quality Open Educational Resources (OERs) that are accessible for all.\nAs illustrated in Figure¬†1.1, OERs are licensed in such a way that everyone has the right to engage in ‚Äú5 Rs‚Äù when using OERs. The 5 Rs of OERs are:\n\nRetain - the right to make, own, and control copies of the content (e.g., download, duplicate, and store copies of an OER).\nReuse - the right to use the content in a range of ways (e.g., as teaching materials on a course, as part of a website, or in a video).\nRevise ‚Äì the right to adapt, adjust, modify, or alter the content itself (e.g., translate the content into another language, create a version for a different programming language).\nRemix ‚Äì the right to combine the original or revised content with other open materials to create something new.\nRedistribute ‚Äì the right to share copies of the original content, any revisions, and remixes with others (e.g., give a copy of the content to a friend).\n\n\n\n\n\n\n\nFigure¬†1.1: OER sketch note by Yvonne Stry\n\n\n\nOERs may be published under different licenses and, in engaging in the 5 Rs, the exact terms of an OER‚Äôs license must be respected. For example, the web-based version of this textbook is published as an OER under the Creative Commons license CC BY-NC-SA. This means that anyone can engage in the 5 Rs with it (i.e., users are free to read and use, edit, remix, and expand upon the textbook) as long as:\n\nthe original author and source is mentioned (hence you should specify who this resource is BY),\nany derived version is not made into a commercial product (NC stands for non-commercial), and that\nany derived versions of this textbook (e.g., a translated version or a version adapted for history scholars) are also shared with this same license (SA stands for share alike).\n\nIn line with the principles of Open Education, all of the datasets used as case studies in this textbook have been published in open access. We will analyse real data from published research studies in the fields of applied linguistics and language education to learn about data management, statistics, and data visualisation.\n\n\n\n\n\n\nQuiz time!\n\n\n\n5) Is it possible to reuse Figure¬†1.1 on a company website?\n\n\n\n\nYes, but you must mention the name of the artist.\n\n\nYes, but you must mention the image's sell-by date.\n\n\nYes, but only if it is the website of a non-profit company.\n\n\nYes, but only after having obtained written consent from the artist.\n\n\n\n\n\n\n\n¬†\n6) Which of these resources can be published as OERs?\n\n\n\n\nLesson plans\n\n\nCourse assessments\n\n\nTextbooks\n\n\nMassive Open Online Courses\n\n\nCourse syllabi\n\n\nHomework tasks\n\n\nSerious games\n\n\nLecture slides\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\nGoing further\n\n\n\n\n\nThere are thousands of high-quality Open Educational Resources (OERs) out there, yet few people are aware of them. OER databases are good starting points to start exploring OERs, e.g.:\n\nhttps://oercommons.org/\nhttps://www.twillo.de/oer/web/\n\nAppendix A also includes a list of recommended next-step OERs on data management, data analysis in R, statistics, data visualisation, Open Science, and reproducibility.\n\n\n\n\n\n\n\nBerez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller & Lauren B. Collister. 2022. The open handbook of linguistic data management. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nThe Turing Way Community. 2022. The turing way: A handbook for reproducible, ethical and collaborative research (1.0.2). Zenodo. https://doi.org/10.5281/zenodo.3233853.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Schola`R`ship</span>"
    ]
  },
  {
    "objectID": "1_OpenScholarship.html#footnotes",
    "href": "1_OpenScholarship.html#footnotes",
    "title": "1¬† Open ScholaRship",
    "section": "",
    "text": "Empirical data is based on what is experienced or observed rather than on theory alone.‚Ü©Ô∏é\nProvided that they have access to the internet and a functioning personal computer.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Open Schola`R`ship</span>"
    ]
  },
  {
    "objectID": "2_Data.html",
    "href": "2_Data.html",
    "title": "2¬† Data files and formats",
    "section": "",
    "text": "Chapter overview\nThis chapter first considers what data means in the context of language research, before turning to how these data are formatted and stored. You will learn about:\nAlong the way, you will get insights into an eye-tracking study involving cute Playmobil figures and a meta-science investigation that highlights the utmost importance of data literacy for research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-DataLanguageSciences",
    "href": "2_Data.html#sec-DataLanguageSciences",
    "title": "2¬† Data files and formats",
    "section": "2.1 Data in the language sciences",
    "text": "2.1 Data in the language sciences\nIn this book, we are concerned with empirical research in the language sciences, in other words, with research that is based on the analysis of data. But what is data exactly? Data can be collected via surveys, measurements, or observations. To begin with, however, these collected datasets are ‚Äúraw‚Äù. Data only becomes information once we have analysed and interpreted the data in a meaningful way. Hence, just like uncooked pasta does not make a flavourful meal, we must learn to ‚Äúcook‚Äù the raw data to obtain meaningful information.\nWhat kind of data are analysed in the language sciences? To get a rough idea of the range of data types analysed in the language sciences, let us take a look at the IRIS database.\n\nIRIS is a collection of instruments, materials, stimuli, data, and data coding and analysis tools used for research into languages, including first, second, and beyond, and signed language learning, multilingualism, language education, language use, and language processing. Materials are freely accessible and searchable, easy to upload (for contributions) and download (for use). (2011)\n\nAs such, IRIS supports Open Science and Open Scholarship (see Chapter 1).\n\n\n\n\n\n\nTask 1\n\n\n\nIn this task and many future tasks, we will make use of the IRIS database.\n\nConnect to the IRIS website and navigate to its Search and Download page.\nScroll down to the filter option ‚ÄòData Type‚Äô.\nClick on ‚ÄòData Type‚Äô and browse through the different data types that are most commonly used in language-related research.\n\n\n\n\n\n\n\nFigure¬†2.1: Screenshot from the IRIS database search page (accessed on 17 April 2024)\n\n\n\na) For which kinds of studies could these different types of data have been collected? Think about both experimental and observational studies.\nb) Which of these data types is most likely to be measured in milliseconds (ms)?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-ResearchData",
    "href": "2_Data.html#sec-ResearchData",
    "title": "2¬† Data files and formats",
    "section": "2.2 Types of research data",
    "text": "2.2 Types of research data\nGiven the wide range of methods used in language research, it is no surprise that they are so many different types of research data. Although the data types listed on the IRIS search page (see Figure¬†2.1 for extract) are very broad and the categories not clearly defined, the list illustrates the breadth of research data types typically analysed in language studies.\nThe first data type category, ‚ÄúOral production‚Äù, for instance, can equally refer to text transcriptions of language users‚Äô oral production, audio, or video files. It can also refer to either raw data or to (more or less) processed data. For example, a transcript of a conversation could have been automatically annotated for part-of-speech, meaning that every word would be marked for their word class (e.g., This_DT is_VBZ not_RB raw_JJ text_NN data_NN ._PUNC), or it could have been manually anonymised by adding placeholders (e.g., Is &lt;NAME&gt; going out with &lt;NAME&gt;?) indicating that certain words have been retracted for data protection reasons.\nThe second most frequent data type category, ‚ÄúClosed response format‚Äù, includes different kinds of questionnaires and tests. Questionnaires may ask study participants to disclose personal information relevant to the research questions using single or multiple-choice questions, such as what language(s) they use at home, how long they have studied a language for, or how old they are. Tests may be designed to assess participants‚Äô language competences (e.g., in the form of a vocabulary or grammar test), as well as other aspects relevant to the research questions being investigated (e.g., short-term memory or baseline reaction times).\nIn this book we will focus on the research processes that take place after the data have been collected. However, it is vital that we are aware of the conditions and context in which the data we are analysing were collected and pre-processed. It is no exaggeration to say that these steps in the research process can entirely change the results of the data analysis. Suppose we decide to compare the abilities of two groups of French L2 learners. To do this, we administered a language production test to two whole classes of secondary school students learning French as a second language using two different teaching methods. If one group had 15 minutes to complete the test and the other had up to 60 minutes, the results would not be comparable.\n\n\n\n\n\n\nQuiz time!\n\n\n\n1) Which other reasons could potentially jeopardise the comparison of test results data from two different groups of pupils?\n\n\n\n\nOne group having French lessons on Tuesday mornings, the other on Friday afternoons.\n\n\nThe two groups having different teachers.\n\n\nOne group having a classroom decorated with French flags and posters about France.\n\n\nOne group having a single native speaker of French, whilst the other has none.\n\n\nOne group having a higher proportion of pupils from migrant families.\n\n\nOne group having a higher proportion of pupils with reading difficulties.\n\n\n\n\n\n\n\n\n\nWhilst there are many ways to ensure that as many factors as possible are controlled for, not all can be controlled for. What is crucial is that all aspects of the data collection process are well documented so that all factors, whether controlled or not, can be taken into account when analysing the data.\nIn research, we usually distinguish between primary data, which is the data that you collected yourself, and secondary data, which is data that was collected by others. Hence if you were to carry out a new study based on data that you found on IRIS, you would be conducting a secondary data analysis. Especially when conducting secondary data analyses, it is crucial that we have enough information about the data itself, i.e.¬†metadata. Metadata is crucial for finding, sharing, evaluating, and reusing datasets. Metadata can be generated automatically and stored within the data file. For example, unless this metadata was explicitly deleted or amended, Microsoft Word files typically contain metadata describing who created the file, when it was first created, and when the file was last modified. For some data and projects, it also makes sense to create separate metadata files that contain additional or more detailed information about the collected data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-FileExtensions",
    "href": "2_Data.html#sec-FileExtensions",
    "title": "2¬† Data files and formats",
    "section": "2.3 Data formats and file extensions",
    "text": "2.3 Data formats and file extensions\nDifferent data types come in different data formats. For audio files, you may be familiar with the MP3 format, but this is by no means the only format in which audio files can be saved. Many other audio file formats exist, such as Waveform Audio File Format (WAVE) and Free Lossless Audio Codec (FLAC).\nWe can usually tell in what format a file is in by looking at its file extension. The file extension is the suffix of the file name. It comes at the end of the file name and is preceded by a dot. The file extension of a WAVE file is .wav, whereas that of an MP3 file is .mp3; hence the file recording.wav is a WAVE file, whereas recording.mp3 is an MP3 file.\n\n\n\n\n\n\nQuiz time!\n\n\n\n2) In which format are Microsoft Word files typically saved?\n\n\n\n\n.msword\n\n\n.docs\n\n\n.odt\n\n\n.docx\n\n\n\n\n\n\n\n¬†\n3) Which of these files are audio files?\n\n\n\n\ndialogue001.mp3\n\n\ndialog_01.csv\n\n\n001_dialog.flac\n\n\ndialogue001.wav\n\n\nDIALOGUE.audio\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n\n\nUnfortunately, many modern operating systems have a tendency to hide file extensions by default. This results in the files recording.wav and recording.mp3 both being displayed as recording in File Finder/Explorer windows (compare Figure¬†2.2 (a) and Figure¬†2.2 (b)). This is misleading and can lead to all kinds of problems.\n\n\n\n\n\n\n\n\n\n\n\n(a) Displaying file names without file extensions\n\n\n\n\n\n\n\n\n\n\n\n(b) Displaying file names with their extensions\n\n\n\n\n\n\n\nFigure¬†2.2: Demonstrating the importance of seeing file extensions.\n\n\n\nTo ensure that you can always see the extensions of the files on your computer in the File Explorer (on Windows) or the File Finder (on macOS), follow these instructions:\n\nOn Windows: https://www.howtogeek.com/205086/beginner-how-to-make-windows-show-file-extensions/.\nOn macOS: https://support.apple.com/en-gb/guide/mac-help/mchlp2304/mac (select the version of your operating system at the top of the page).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-Sharing",
    "href": "2_Data.html#sec-Sharing",
    "title": "2¬† Data files and formats",
    "section": "2.4 Sharing research data and materials",
    "text": "2.4 Sharing research data and materials\nIn line with the principles of Open Science (see Chapter 1), it is important to ensure that both the materials that were used to collect research data (e.g., questionnaire items, audio, image or video stimuli, language aptitude tests, etc.) and the data themselves are made openly available to the research community, whenever legally possible and ethically responsible. Sharing materials ensures that studies can be replicated, for example with new participants or in a different language. Sharing research data also allows independent researchers to reproduce the results of studies, allowing them to verify the reported results and to conduct additional analyses that may confirm, contradict, or extend the conclusions of the original studies.\nYou may be wondering how linguists and language education researchers can make their research data and materials publicly available. Table¬†2.1 provides a non-exhaustive list of public repositories where researchers can upload research data and materials (with figures collected in early June 20241). Some are specific to the language sciences, while others cater to all research disciplines. If you completed the first task in Section 2.1, you should already be familiar with at least one of these! üòâ All of the examples, tasks, and exercises in this book are based on research data and materials that researchers have made available in open access on one or more of these repositories.\n\n\n\n\nTable¬†2.1: Non-exhaustive list of public repositories of research data and materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\nDiscipline\nNb. of entries\nProvides DOI\nOnline since\n\n\n\n\nDryad\nAll\n60000\nYes\n2008\n\n\nFigshare\nAll\n8000000\nYes\n2012\n\n\nHAL\nAll\n5000000\nNo\n2001\n\n\nHarvard Dataverse\nAll\n160000\nYes\n2006\n\n\nIRIS\nLinguistics\n3500\nNo\n2011\n\n\nOpen Science Repository, OSF\nAll\n153663\nYes\n2012\n\n\nTroms√∏ Repository of Language and Linguistics, TROLLing\nLinguistics\n4500\nYes\n2014\n\n\nVivil\nClinical research\n7000\nYes\n2013\n\n\nZenodo\nAll\n3750000\nYes\n2013\n\n\n\n\n\n\n\n\n\nIn the following tasks, we will look at a study by Schimke et al. (2018) (see Figure¬†2.3 (a)), which is an example of a publication which was awarded the Open Data and the Open Materials badges (see Figure¬†2.3 (b)). This means that the research materials and data associated with this study can be found in an open, online repository:\n\n‚ÄúThis article has been awarded Open Materials and Open Data badges. All materials and data are publicly accessible via the IRIS Repository at https://www.iris-database.org/iris/app/home/detail?id=york:934337. Learn more about the Open Practices badges from the Center for Open Science: https://osf.io/tvyxz/wiki‚Äù Schimke et al. (2018).\n\nThe authors could have chosen to upload their materials and data to any of the online repositories listed in Table¬†2.1 but, in this case, they chose IRIS.\n\n\n\n\n\n\n\n\n\n\n\n(a) Title page of the Schimke et al. (2018)\n\n\n\n\n\n\n\n\n\n\n\n(b) The Open Data and Open Materials badges\n\n\n\n\n\n\n\nFigure¬†2.3: An example of a publication for which both research materials and data have been published.\n\n\n\nAmong other results, Schimke et al. (2018) report on two eye-tracking experiments. One of these experiments involved Spanish-speaking participants listening to ambiguous sentences in Spanish whilst looking at images of Playmobil figures (see Figure¬†2.4 for an example).\n\n\n\n\n\n\nFigure¬†2.4: Image from Experiment 1 in Schimke et al. (2018)\n\n\n\n\n\n\n\n\n\nNote¬†2.1: How did the experiment work?\n\n\n\nIn this eye-tracking experiment, participants were instructed to decide whether the sentences they heard matched the Playmobil images or not. Consider the following two sentences from the experiment:\n\n\nEl barrendero se encontr√≥ con el cartero antes de que recogiera las cartas.\n[The street sweeper met the postman before he fetched the letters.]\nEl barrendero se encontr√≥ con el cartero antes de que recogiera la escoba.\n[The street sweeper met the postman before he fetched the broom.]\n\n\nUp until the point at which either las cartas [the letters] or la escoba [the broom] are heard, it is unclear who is doing the fetching. From a grammatical point of view, it could be either the street sweeper or the postman.\nParticipants were presented with Figure¬†2.4 as they were listening to either Sentence 1 or Sentence 2. At the same time, the researchers measured how long it took for the participants to look at the subject governing the verb recogiera. In other words, for Sentence 1, they were interested in how long it took participants to focus on the postman Playmobil figure and, in Sentence 2, on the street sweeper. Such fine measurements are made in milliseconds, i.e.¬†in thousandths of seconds, using a special eye-tracking device.\n\n\n\n\n\n\n\n\nTask 2\n\n\n\nImagine that you want to run an experiment similar to the one carried out in Schimke et al. (2018). You can reuse the Playmobil image files created by the researchers as they helpfully uploaded them to the IRIS database.\nIn which file format do you think the images are archived? To find out, click here to go directly to the list of data and materials associated with the study. There are four entries in the IRIS database that are associated with this study. Select the ‚ÄúPictorial‚Äù entry which contains the images. It allows you to download a ZIP file called Images_online.zip. ZIP is an archive file format that can contain one or more compressed files. Download this ZIP file.\nOnce the download was successful, navigate to the folder where the file was saved on your computer and unzip the file, i.e., decompress it and extract its contents:\n\nTo unzip on Windows, double-click the .zip file\n\nselect ‚ÄòExtract All‚Äô,\nselect a folder,\nand then click ‚ÄòExtract‚Äô.\n\nOn a Mac, simply double-click the .zip file to unzip it.\nIf you are using the Linux command line, use the command unzip followed by the name of the file to unzip it.\n\nYou should find that the ZIP file contains a folder entitled ‚ÄòImages‚Äô, which contains 58 pictures of different combinations of Playmobil figures that correspond to the experiment‚Äôs stimulus sentences.\n1a) In which file format are these Playmobil image files?\n\n\n\n\nBMP\n\n\nGIMP\n\n\nJPEG\n\n\nPNG\n\n\nGIF\n\n\n\n\n\n\n\n¬†\nImage files typically contain metadata that is embedded in the image files themselves. This metadata may include the dimensions of the image and its colour profile. To view this metadata, right-click on one of the image files that you have extracted from the ZIP file and select the option to get more information about the file, e.g., ‚ÄúGet Info‚Äù or ‚ÄúProperties‚Äù.\n1b) How wide are these Playmobil images in pixel?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#working-with-tabular-data",
    "href": "2_Data.html#working-with-tabular-data",
    "title": "2¬† Data files and formats",
    "section": "2.5 Working with tabular data",
    "text": "2.5 Working with tabular data\nThe measurements made by the eye-tracking device in Schimke et al. (2018)‚Äôs eye-tracking experiments were stored in the form of tables. Table¬†2.2 is an extract of a table that contains processed eye-tracking data from Schimke et al. (2018). It forms part of the study‚Äôs supplementary materials and can also be downloaded from the IRIS database.\nIn this table, each row corresponds to the data associated with one participant‚Äôs eye movements while listening to a single stimulus sentence and looking at the corresponding Playmobil image (e.g., Figure¬†2.4). The extract displayed as Table¬†2.2 only shows the data associated with the first six stimulus sentences (items) that participant ‚Äús1‚Äù, a Spanish L2 learner, listened to. The columns crit1, crit2 and crit3 contain values derived from the measurements made using the eye-tracking device.2 From Table¬†2.2, we can also see that participant ‚Äús1‚Äù was 19 years old when they started formally learning Spanish (AoO stands for ‚Äúage of onset of formal instruction‚Äù) and that they were 20 when the experiment was conducted.\n\n\n\n\nTable¬†2.2: Extract of table containing eye-tracking data from Schimke et al.¬†(2018)'s appendix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlanguage\nsubject\ndisambiguation\nitem\ncrit1\ncrit2\ncrit3\nAoO\nage\n\n\n\n\nS\ns1\n1\n1\n0.3451355\n-0.5618789\n0.7036070\n19\n20\n\n\nS\ns1\n2\n2\n-0.2679332\n-1.5849625\n0.1852149\n19\n20\n\n\nS\ns1\n1\n3\n-1.1563420\n0.9898042\n-1.5849625\n19\n20\n\n\nS\ns1\n2\n4\n-1.5849625\n-0.0874628\n-1.5849625\n19\n20\n\n\nS\ns1\n1\n5\n1.5849625\n0.1831223\n1.5849625\n19\n20\n\n\nS\ns1\n2\n6\n-0.7824086\n-0.8548021\n-1.1758498\n19\n20\n\n\n\n\n\n\n\n\n\nWhen working with data, tables are ubiquitous. Data stored in tables are called tabular data. Hence, learning to work with tabular data is a crucial data literacy skill.\nIn the language sciences, the results of most studies (whether experimental or corpus studies) are stored in tables. For example, when researchers conduct an online survey, the data collected by the online survey platform (e.g., Qualtrics, SoSci, SurveyMonkey) is automatically stored in the form of one or more table(s). These can then be exported from the survey platform in various tabular file formats (e.g., .csv, .json, .xlsx).\nIn some cases, data may be collected by analogue means, e.g., by getting participants to answer a paper questionnaire or collecting school children‚Äôs work on paper. However, for quantitative analysis, analogue research data are first digitalised. Then, the data are typically stored as text files in file formats such as .txt or .csv.\n\n2.5.1 Delimiter-separated values (DSV) files\nTables can be stored in many data formats but the simplest and most widely used in linguistic research are text files with delimiter-separated values (DSV). For sharing and archiving research data, DSV files are favoured over formats specific to propriety software such as .xslx (Microsoft Excel files) or .numbers (Apple Numbers files). This is because DSV files can be ‚Äúunderstood‚Äù by many different programs and on all operating systems. The fact that they are simple text files means that we will also be able to reliably read them in the future, even if programs such as Excel or Numbers have evolved or have been discontinued. Reliability and compatibility are fundamental to maintaining the integrity of research data and ensuring that data can be reused, even in the distant future.\nIn DSV files, each value (e.g., measurement or response) is separated by a specific separator character. In principle, any character can be used to separate values, but the most common separators are the comma (,), tab (\\t), and colon (:). Below is the .csv file corresponding to Table¬†2.1.\nRepository,Discipline,Nb. of entries,Provides DOI,Online since\nDryad,All,60000,Yes,2008\nFigshare,All,8000000,Yes,2012\nHAL,All,5000000,No,2001\nHarvard Dataverse,All,160000,Yes,2006\nIRIS,Linguistics,3500,No,2011\n\"Open Science Repository, OSF\",All,153663,Yes,2012\n\"Troms√∏ Repository of Language and Linguistics,TROLLing\",Linguistics,4500,Yes,2014\nVivil,Clinical research,7000,Yes,2013\nZenodo,All,3750000,Yes,2013\nAs you can see, the values are separated by commas.3 Additionally, some of the values are enclosed in, or delimited by, double quotation marks (\"). This prevents any commas that may occur within an actual field value, e.g., the comma in the field Open Science Repository, OSF, from being interpreted as a separator character.\nGiven that DSV files are text files, it is possible to open them in a free plain-text editor (e.g., Notepad++ or BBEdit) or a text-processing program (e.g., Microsoft Word or LibreOffice Writer). However, these programmes will typically display DSV files as in Figure¬†2.5.\n\n\n\n\n\n\nFigure¬†2.5: The .csv file corresponding to Table¬†2.1 opened in Word\n\n\n\nWe can probably agree that what we are seeing in Figure¬†2.5 is not a very reader-friendly way to display tabular data! This is why DSV files are more often opened in spreadsheet programs (e.g., LibreOffice Calc, Google Sheets, Microsoft Excel, Numbers) than in text-editing programs. Let‚Äôs find out how in the next section.\n\n\n2.5.2 Opening DSV files in LibreOffice Calc\nThere are several ways to open a DSV file in LibreOffice Calc but the safest is to launch LibreOffice (see Task 1 in Section 1.2 if you have not yet installed LibreOffice) and, from the list of options under ‚ÄòCreate‚Äô, click on ‚ÄòCalc Spreadsheet‚Äô to open up a blank spreadsheet. Then, from the ‚ÄòFile‚Äô drop-down menu, select ‚ÄòOpen‚Ä¶‚Äô or use the keyboard shortcut Ctrl/Cmd + O and locate the DSV file that you wish to open.\nOn opening a DSV file in LibreOffice Calc, we get a dialogue box with various options (see Figure¬†2.6).\n\n\n\n\n\n\nFigure¬†2.6: Text import dialogue in LibreOfficeCalc\n\n\n\nTo correctly import this particular DSV file, it is necessary to specify that the separator character is the comma (,) and that the delimiter character is the double quotation mark (\") (see selected options in Figure¬†2.6). With these settings in LibreOffice Calc, the table is rendered as in Figure¬†2.7.\n\n\n\n\n\n\nFigure¬†2.7: CSV file opened in LibreOffice Calc\n\n\n\nNote that if you open a DSV file in Excel or Google Sheets, you will not be shown such a dialogue box. Instead, these programs assume that they can guess which separator and delimiter characters your file uses. Whilst this may, at first, sound convenient, this is not good news: you should be the one in control of how your data files are interpreted, not the program! In the next section, you will learn why opening DSV files such as .csv and .tsv files in Microsoft Excel, Google Sheets, or Numbers can be very dangerous. In some cases, these programs will ‚Äòcorrupt‚Äô, i.e.¬†permanently damage, your DSV files, which can lead to irreversible data loss!\nThe bad news is that, if you are using Windows or MacOS, it is very likely that either Excel or Numbers is your default app to open DSV files. This means that if you double click on a .csv and .tsv file in your Finder/Explorer window, the file will likely automatically open up in either Excel or Numbers. This is why it is important you do not double-click on such files to open them: Opening a file just once with these programs can lead to data loss! If this happens to you with a file that you have downloaded from a repository, your best bet is to delete your local version of the file and download a fresh version so that you can start again from scratch.\n\n\n\n\n\n\nTask 3\n\n\n\nIn this task, we will practice opening a DSV file in LibreOffice Calc. Our example file is a real dataset from Schimke et al. (2018). We will begin by downloading it from the public repository IRIS.\nIn addition to the eye-tracking experiments, Schimke et al. (2018) conducted two further experiments in which participants completed a gap-filling task via an online survey platform. In the first of these experiments, the participants were native (L1) speakers of French, German, and Spanish. In the second, they were French- and Spanish-speaking learners (L2) of German.\nIn both experiments, the L1 and L2 participants were shown ambiguous sentences similar to the ones used in the eye-tracking experiment with the Playmobil images (see Note¬†2.1). After having read each stimulus, the participants were asked to complete a gap-fill task according to their understanding of the preceding ambiguous sentence. Participants were told ‚Äúthat there were no incorrect responses and that they should answer spontaneously‚Äù (Schimke et al. 2018: 755). Below is an example questionnaire item in the three languages examined:\n\n\n1. Der Brieftr√§ger ist dem Stra√üenfeger begegnet, bevor er schnell ein Sandwich geholt hat. ___________________ hat ein Sandwich geholt.\n2. Le facteur a rencontr√© le balayeur avant qu‚Äôil prenne rapidement un sandwich. ___________________ a pris un sandwich.\n3a. El cartero se reuni√≥ con el barrendero antes de que √©l recogiera velozmente un emparedado. ___________________ recogi√≥ un emparedado.\n3b. El cartero se reuni√≥ con el barrendero antes de que recogiera velozmente un emparedado. ___________________ recogi√≥ un emparedado.\n\n\nNote that, for Spanish, there were two types of stimuli: one with an overt pronoun (as in 3a. with √©l) and one without (as in 3b. with a null pronoun), as both variants are possible in Spanish. All three examples translate as:\n\n\nThe postman encountered the street sweeper before he quickly fetched a sandwich. ___________________ fetched a sandwich.\n\n\nTo complete the gap, participants could either select ‚ÄòThe postman‚Äô or ‚ÄòThe street sweeper‚Äô.\n\nGo back to the study‚Äôs page on IRIS and select the second entry entitled ‚ÄòOther questionnaire‚Äô which, among other things, contains ‚ÄòWritten production data‚Äô.\n\nNote that this database entry includes both research data and research materials: the file sentences_offline_task.xlsx contains the full list of questionnaire items, including both experimental and filler items, with which we could reconstruct the experiment to replicate it with a new set of participants. For now, however, we are not interested in obtaining materials to replicate the study, but rather in examining the study‚Äôs original data.\nThis IRIS entry also contains three data files. The last file (logoddslearnersfinal.txt) is the DSV file that was used to create Table¬†2.2 above.\nIn this task, we are going to look at the questionnaire data corresponding to the gap-filling task experiment conducted with German L2 learners, which is contained in the data file offlinedataLearners.txt:\n\nDownload the offlinedataLearners.txt file (which is the second listed) and save it on your computer (see Section 3.3).\nLaunch LibreOffice (see Section 1.2 if you have not yet installed LibreOffice) and, from the list of options under ‚ÄòCreate‚Äô, click on ‚ÄòCalc Spreadsheet‚Äô to open up a blank spreadsheet.\nFrom the ‚ÄòFile‚Äô drop-down menu, select ‚ÄòOpen‚Ä¶‚Äô or use the keyboard shortcut ‚ÄòCtrl/Cmd + O‚Äô. Find offlinedataLearners.txt in the folder where you saved it and click on ‚ÄòOpen‚Äô.\nA ‚ÄòText Import‚Äô dialogue box will pop up. This a DSV file, not a fixed-width file, so ensure that the option ‚ÄòSeparated by‚Äô is selected. If not already set by default, it is also a good idea to select ‚ÄòUnicode (UTF-8)‚Äô for the ‚ÄòCharacter set‚Äô.\nExperiment with the different ‚ÄòSeparator Options‚Äô until the preview at the bottom of the dialogue box looks like a table.\nEnsure that, apart from the ‚ÄòSeparator Options‚Äô, all other options in the dialogue box are unselected and then click on ‚ÄòOK‚Äô.\n\n\nQuiz time!\na) What is the separator character in the file offlinedataLearners.txt?\n\n\n\n\nTab\n\n\nComma\n\n\nSemicolon\n\n\nSpace\n\n\nAll of them\n\n\n\n\n\n\n\n¬†\nb) What is the delimiter character in the file offlinedataLearners.txt?\n\n\n\n\nBoth \" and '\n\n\n'\n\n\n\"\n\n\nThere is none.\n\n\n\n\n\n\n\n¬†\nc) How many observations does the file offlinedataLearners.txt contain?\n\n\n\n\n700\n\n\n3500\n\n\n5\n\n\n3505\n\n\n701\n\n\n\n\n\n\n\n¬†\nd) In this table, what does each observation correspond to?\n\n\n\n\nA single participant's response to a single sentence gap.\n\n\nAll the responses in a single language.\n\n\nAll the responses to a single sentence in a single language.\n\n\nAll the responses from a single participant.\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\n\nWhat if I absolutely have to open a DSV file in Excel? üòß\n\n\n\n\n\nIf you absolutely must open a DSV file (e.g., a .csv or .tsv file) in Excel (for example because you do not have sufficient permissions to install LibreOffice on the computer that you are using), do not open the file by double clicking on the file as this will automatically trigger Excel‚Äôs problematic auto-formatting behaviour (see Section 2.6)! Instead, first launch Excel and create a new blank workbook. Then navigate to the ‚ÄòData‚Äô tab, select the ‚ÄòGet Data‚Äô option, and then ‚ÄòFrom Text/CSV‚Äô (see Figure¬†2.8). In the following dialogue, you can specify how the data should be imported. The options are very similar to the ones offered in LibreOffice (see above).\nNote that with this method it may be possible to prevent Excel from automatically (and irreversibly!) applying transformations to your data. However, sadly, this may not suffice. Read on to find out more‚Ä¶\n\n\n\n\n\n\nFigure¬†2.8: Import data into excel",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#sec-ExcelWarning",
    "href": "2_Data.html#sec-ExcelWarning",
    "title": "2¬† Data files and formats",
    "section": "2.6 A word of warning about spreadsheet programs ‚ö†Ô∏è",
    "text": "2.6 A word of warning about spreadsheet programs ‚ö†Ô∏è\nYou should be aware that opening DSV files in spreadsheet programs can corrupt the files! Once a file is corrupted, it is often not possible to retrieve the original data so this is very bad news, indeed. Such problems are particularly frequent when opening DSV files with Microsoft Excel and Google Sheets. This is because the default settings in these programs surreptitiously modify files upon opening.\nThese ‚Äòauto-format‚Äô modifications include replacing certain values by dates (e.g., changing 3-4 to March, 4th) or numbers (e.g., changing 1.23E5 to 123000)4, removing leading zeros (e.g., changing 001 to 1), or misinterpreting certain characters (e.g., the value -ism will generate an error because the hyphen is interpreted as minus sign).\nNot only can these auto-format modifications lead to inaccurate data analysis but, in the worst of cases, they can even cause data loss. The crux of the problem is that often users do not realise what the program has done in the background. How bad can this be? Find out by completing the task below.\n\n\n\n\n\n\nTask 4\n\n\n\nIn this task, you will find out how genetics researchers who use spreadsheets for their analyses regularly have their data so badly damaged that it affects the results of their publication. Though we have no statistics on how spreadsheet errors affect the work of linguists, it is (unfortunately) very likely to be just as bad as in genetics.\nZiemann, Eren & El-Osta (2016) reported that a fifth of genetics publications with supplementary .xls or .xlsx files with gene lists contained errors caused by Excel‚Äôs auto-formatting behaviour. The results of this study shocked the research community and a report about it went viral. Click on the link below to read the open-access article ‚ÄúGene name errors: Lessons not learnt‚Äù by Abeysooriya et al. (2021) to find out whether the situation has improved since 2016 and answer the questions below.\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu & Mark Ziemann. 2021. Gene name errors: Lessons not learned. PLOS Computational Biology. Public Library of Science 17(7). e1008984. https://doi.org/10.1371/journal.pcbi.1008984.\n\na) Has the proportion of genetics publications with Excel gene lists affected by auto-formatting errors decreased since 2016?\n\n\n\n\nNo, it increased between 2016 and 2020.\n\n\nNo, it has remained stable.\n\n\nYes, it decreased between 2016 and 2020.\n\n\n\n\n\n\n\n¬†\nb) Does using LibreOffice Calc (see Section 1.2) also cause these same issues?\n\n\n\n\nNo, if you cannot afford Excel, then LibreOffice Calc is an excellent open-source alternative.\n\n\nYes, LibreOffice is just as likely to cause such errors.\n\n\nNo, but whilst LibreOffice is better than Excel or Google Sheets, it is still less than ideal for data analysis.\n\n\n\n\n\n\n\n¬†\nc) Did highly reputable journals publish fewer articles with erroneous Excel gene lists?\n\n\n\n\nNo, publications with problematic Excel files were found in more or less equal proportions in all journals.\n\n\nYes, they published fewer.\n\n\nNo, they published more.\n\n\n\n\n\n\n\n¬†\n\n\nIt is worth noting that, for some Windows users, these auto-formatting issues can corrupt files that they have never actively opened in Excel! ü§Ø This happens when Windows applies Excel‚Äôs default settings to all CSV files, regardless of what program they are actually opened with. To ensure that this does not happen to you, check that Excel is definitely not your default app to open .csv and .tsv files (see below for instructions).\n\n\n\n\n\n\nOpening a .csv or .tsv file in LibreOffice from a File Finder/Explorer window\n\n\n\n\n\nRemember that to open a .csv or .tsv file on your computer, should never ever double-click on it and let the default program open it! As we saw in Section 2.6, this can break or ‚Äòcorrupt‚Äô the file. To avoid accidentally double-clicking on a .csv or .tsv file and having the file corrupted, I recommend making either LibreOffice or a plain-text editor (e.g., Notepad++ or BBEdit) your default application to open up such files.\nOn MacOS, you can change the default application used to open files of any file extensions by right-clicking a file name with this particular extension and than selecting ‚ÄòGet Info‚Äô (Figure¬†2.9 (a)). In the example below, Numbers is the default application for all .csv files (see Figure¬†2.9 (b)). In the dropdown menu ‚ÄòOpen with:‚Äô, you can then select LibreOffice (provided you have installed it beforehand!) and finally click on ‚ÄòChange All‚Ä¶‚Äô (Figure¬†2.9 (c)). You will be asked to confirm your choice.\n\n\n\n\n\n\n\n\n\n\n\n(a) ¬†\n\n\n\n\n\n\n\n\n\n\n\n(b) ¬†\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) ¬†\n\n\n\n\n\n\n\nFigure¬†2.9: Changing the default application for a file extension on MacOS\n\n\n\nIf your operating system is Windows, you should look in your Windows‚Äô settings for the option ‚ÄòDefault Apps‚Äô (see Figure¬†2.10).\n\n\n\n\n\n\nFigure¬†2.10: Default apps in Windows settings\n\n\n\nIn the next step, select ‚ÄòChoose default apps by file type‚Äô. Here, you can search for .csv as a file type, and choose which program you want to set as the default program for opening .csv files. If Excel is currently your default (as in Figure¬†2.11 (a)), you can click on Excel and choose a different program. LibreOffice is a sensible, open-source alternative (see Figure¬†2.11 (b)). A plain-text editor such as Notepad would also be fine (also listed on Figure¬†2.11 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Excel as the default programme for .csv files\n\n\n\n\n\n\n\n\n\n\n\n(b) Changing the default programme for .csv files\n\n\n\n\n\n\n\nFigure¬†2.11: Changing the default app for opening .csv files in Windows\n\n\n\nIf it is not possible to adjust the default app settings, either due to insufficient permissions or because you only have temporary access to this PC, do not to open .csv or .tsv files with the default program. Instead, right-click on the file name and, using the ‚ÄòOpen with‚Äô option, select the option to open the file with LibreOffice, if available, or else with a plain-text editor.\n\n\n\n\n\n\n\n2011. IRIS. https://iris-database.org/.\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu & Mark Ziemann. 2021. Gene name errors: Lessons not learned. PLOS Computational Biology. Public 17(7). e1008984. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nSchimke, Sarah, Israel de la Fuente, Barbara Hemforth & Saveria Colonna. 2018. First language influence on second language offline and online ambiguous pronoun resolution. Language Learning 68(3). 744‚Äì779. https://doi.org/10.1111/lang.12293.\n\n\nZiemann, Mark, Yotam Eren & Assam El-Osta. 2016. Gene name errors are widespread in the scientific literature. Genome Biology 17(1). 177. https://doi.org/10.1186/s13059-016-1044-7.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "2_Data.html#footnotes",
    "href": "2_Data.html#footnotes",
    "title": "2¬† Data files and formats",
    "section": "",
    "text": "Note that, for some repositories, the number of entries includes other types of research outputs, e.g., preprints and figures.‚Ü©Ô∏é\nDetails of what these values mean are not relevant here but, for those of you who are curious, they correspond to the ‚Äúlog odds of looks‚Äù that participant made towards one or the other Playmobil figure whilst listening to the experimental stimulus sentences at three time points, called ‚Äúcritical regions‚Äù. These critical regions include the time window between the onset of the pronoun and 480 milliseconds after the onset of the disambiguating information. Schimke et al. (2018: 768‚Äì769) explain that ‚Äú[a] positive value of the log odds indicates more looks to the subject than to the object antecedent, while a negative value indicates the reverse pattern.‚Äù‚Ü©Ô∏é\nNote that the file extension .csv stands for ‚Äúcomma-separated values‚Äù. Confusingly, however, DSV files are often given a .csv extension even when the separator character is not the comma. As a result, even though the .tsv extension stands for ‚Äútab-separated values‚Äù, .csv files are frequently separated by a tab (\\t) rather than comma. Isn‚Äôt that fun? üôÉ‚Ü©Ô∏é\nIn scientific notation, ‚ÄúE‚Äù stands for ‚Äúexponent‚Äù, which refers to the number of times a number needs to be multiplied by 10. This notation is used as a shorthand way of writing very large or very small numbers. This is why ‚Äú1.23E5‚Äù is interpreted by Excel as 1.23 multiplied by 10 to the power of 5, which is to say: 1.23 multiplied by 100,000. This operation shifts the decimal point five places to the right, resulting in the number 123000.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data files and formats</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html",
    "href": "3_DataManagement.html",
    "title": "3¬† Data management",
    "section": "",
    "text": "Chapter overview\nEven if you are confident that you have no trouble managing your computer files, it is still worth taking a few minutes to read up on the basics of data management. This is especially true if you consider yourself a ‚Äúdigital native‚Äù as modern operating systems have made the way that computers deal with files very opaque.\nIn this chapter, you will learn about:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#recipes-for-successful-data-management-masterchef-meets-masters-thesis",
    "href": "3_DataManagement.html#recipes-for-successful-data-management-masterchef-meets-masters-thesis",
    "title": "3¬† Data management",
    "section": "3.1 Recipes for successful data management: MasterChef meets Master‚Äôs Thesis",
    "text": "3.1 Recipes for successful data management: MasterChef meets Master‚Äôs Thesis\nData management is hardly a ‚Äúhot‚Äù topic that people like to dwell on. That‚Äôs a shame because good file management is absolutely central to be able to conduct research and poor file management has the potential to seriously ‚Äòspice things up‚Äô‚Ä¶ but not in a good way! üå∂Ô∏è Whether you are working on a short course assignment, your Master‚Äôs or PhD thesis, or as part of a large research project team: research-related files must be named appropriately and safely stored in meaningful places.\nImagine trying to make a curry in an utterly disorganised kitchen that contains dozens of different spices, scattered across different cabinets and drawers, with vague or misleading labels. For example, you might have three jars labelled ‚ÄúChilli‚Äù and no way of knowing which is mild ‚ÄúKashmiri Chilli‚Äù as opposed to the extra hot ‚ÄúThai Bird‚Äôs Eye Chilli‚Äù. The third might not be chilli at all, but actually a jar of paprika that has been entirely mislabelled. Some of these spices have been gathering dust for decades but the labels have no best-before dates so there is no way of knowing which are still fragrant. Cooking in such a kitchen would turn even the simplest cooking task into a tedious, time-consuming, and error-prone chore: If you‚Äôre not extremely careful, you could easily end up serving something that is bland or, in the worst of cases, entirely inedible! Similarly, in research, if your files are poorly named or stored haphazardly, it will make your work far less efficient, considerably more error-prone, and ultimately utterly frustrating.\nBut the good news is: just as a tidy, well-organized kitchen can greatly enhance your cooking experience, good file management can streamline your research process, help you avoid making mistakes, and reduce stress. In the following sections, we will cook up some good practices for file naming, data management, and project organisation. We will start with basic recipes for naming and managing your files. See the ‚ÄòGoing further‚Äô boxes for tips on learning the ‚Äògourmet skills‚Äô needed to handle more complex projects. So, let‚Äôs don the chef‚Äôs hat and learn how to create a user-friendly computer workspace. And remember, as with cooking, practice makes perfect! üßëüèΩ‚Äçüç≥",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#sec-FileNaming",
    "href": "3_DataManagement.html#sec-FileNaming",
    "title": "3¬† Data management",
    "section": "3.2 Naming conventions",
    "text": "3.2 Naming conventions\nFile names are labels. They tell us what is inside a file and helps us identify the correct file quickly and reliably. If you had to run to the printing shop to get your thesis printed in time for a tight deadline, which of these sets of files would you rather have to choose from? Which is more likely to lead you to getting the wrong version printed?\n\n\n\n\n\n\n\n\n\n\n\n(a) ¬†\n\n\n\n\n\n\n\n\n\n\n\n(b) ¬†\n\n\n\n\n\n\n\nFigure¬†3.1: Two sets of file names, one clearly better than the other.\n\n\n\nLike the labels on your neatly organised spice jars, file and folder names should be clear, concise, and easily readable. Good file and folder names should be both human-friendly and computer-friendly.\nBy human-friendly we mean that you and any other human being should easily be able to understand what a folder or file contains. Just like you wouldn‚Äôt want a label on a spice jar to be a random string of numbers (e.g., 0171) or only include the best-before date but nothing else (e.g., 31 Jan 2028), you also wouldn‚Äôt want to guess what a file contains based on an ambiguous or unclear name like Chili. Labels should be informative but succinct (e.g., Thai Bird's Eye Chilli 31 Jan 2028 not Thai Bird's Eye Chilli bought on December 19, 2023 whilst Christmas shopping with mum, note that the best before date is 31 January 2028)! Unless you and all your colleagues read Thai, do not be tempted to write the part of the file name in Thai as this could also lead to misunderstandings.\nAnother reason for not including Thai characters in your file name is that it would not be computer-friendly. In general, computers are not good at dealing with names that contain anything else but Latin alphanumeric characters, e.g., the letters A to Z and a to z with no accents and the numbers 0 to 9. Hyphens (-) and underscores (_) can also be used, but not spaces. The dot (.) is reserved for the file extension and should ideally not be used elsewhere in the file name.\nHence, whilst Thai Bird's Eye Chilli 31 Jan 2028 is human-friendly, it is not computer-friendly. To make it a computer-friendly label, we need to remove the apostrophe. Whilst spaces are not strictly forbidden, they can cause all kinds of issues and are therefore also best avoided. Space characters can be replaced by hyphens (-) and underscores (_) and the two can be combined in a meaningful way. For example, in the label Thai-Birds-Eye-Chilli_31-Jan-2028, the _ distinguishes between two different pieces of information, whilst the - helps humans to parse individual words within a piece of information. Using such patterns consistently not only helps humans to read file names efficiently, it also means that computers can easily ‚Äòparse‚Äô, i.e., break down such names into meaningful items. This can be very useful to search for files or automatically extract metadata from file names.\n\n\n\n\n\n\nFigure¬†3.2: To help us remember the different, systematic ways to use letter case, hyphens, and underscores in naming conventions, these patterns have fun names (art work by @allison_horst).\n\n\n\nIt is fine to use both lower-case and upper-case letters in file and folder names. However, some operating systems will treat upper-case and lower-case letters as the same, whilst others will not. This means that you should avoid having file names that are only distinguishable by case.\nFinally, it is worth noting that file names cannot be infinitely long! The maximum length of a file name depends on the operating system and the application that you use1 but, as a rule of thumb, if you can display the entire file name in a reasonably sized Finder window (on macOS) or File Explorer window (on Windows), its length is unquestionably both human- and computer-friendly.\n\n\n\n\n\n\nQuiz time!\n\n\n\n1) In which case is this file name? my_first_file_name.R\n\n\n\n\ncamelCase\n\n\nkebab-case\n\n\nUpperCamel\n\n\nlower_snake\n\n\nUPPER_SNAKE\n\n\n\n\n\n\n\n¬†\n2) Why is this file name problematic? MyDocument final.1a.docx\n\n\n\n\nMixed capitalisation\n\n\nLack of clarity\n\n\nUse of special character other than _ or -\n\n\nSpaces in file name\n\n\n\n\n\n\n\n¬†\n3) Which of these file names are both human-friendly and computer-friendly?\n\n\n\n\nAnalysis_24April.R\n\n\n05.01.24_Draft.docx\n\n\nMANUSCRIPT_CORRECTIONS.docx\n\n\nMC1.png\n\n\n2024-01-05_TermPaper.docx\n\n\n\n\n\n\n\n\n\nIt is also important to ensure that file names are easily sortable. If you have a series of files that document a process, consider beginning each file name with a number that correspond to the order of the process, e.g., 01_DataPreparation.R, 02_DataAnnotation.R, 03_AnnotationEvaluation.R. Left-padding the numbers with one or more 0 will mean that the files are sorted numerically, even when files are listed alphabetically (see Figure¬†3.3 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) File names without additional zeros numbers\n\n\n\n\n\n\n\n\n\n\n\n(b) File names with left-padded numbers\n\n\n\n\n\n\n\nFigure¬†3.3: Why left-padding file names is good file naming practice.\n\n\n\nIt is often a good idea to include the date in file names. However, many date formats are not easily sortable (see Figure¬†3.4 (a)). Formatting dates using the ‚ÄòYYYY-MM-DD‚Äô format as in Figure¬†3.4 (b) will allow you to easily sort your files in chronological order.\n\n\n\n\n\n\n\n\n\n\n\n(a) File names with a non-ordered date format\n\n\n\n\n\n\n\n\n\n\n\n(b) File names with an ordered date format\n\n\n\n\n\n\n\nFigure¬†3.4: Why using the YYYY-MM-DD is good file naming practice.\n\n\n\nEven though computers have gotten much better at dealing with folder and file names containing spaces and special characters, using anything other than basic Latin alphanumeric characters, - and _ in file and folder names will - sooner or later - cause you or your colleagues some serious issues. This is especially true when you start coding. Do not delay getting used to using systematic, human- and computer-friendly folder and file names! In the long run, these simple guidelines will make your digital life much smoother and save you much time and unnecessary stress.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#sec-FoldersPaths",
    "href": "3_DataManagement.html#sec-FoldersPaths",
    "title": "3¬† Data management",
    "section": "3.3 Folders and paths",
    "text": "3.3 Folders and paths\nNow that you know how to name your files and folders sensibly, we can turn to best practices for organising these files and folders. Returning to our kitchen analogy, imagine that, over many years, you collected hundreds of recipes from friends and family. These recipes are jotted down on individual sheets of paper, all of which have been thoughtlessly tossed into a large kitchen drawer called ‚ÄòDocuments‚Äô, which also happens to contain receipts for kitchen appliances still under warranty, takeaway brochures, and various other bits of paper. In such a kitchen, finding Aunt Sophie‚Äôs famous caramelised apple cake could take a while! If, however, you had a dedicated kitchen drawer for recipes which contained neatly labelled folders different types of dishes, you would know to look for this cake recipe in the Desserts folder. Within the Desserts folder, you could have sub-folders for different types of desserts (e.g., cakes, ice creams, trifles). This would make finding Aunt Sophie‚Äôs recipe an absolute piece of cake!\nThinking about how to structure folders and sub-folders for your projects is about creating a kind of road map that should be readily interpretable by both humans and computers. This is where the concept of ‚Äòpaths‚Äô arises. Paths, in simple terms, describe the location of a file or a folder in a computer‚Äôs filesystem. There are different types of paths. An absolute path provides a complete path from the computer‚Äôs ‚Äúroot folder‚Äù. If our house were our root folder, the absolute path to Aunt Sophie‚Äôs recipe would be \"/Kitchen/Recipes/Desserts/Cakes/Apple-Cake_Aunt-Sophie\". Hence, just like your home‚Äôs postal address, which ideally specifies your home‚Äôs absolute location worldwide, an absolute path provides a complete path from a computer‚Äôs root folder to the file or folder in question.\nBy contrast, a relative path represents the location of a file or folder relative to another folder. Hence, if we already have the Dessert folder open in front of us, the relative path to the apple cake recipe would simply be \"Cakes/Apple-Cake_Aunt-Sophie\". However, if we wanted to access a recipe in the Starters folder from the Cakes folder, we would first have to go ‚Äúback up the path‚Äù from the Cakes folder to the Recipes drawer. This is achieved by adding ../ to the front of the relative path, e.g., \"../Starters/Soups/Pea-Mint-Soup_Barbara\".\nFor example, \"/Users/lefoll/Documents/Teaching/RstatsTextbook/ToDo.txt\" is the absolute path from my computer‚Äôs root folder to the file containing my to-do list in relation to this textbook project. By contrast, a ‚Äúrelative path‚Äù represents the location of a file or folder relative to another folder. Hence, if I am already in the directory \"/Users/lefoll/Documents/Teaching/RstatsTextbook/\", the relative path to my to-do list is only \"ToDo.txt\".\nNote that, here, we use the term ‚Äúfolder‚Äù as a metaphor for a computer file directory. Most modern operating systems use folder icons that look like the kind of paper file folders that office workers use to have piled up on their desks as a means of visually representing directories in computer file systems.\nTo complicate things a little, the way file paths are written varies depending on the computer‚Äôs operating system. In Unix-based systems like Linux and macOS, paths are written using forward slashes (e.g., \"/Users/elen/Documents/Teaching/RstatsTextbook/ToDo.txt\"), whereas on Windows, paths are written using backslashes (e.g., \"C:\\Users\\elen\\Documents\\Teaching\\RstatsTextbook\\ToDo.txt\").\nThere are many ways to find out where your files are stored on your computer. Let us begin by opening a Finder window (on macOS) or a File Explorer window (on Windows). Navigate to the folder which contains the file for which you want to find the absolute path. Alternatively you could use your computer‚Äôs search function to search for the file. Once you have found it:\n\non Windows: Right-click on the file (in some older Windows versions, you may also need to press the ‚Äúshift‚Äù key). Among the options presented to you, click on the one to copy the file path (e.g., ‚ÄúCopy as path‚Äù or similar in the language of your operating system).\non macOS: Right-click on the file and then press the Option/‚å• key on your keyboard. Pressing down this key will change the options you are given after having right-clicked. One of these options should now be ‚ÄúCopy ‚Ä¶ as Pathname‚Äù (or something equivalent in the language of your operating system). Click on this option.\n\nThen, open any text-editing programme (e.g., LibreOffice Writer, Microsoft Word, TextEdit, or NotePad++) and use the shortcut Ctrl/Cmd + V to paste your file‚Äôs path in the empty document. If you are on Windows, your path should have backslashes, whereas if you are on Linux or macOS, your path should have forward slashes.\n\n\n\n\n\n\nQuiz time!\n\n\n\n\n\n\n\n\n\nFigure¬†3.5: Screenshot of a Finder window showing the hierarchical folder structure within the UzK folder (which stands for University of Cologne)\n\n\n\n4) What is the absolute path to the highlighted file in Figure¬†3.5?\n\n\n\n\nUsers\\lefoll\\Documents\\UzK\\2024_SoSe_Stats\\Rscripts\\2_ErrorsAreFun.R\n\n\nUsers/lefoll/Documents/UzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\nUzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n../UzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\n\n\n\n¬†\n5) From the ‚ÄúUzK‚Äù folder, what is the relative path to the highlighted file in Figure¬†3.5?\n\n\n\n\n../UzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\nRscripts/2_ErrorsAreFun.R\n\n\n2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\nUzK/2024_SoSe_Stats/Rscripts/2_ErrorsAreFun.R\n\n\n\n\n\n\n\n¬†\n6) From the ‚ÄúRscripts‚Äù folder, what is the relative path to the folder ‚Äú2023_SoSe_CADS‚Äù (see Figure¬†3.5)?\n\n\n\n\n../../2023-SoSe-CADS\n\n\n../../2023_SoSe_CADS\n\n\n../2023_SoSe_CADS\n\n\n../../../2023_SoSe_CADS\n\n\n\n\n\n\n\nHint: From the Rscript folder, you will need to go ‚Äúback up the path‚Äù twice: once to get to the course folder 2024_SoSe_Stats and a second time to get to the UzK folder, before you can move to the 2023_SoSe_CADS folder. Going back up the path is achieved with ../.\n\n\n\n\n\n\n\n\nTask\n\n\n\nRead the abstract of the following academic article. What was this experimental study about?\n\nTerai, Masato, Junko Yamashita & Kelly E. Pasich. 2021. Effects of Learning Direction in Retrieval Practice on EFL Vocabulary Learning. Studies in Second Language Acquisition 43(5). 1116‚Äì1137. https://doi.org/10.1017/S0272263121000346.\n\na) According to the study, which is the most effective way of learning vocabulary in a foreign language?\n\n\n\n\nBy first reading a word in one's native language, and then reading a translation in the target language.\n\n\nBy first reading a word in the target language, and then a translation in one's native language.\n\n\nIt's impossible to tell as all learners are individuals.\n\n\nBeginners learn better if they are first exposed to a word in their native language and then in the target language. The opposite is true for more advanced learners.\n\n\n\n\n\n\n\n¬†\nThe authors of this article have published the data and materials associated with this study on IRIS. You can find them here: https://iris-database.org/search/?s_publicationAPAInlineReference=Terai%20et%20al.%20(2021)\nb) In which format are the video files associated with this publication?\n\n\n\n\n.mov\n\n\n.mp4\n\n\n.mxf\n\n\n.avi\n\n\n\n\n\n\n\n¬†\nc) In which format is the analysis code which they shared on IRIS?\n\n\n\n\nPython\n\n\nRmarkdown\n\n\nR\n\n\nHTML\n\n\n\n\n\n\n\n¬†\nd) The associated materials also include a section entitled ‚ÄúScores on measures / tests‚Äù. Download the file dataset1_ssla_20210313.csv from this section. Which character is used as the separator in this delimiter-separated values (DSV) file?\n\n\n\n\nSpace\n\n\nSemicolon\n\n\nTab\n\n\nColon\n\n\nComma",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#backing-up-data-fire-safety-measures-in-the-digital-kitchen",
    "href": "3_DataManagement.html#backing-up-data-fire-safety-measures-in-the-digital-kitchen",
    "title": "3¬† Data management",
    "section": "3.4 Backing up data: ‚ÄòFire safety‚Äô measures in the digital kitchen üßØ",
    "text": "3.4 Backing up data: ‚ÄòFire safety‚Äô measures in the digital kitchen üßØ\nA basic principle of sound data management consists in keeping a copy of all your files in more than one place. This ensures that, should something go awry, your research is not lost forever but instead can be recovered and restored promptly. There are many ways things could go wrong: laptops can get stolen or permanently damaged (laptops are not terribly keen on hot chocolate as it turns out‚Ä¶ üôà), computer files can be corrupted and become unusable, you or someone else may accidentally delete files, your computer can become infested with a nasty virus, etc.\nAn effective way to protect your projects is to abide by the 3-2-1 rule (Schweinberger 2022). It‚Äôs simple:\n\nEnsure that you have at least three copies of your data (e.g., one that you work with on your personal computer and two back-up copies).\nSplit the backup copies between two different storage media (e.g., a hard-drive stored in your office and online in a secure cloud service).\nStore one of these copies in a secure place off-site (i.e., not where your computer usually is).\n\nOne solution is to store your three copies on:\n\nyour personal laptop or computer,\na backup hard drive stored in a secure location, and\na secure online repository such as the data management system provided by your institution, e.g., Sciebo, ownCloud, or GitLab.\n\nChoosing an online repository will protect your data if your computer malfunctions or is damaged or stolen, but remember that it can also potentially make your data accessible to others. This is particularly true of commercial back-up solutions such as Microsoft‚Äôs OneDrive, Google‚Äôs Drive, Apple‚Äôs iCloud, or Dropbox, which although convenient and very user-friendly, should not be used to store sensitive data (e.g., data that may be used to identify individuals, contain financial information, health records, location data, or proprietary research data). Always check if your institution has its own, secure cloud option. If not, keeping a second hard-drive copy in a separate, secure location is likely the safest solution.\nWhilst the 3-2-1 rule stipulates that you should keep at least three copies of each file, in an optimal scenario, each file should exist only once at each location (e.g., on your laptop, a separate hard-drive, and the server of an online repository). It is quite easy to (often unknowingly) end up with several duplicates of the same file on any one machine but this can cause issues if, for example, you end up updating the wrong version of the file. Avoiding and eliminating file duplicates is therefore an important step towards proficient data management.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#conclusion",
    "href": "3_DataManagement.html#conclusion",
    "title": "3¬† Data management",
    "section": "3.5 Conclusion",
    "text": "3.5 Conclusion\nSound data management - comprising of both good folder and file naming practices and the smart organisation of these folders and files - is the foundation for efficient research workflow. Understanding and applying these basic principles of file management will ensure that everything in your digital ‚Äòkitchen‚Äô has its place, is well labelled, and easy to find. By ensuring that we keep our kitchens clean, tidy, and safe, we can whip out some truly delicious dishes!\n\n\n\n\n\n\nFigure¬†3.6: Artwork by xkcd\n\n\n\nWhilst the above caption is true, if it helps, you might want to imagine that someone very judgemental could actually look in your Documents folder at any given time!\n\n\n\n\n\n\nGoing further\n\n\n\n\n\nThis short online module is ideal to learn more about smarter ways to work with files and data:\nThe University of Queensland Library. 2023. Work with Data and Files. The University of Queensland. https://uq.pressbooks.pub/digital-essentials-data-and-files/. (14 May, 2024).\nTo go further, here are some great in-depth resources to learn more about data management in linguistics and education research specifically:\n\nBerez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller & Lauren B. Collister. 2022. The Open Handbook of Linguistic Data Management. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\nLewis, Crystal. Data Management in Large-Scale Education Research. https://datamgmtinedresearch.com/. (14 May, 2024).\n\nBoth of these are available as Open Educational Resources.\n\n\n\n\n\n\n\nalvinashcraft, alexbuckgit, ArcticLampyrid & bearmannl. 2022. Maximum path length limitation. Learn Microsoft. https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation.\n\n\nSchweinberger, Martin. 2022. Data management, version control, and reproducibility. https://ladal.edu.au/repro.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "3_DataManagement.html#footnotes",
    "href": "3_DataManagement.html#footnotes",
    "title": "3¬† Data management",
    "section": "",
    "text": "For example, many Windows applications have a maximum file path length of 260 characters(alvinashcraft et al. 2022).‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data management</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html",
    "href": "4_InstallingR.html",
    "title": "4¬† Installing R and RStudio",
    "section": "",
    "text": "Chapter overview\nThis chapter is designed to help you get started using R and RStudio, assuming no prior use of either. We will be covering the following topics:\nIf you already have some experience of using R and RStudio, please ensure that both are up-to-date. Whilst parts of this chapter will likely be revision, others may be the opportunity to learn some new tips about setting up and using R in RStudio, installing and citing packages. Once you‚Äôve skimmed through this chapter, feel free to swiftly move on to Chapter 5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#why-learn-r",
    "href": "4_InstallingR.html#why-learn-r",
    "title": "4¬† Installing R and RStudio",
    "section": "4.1 Why learn R?",
    "text": "4.1 Why learn R?\nIn short, because R can do it all! üôÉ This statement is only a slight exaggeration: R is indeed a highly versatile programming language and environment that allows us to do a multitude of tasks relevant to the language sciences. These include data handling and processing, statistical analysis, creating effective and appealing data visualisations, web scraping, text analysis, generating reports in various formats, designing web pages, and interactive apps, and much, much more! üí™\nWhilst some will claim that R has a steep learning curve, this textbook aims to prove that the opposite is true! Whilst it‚Äôs fair to say that, as with all new things, it will take you a while to get the hang of it, once you‚Äôve got started, you will see that your possibilities are (pretty much) endless and that learning how to do new things in R makes for fun and very rewarding challenges. What‚Äôs more, this textbook introduces the {tidyverse} approach to programming in R, which is particularly accessible to beginners. We will also use RStudio to access R, which makes things considerably more intuitive and generally easier to work with.\nWhat‚Äôs more, both R and the RStudio Desktop version that we will be using are free and open source (see Section 1.2), which means that they are accessible to all, regardless of their institutional affiliation or professional status. This is in contrast to proprietary statistical software such as SPSS, for which you or your university needs to buy an expensive license. To get started in R, all you will need is access to the internet, a computer (unfortunately, a tablet will not suffice), and the intrinsic motivation to work your way through the basic skills taught in this textbook.\n\n[U]sing R - it‚Äôs like the green and environment-friendly gardening alternative to buying plastic wrapped tomatoes in the supermarket that have no taste anyway. (Martin Schweinberger 2022)\n\n\n\n\n\n\nFigure¬†4.1: ‚ÄúTomato Harvest, Yellow & Red‚Äù by OakleyOriginals (CC BY 2.0).\n\n\n\n\nLast but not least, in choosing to learn R, you are entering a vibrant community of users. As an open-source programming environment, R is the product of many different people‚Äôs contributions. Everyday, new packages, functions, and resources are being developed, improved, and shared with the community. Given that R has evolved into one of the most popular languages for scientific programming (and has become ‚Äúthe de facto standard in the language sciences‚Äù Winter 2019: xiii), many of these have been created by scientists and are particularly well-suited to research workflows. Moreover, the R community is known for being welcoming, supportive, and inclusive (sadly, the same cannot be said of all communities in the computing world). This is reflected in the strong presence of many community-led initiatives such as RLadies and RainbowR, which encourage under-represented groups to participate in and contribute to the R community. ü§ó\n\n\n\n\n\n\nFigure¬†4.2: Logo of the RLadies Ribeir√£o Preto meet-up group, one of many RLadies chapters.\n\n\n\n\n‚ÄúLook, I am studying languages so why should I learn to code?‚Äù ü§î\nUsing scripts rather than GUI software will help you make your research less error-prone, more transparent, and sustainable. Being open-source, there are no restrictions as to who can run R code and older versions are available ensuring that exact reproduction is possible, even years later. As many other language scientists use R, you will be able to collaborate with others and understand other researchers‚Äô R code. As we will see in a future chapter, in RStudio, it is also very easy to export R code and share your scripts, for example as part of an appendix to your research publication, in various formats (including .html that can be opened in any browser and .pdf).\nIn addition, learning to code in R is an excellent way to understand the basics of data literacy and statistical reasoning. These are skills that are highly valued among employers, both in academia and the industry. Many companies, public institutions (e.g., ministries, hospitals, national agencies) and NGOs hire data scientists who often work in R. And, even if you end up doing little to no coding yourself, understanding the basic principles of programming is undoubtedly a highly useful skill in the modern world.\n\n\n\n\n\n\nWhat about learning Python instead? üêç\n\n\n\n\n\nSome of you may be wondering whether you should be learning Python rather than R. Both are widely used languages in scientific programming and data science. At the time of writing, there are more resources specifically aimed at linguists and education researchers in R than there are in Python simply because it is currently the most widely used language in these disciplines. Should you wish to learn Python at a later stage, many of the same principles that you will have learned in this textbook will apply: it should feel somewhat like learning Italian when you already speak Spanish or French.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#installing-r-and-rstudio",
    "href": "4_InstallingR.html#installing-r-and-rstudio",
    "title": "4¬† Installing R and RStudio",
    "section": "4.2 Installing R and RStudio",
    "text": "4.2 Installing R and RStudio\n\n4.2.1 What are R and RStudio? And why do I need both?\nAs a beginner, it‚Äôs easy to confuse R and RStudio, but it‚Äôs important to understand that they are two very different things. R is a programming environment for statistical computing and graphics that uses the programming language R. Think of it as the engine with which we will learn to perform lots of different tasks. RStudio, by contrast, is a set of tools, a so-called ‚Äòintegrated development environment‚Äô (IDE). It makes working in R much more intuitive and efficient. If R is the engine of our car, you can imagine RStudio as our dashboard. Hence, even though we will later on appear to only be working in RStudio, R will actually be doing the heavy-lifting, under the hood.\n\n\n\n\n\n\n\n\n\n\n\n(a) Logo of the programming language and environment R\n\n\n\n\n\n\n\n\n\n\n\n(b) Logo of the IDE RStudio (RStudio¬Æ is a trademark of Posit Software, PBC)\n\n\n\n\n\n\n\nFigure¬†4.3: Even the two logos are easy to confuse, but remember that R and RStudio are two very different things!\n\n\n\n\n\n\n\n\n\nUsing other IDEs to work in R\n\n\n\n\n\nAt the time of writing, RStudio is the most widely used Integrated Development Environment (IDE) to work in R. However, it is worth noting that many other IDEs that can be used to access R. These include:\n\nJupyter notebook\nVisual Studio Code\nPyCharm\nEclipse\n\nWhilst this textbook will assume that everyone is working in RStudio, if you are already familiar with another IDE that works well with R, you are welcome to continue working in that IDE. Each IDE has a different feel to it and offers different functions so, ultimately, it‚Äôll be up to you to find the one that suits you best!\n\n\n\n\n\n4.2.2 Installing R\n\nGo to the website of the Comprehensive R Archive Network (CRAN): https://cran.r-project.org.\nClick on the ‚ÄúDownload R for ‚Ä¶‚Äù link that matches your operating system (Linux, macOS or Windows), then:\n\nFor Windows, click on the top ‚Äòbase‚Äô link, also marked as ‚Äúinstall R for the first time‚Äù (Note that you should also use this link if you are updating your R version). On the next page, click on the top ‚ÄúDownload R‚Äù link.\nFor MacOS, click on either the top .pkg link if you have an Apple silicon Mac (e.g., M1, M2, M3) or the second .pkg link, if you have an older Intel Mac.\nFor Linux, click on your Linux distribution and then follow the instructions on the following pages.\n\n\n\n\nOnce you have downloaded one of these R versions, navigate to the folder where you have saved it (by default, this will be your Downloads folder), and double click on the executable file to install R.\nFollow the on-screen instructions to install R.\nTest that R is correctly installed. On Windows and MacOS, navigate to your Applications folder and double click on the R icon. On Linux, open up R by typing R in your terminal. This should open up an R Console. You can type R commands into the Console after the command prompt &gt;. Type the following R code after the command prompt and then press enter: plot(1:10).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Test command in the R Console\n\n\n\n\n\n\n\n\n\n\n\n(b) Resulting plot (note that the proportions of your plot may be different depending on the size of your window)\n\n\n\n\n\n\n\nFigure¬†4.4: Testing R\n\n\n\n‚úÖ If you see the plot above, you have successfully installed and tested R and you can go on to installing RStudio.\n‚ö†Ô∏è If that‚Äôs not the case, make a note of the errors produced (copy and paste them into a text document or take a screenshot) and search for solutions on the internet. It is very likely that many other people have already encountered the same problem as you and that someone from the R community has posted a solution online.\n\n\n\n\n\n\nWhat to do if you cannot get R and/or RStudio working on your computer\n\n\n\n\n\nThe aim of this chapter is to install both R and R Studio on your own computer so that you can write and run your own scripts locally (i.e., on your own computer without the need for an internet connection). In some cases, however, this might not be possible. For example, because the programmes are not available for your operating system, or because you do not have admin rights on your computer, or because your disk is full and you cannot delete anything. None of these situations are ideal to do research, but don‚Äôt give up on learning R: there is an alternative!\nYou can sign up to Posit Cloud. Posit Cloud will allow you to run R in RStudio in a browser (e.g., Firefox or Chrome) without having to install anything on your computer. Although Posit Cloud‚Äôs free plan is limited, it will suffice to learn the contents of this textbook. You will be able to follow the textbook in exactly the same way as everyone else. However, you will need a stable internet connection and you may find that you need to be a bit more patient as things are likely to run a little slower. If you decide to opt for the Posit Cloud solution, create a free account and then go straight to Setting up RStudio.\n\n\n\n\n\n4.2.3 Installing RStudio\nWhen you head over to their website, it may be confusing to you that the company that provides RStudio, Posit, also offers paid-for versions of RStudio and other paying services. Do not worry, we will not need any of these: These are products designed for companies and large organisations. The version of RStudio Desktop that we will be using, however, is completely free and, given that it is open source, even if Posit decided to stop working on this product one day, others in the R community would take over. Such is the beauty of open-source software! ü§ó\n\nHead over to this page https://posit.co/download/rstudio-desktop/ to download the latest version of RStudio Desktop.\nAs you have already installed R, you can jump straight to the section entitled ‚Äú2: Install RStudio‚Äù. The website should have detected which operating system your computer is running on, so that you can most likely simply click on the ‚ÄúDownload RStudio Desktop‚Ä¶‚Äù button. Your download should start straight away.\n\nIf an incorrect operating system is detected, simply scroll down the page to find your operating system and download the corresponding version of RStudio.\n\n\n\n\nOnce you have downloaded RStudio, navigate to the folder where the downloaded file has been saved (by default, this will be your Downloads folder), and double-click on the executable file to install RStudio.\nFollow the on-screen instructions to install RStudio.\n\nIf you run into any issues that you cannot solve with existing online posts, the Posit Community forums are a good place to ask for help.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#setting-up-rstudio",
    "href": "4_InstallingR.html#setting-up-rstudio",
    "title": "4¬† Installing R and RStudio",
    "section": "4.3 Setting up RStudio",
    "text": "4.3 Setting up RStudio\nFrom now on, we will only be accessing R through RStudio. When you open up RStudio for the first time, you might find the layout rather intimidating. The application window is divided into several sections, which we call ‚Äòpanes‚Äô. Each pane also has several tabs. Although it may seem overwhelming at first, you will soon see that these different panes and tabs will actually make life much easier.\n\n4.3.1 Global options\nBefore we get staRted properly, however, we need to change some of the default settings of RStudio. The first set of changes that we are going to make ensure that, each time we launch a new R session in RStudio, we start afresh.\nTo do so, head over to the ‚ÄòTools‚Äô drop-down menu and click on ‚ÄòGlobal Options‚Äô. Make sure that the first three boxes are unticked (see Figure¬†4.5 (a)). Under ‚ÄúSave workspace to .RData on exit‚Äù, select the option ‚ÄúNever‚Äù. Always starting afresh is good programming practice. It avoids any problems being carried over from previous R sessions. You can think of it like cooking in a freshly cleaned, tidy kitchen. It‚Äôs much safer than preparing a meal in a messy, possibly even contaminated kitchen! Or use the keyboard shortcut Ctrl/Command + ,\n\n\n\n\n\n\n\n\n\n\n\n(a) General tab\n\n\n\n\n\n\n\n\n\n\n\n(b) Code tab\n\n\n\n\n\n\n\nFigure¬†4.5: RStudio‚Äôs Global Options\n\n\n\nNext, under the ‚ÄòGlobal Options‚Äô tab ‚ÄòCode‚Äô of the ‚ÄòGlobal Options‚Äô window, ensure that the fourth option ‚ÄúUse native pipe operator‚Äù is ticked (see Figure¬†4.5 (b)). This is a new feature in R that is very useful so we will make use of it in this textbook. The other options are not relevant for now.\nFinally, head over to the ‚ÄòPane Layout‚Äô tab. From here, you can rearrange the panes of your RStudio window. To do so, click on the ÔπÄ symbols to get a drop-down menu corresponding to each pane. You can also select which tabs you would like to see in each pane. If you are already familiar with RStudio, feel free to stick to your favourite set-up. Personally, I use the panes layout below and, if you are new to R, I recommend that you select this layout, too. You can always go back to these settings to change this setup at any stage. Don‚Äôt forget to click on ‚ÄòOK‚Äô at the bottom of the ‚ÄòGlobal Options‚Äô page to save your settings. Then, the panes in your RStudio window should be ordered as in Figure¬†4.6 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Panes Layout tab\n\n\n\n\n\n\n\n\n\n\n\n(b) Customised panes layout\n\n\n\n\n\n\n\nFigure¬†4.6: Recommended RStudio panes layout\n\n\n\n\n\n4.3.2 Testing RStudio\nIt is now time we tested whether RStudio is communicating well with R. To do so, let‚Äôs run the same test as in the R Console. This time, head over to the ‚ÄòConsole‚Äô tab in the top right pane of your RStudio window and, after the command prompt &gt;, type: plot(1:10) and then press enter. You should see the same plot as earlier on (see Figure¬†4.4 (b)), appearing in the Plots tab of the bottom-right pane of your RStudio window.\nIf you get the following error message Error in plot.new() : figure margins too large, this is because your bottom-right pane is hidden from view or too small for the plot to be printed there. Click on the small two-window icon in the bottom-right corner if it is hidden (see Figure¬†4.7 (a)). Or, if it is too small, click on the dividing line between the two right-hand side panes and, whilst still holding down the mouse button, drag up the line until it is about halfway up. Then, re-type the command plot(1:10) in the Console pane and press enter again. The plot should appear as in Figure¬†4.7 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Hidden (minimised) bottom-right pane\n\n\n\n\n\n\n\n\n\n\n\n(b) Now the dividing line between the two panes is halfway up and the plot has been successfully output in the Plots pane\n\n\n\n\n\n\n\nFigure¬†4.7: Testing that RStudio is communicating well with your R installation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#installing-r-packages",
    "href": "4_InstallingR.html#installing-r-packages",
    "title": "4¬† Installing R and RStudio",
    "section": "4.4 Installing R packages",
    "text": "4.4 Installing R packages\n\n4.4.1 What are packages?\nYou now have a base installation of R. Base R is very powerful and comes with many standard packages and functions that R users use on a daily basis. If you click on the Packages tab in the bottom-right pane and scroll down, you will see that there are many packages available. Only a few are selected. These are part of the base R installation.\nYou can think of base R as a fully functional student kitchen. It is rather small and only has the most essential ingredients and equipment, but it still has everything you need to cook simple, delicious meals. Downloading and installing additional packages is like buying fancier ingredients (i.e., packages with datasets) or more sophisticated and specialised kitchen devices (i.e., packages that provide additional functions).\nIn addition to the members of the R Core Team who develop and maintain base R, thousands of R users develop and share additional R packages every day. These enable us to vastly increase the capacities of base R. Packages are a very helpful way to bundle together a set of functions, data, and documentation files so that other R users can easily download these bundles and add them to their local R installation.\nThroughout this textbook, the names of packages will be enclosed in curly brackets like this: {ggplot2}.\n\n\n\n\n\n\nQuiz time!\n\n\n\n1) Which of these packages is not part of base R?\n\n\n\n\n{graphics}\n\n\n{stats}\n\n\n{ggplot2}\n\n\n{datasets}\n\n\n\n\n\n\n\n¬†\n2) Is it possible to create an R package that provides access to the full texts of all of Jane Austen‚Äôs published novels for computational text analysis in R?\n\n\n\n\nYes, pretty much anything is possible in R!\n\n\nNo way, that sounds impossible!\n\n\n\n\n\n\n\n¬†\n3) Is the {janeaustenr} package installed as part of base R?\n\n\n\n\nYes\n\n\nNo\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Installing packages\nTo install a package, you will first need to download it from the internet. Packages are typically stored on different websites (online repositories), but the most trustworthy one and easiest to work with is CRAN (Comprehensive R Archive Network). To install the {janeaustenr} package from CRAN, simply type the following command in the Console pane and then type enter: install.packages(\"janeaustenr\").\nThis command will take a few seconds to run (or longer depending on how slow your internet connection is). You should then see a message in red in the Console indicating (among other things that you can ignore) that the package has been successfully downloaded and its size (here: 1.5 megabyte), as well as the path to where the package‚Äôs content has been saved on your computer (see Figure¬†4.8). You do not need to worry about any of the other information.\n\n\n\n\n\n\nFigure¬†4.8: Screenshot showing that the package has been correctly installed.\n\n\n\nTo check that the package has been successfully downloaded and installed, head over to the Packages tab of the bottom-right pane and scroll down to the {janeaustenr} package, or search for it using the search window within this same tab. The {janeaustenr} package should now be visible, which tells us that the package is installed on your computer. Note, however, that the checkbox next to it is currently empty. This means that the package hasn‚Äôt been loaded in our current R session and therefore cannot be used yet.\n\n\n\n\n\n\nMore ways of installing R packages\n\n\n\n\n\nThere are other ways to install packages, e.g., from Bioconductor and GitHub.\nTo find out more, read Section 1.5 from Douglas et al. (2024), which is available as an Open Educational Resource (see Chapter 1).\n\n\n\n\n\n4.4.3 Loading packages\nIf you want to use the fancy ingredient or new kitchen device that was delivered in the package that you installed, you first need to get it out of the fridge or the cupboard and place it on your kitchen counter. This is the equivalent of ‚Äúloading‚Äù a package. The command to load a package is library(). This is because, rather confusingly, once they are unpacked (i.e., installed), packages are usually referred to as libraries.\nTo load the {janeaustenr} package, enter the following command in the Console:\n\nlibrary(janeaustenr)\n\nIf you correctly installed the package and have not misspelt the command, it may look like nothing has happened, as the Console returns nothing (see first red ellipse on Figure¬†4.9). However, if you go back to your Packages tab and scroll down to the {janeaustenr} package, you will see that the box next to it is now ticked (see second ellipse on Figure¬†4.9). This means that the package is loaded and ready to be used.\n\n\n\n\n\n\nFigure¬†4.9: Loading a library\n\n\n\nNote that whilst you only need to install each package once, you will need to load it every time we want to use it in a new R session. This is because, when we close Rand start a new R session, our kitchen is perfectly clean and tidy and everything is back in storage. And the good news is that we don‚Äôt even need to do the washing-up! üôÉ",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#package-documentation",
    "href": "4_InstallingR.html#package-documentation",
    "title": "4¬† Installing R and RStudio",
    "section": "4.5 Package documentation",
    "text": "4.5 Package documentation\nTo find out more about any package or function, simply use the command help() or its shortcut ?. For example, to find out more about the {janeaustenr} package, enter the command help(janeaustenr) or ?janeaustenr in the Console. The help file will open up in the Help tab of the bottom-right pane. It contains the name of the package and a short description, as well as the name of the package maintainer, Julia Silge, and some additional links.\nOne of these links takes us to the package creator‚Äôs GitHub repository. This is where we can find a source code for the package, should we want to check how it works under the hood, or amend it in any way. Click on this link and scroll down the package‚Äôs GitHub page to consult its README file. This document informs us that the package includes plain text versions of Jane Austen‚Äôs six completed, published novels and tells us under what name they are stored within the library. For example, to access Pride and Prejudice, we need to load the library object prideprejudice. Note that the object names in R cannot contain spaces or hyphens.\nPick your favourite Jane Austen novel and enter its corresponding object name in the Console, e.g., emma. The entire novel will be printed in the Console output! You can print only a few lines by selecting them within square brackets, e.g., the command emma[20:25] will only print lines 20 to 25 of the object emma (see Figure¬†4.10).\n\n\n\n\n\n\nFigure¬†4.10: Screenshot showing a selection of lines from the object emma (note that you can adjust the size of the Console pane to see more or less of the text at any one time).\n\n\n\nTo find out more about a dataset or function within a package, use the functions help() or ?, e.g., help(emma) or ?emma. In this case, the help file provides us with a short description of this object and a link to the original source from which the package creator obtained the novel (which is in the public domain, otherwise it would not be possible to share it in this way).\n\n\n\n\n\n\nQuiz time!\n\n\n\n4) Which is the correct R object name to access Jane Austern‚Äôs novel ‚ÄòSense and Sensibility‚Äô?\n\n\n\n\nsenseandsensibility\n\n\nSense&Sensibility\n\n\nSense and Sensibility\n\n\nSensesensibility\n\n\nsensesensibility\n\n\n\n\n\n\n\n¬†\n5) What is the source for the R object containing Jane Austern‚Äôs novel ‚ÄòSense and Sensibility‚Äô?\n\n\n\n\nhttps://jasna.org/austen/works/sense-sensibility/\n\n\nhttps://www.goodreads.com/book/show/14935.Sense_and_Sensibility\n\n\nhttps://en.wikipedia.org/wiki/Sense_and_Sensibility\n\n\nhttp://www.gutenberg.org/ebooks/161\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n6) What is the first word of the 66th line in the R object containing Jane Austern‚Äôs novel ‚ÄòSense and Sensibility‚Äô?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#citing-r-packages",
    "href": "4_InstallingR.html#citing-r-packages",
    "title": "4¬† Installing R and RStudio",
    "section": "4.6 Citing R packages",
    "text": "4.6 Citing R packages\nWhen we use a package that is not part of base R, it is very important to reference the package properly. There are two main reasons for doing this. For a start, the people who create and maintain these packages largely do so in their free time and they deserve full credit for their incredibly valuable work and contribution to science. Hence, whenever you use a package for your research, you should cite it, just like you would other sources.\nThe help page of the {janeaustenr} package already informed us that the maintainer of the package is Julia Silge. To get a full citation, however, we should use the citation() function. Enter citation(\"janeaustenr\") in the Console to find out how to cite this package.\nNote that the recommended bibliographic reference also includes the package version, which is important for reproducibility as the package may evolve and someone wanting to reproduce your analysis (and this may well be future you!) will need to know which version you used. This is the second main reason why we should be diligent about citing the packages that we used. In a research report, thesis, or academic article, you could cite the {janeaustenr} package like this:\n\nWe used the janeaustenr package (Silge 2022) to access Jane Austen‚Äôs six published novels in R (R Core Team 2024).\n\nYou can see the full references by hovering on the in-text citation links or by going to the References section of this book.\n\n\n\n\n\n\nMore about referencing packages\n\n\n\n\n\nYou may also want to install the {report} package, which includes a number of useful functions for citing R versions and R packages:\n\nreport::report_system()\n\nAnalyses were conducted using the R Statistical language (version 4.4.1; R Core\nTeam, 2024) on macOS Sonoma 14.5\n\nreport::cite_packages()\n\n  - Makowski D, L√ºdecke D, Patil I, Th√©riault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. &lt;https://easystats.github.io/report/&gt;.\n  - Moroz G (2020). _Create check-fields and check-boxes with checkdown_. &lt;https://CRAN.R-project.org/package=checkdown&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Silge J (2022). _janeaustenr: Jane Austen's Complete Novels_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=janeaustenr&gt;.\n  - Xie Y (2024). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.47, &lt;https://yihui.org/knitr/&gt;. Xie Y (2015). _Dynamic Documents with R and knitr_, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;https://yihui.org/knitr/&gt;. Xie Y (2014). \"knitr: A Comprehensive Tool for Reproducible Research in R.\" In Stodden V, Leisch F, Peng RD (eds.), _Implementing Reproducible Computational Research_. Chapman and Hall/CRC. ISBN 978-1466561595.\n\nreport::report_packages()\n\n  - report (version 0.5.9; Makowski D et al., 2023)\n  - checkdown (version 0.0.12; Moroz G, 2020)\n  - R (version 4.4.1; R Core Team, 2024)\n  - janeaustenr (version 1.0.0; Silge J, 2022)\n  - knitr (version 1.47; Xie Y, 2024)\n\n\nTo find out more, it is also worth reading Steffi LaZerte‚Äôs blog post on ‚ÄúHow to cite R and R packages‚Äù: https://ropensci.org/blog/2021/11/16/how-to-cite-r-and-r-packages/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "4_InstallingR.html#keeping-things-up-to-date",
    "href": "4_InstallingR.html#keeping-things-up-to-date",
    "title": "4¬† Installing R and RStudio",
    "section": "4.7 Keeping things up to date ‚ú®",
    "text": "4.7 Keeping things up to date ‚ú®\nAs with all software, it is a good idea to keep your installations of RStudio and R up-to-date. New features are constantly being added, bugs are fixed and updates may include important security patches.\n\n4.7.1 Updating RStudio\nBy default, RStudio will let you know when a new version is available in a pop-up window. To update RStudio simply follow the same instructions as for the first installation (see Section 4.2.3). This time, however, when you add RStudio to your apps, you will get a pop-up message warning you that an older version of this programme already exists on your computer (see Figure¬†4.11). You can safely click on the option ‚ÄúReplace‚Äù. All of your previous Global Options settings will be transferred to your updated RStudio version so this should be a quick-and-easy process.\n\n\n\n\n\n\nFigure¬†4.11: Warning message on MacOS when installing an updated version of RStudio\n\n\n\nYou can also check which version of RStudio you are running by clicking on the ‚ÄúHelp‚Äù menu in RStudio‚Äôs top toolbar and then selecting the option ‚ÄúAbout RStudio‚Äù. In the ‚ÄúHelp‚Äù drop-down menu, you also have an option to ‚ÄúCheck for Updates‚Äù.\n\n\n4.7.2 Updating R\nUpdating R is a little more complex because you will also need to update all of your R packages. Some of the packages that you use may not (yet) be available for the latest R version. This is why, for beginners, I do not recommend updating R in the middle of a project. That said, it is a good idea to keep your R version up-to-date. To find out which version of R you are currently working with, run this command in the Console.\n\nR.version.string\n\n[1] \"R version 4.4.1 (2024-06-14)\"\n\n\nCompare this version number with the number of the latest version available on CRAN (see Figure¬†4.12). If the version that you are running is not the same as the latest R version available on CRAN, you might want to update it. As a rule of thumb, it is a good idea to do an update if your version is more than six months old. To proceed with the update, close RStudio on your computer. Then, follow the same instructions as for the first-time installation of R (see Section 4.2.2).\n\n\n\n\n\n\nFigure¬†4.12: CRAN R for macOS page using the latest recommended R version\n\n\n\n\n\n4.7.3 Updating R packages\nOnce you have updated R, it is important that you also update your installed packages. To do so, run the following command in the Console:\n\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\nAlternatively, you can also go to the Packages tab of RStudio and click on the button ‚ÄúUpdate‚Äù. A pop-up window will appear with a list of the packages that need updating. Click on ‚ÄúSelect All‚Äù and ‚ÄúInstall Updates‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n(a) ‚ÄòUpdate‚Äô button in RStudio‚Äôs Packages tab\n\n\n\n\n\n\n\n\n\n\n\n(b) ‚ÄòUpdate Packages‚Äô dialogue in RStudio\n\n\n\n\n\n\n\nFigure¬†4.13: Updating packages using RStudio‚Äôs Graphical User Interface (GUI)\n\n\n\nNote that, if you have installed a lot of packages, this updating operation could take a while. It requires a stable internet connection and a bit of patience. üßòüèæ\n\n\n\n\n\n\nAn easier way to update R using {installr} (for Windows only)\n\n\n\n\n\nThe {installr} package simplifies updating R on Windows. To install the package use the usual commands:\n\ninstall.packages(\"installr\") # Run this command the first time you use the package.\n\nlibrary(installr) # Run this command everytime you want to update R using this package.\n\nThen, run the updateR() function, which automates the updating process by detecting your current R version, comparing it with the latest available version, and guiding you through the process of downloading and installing the latest version.\nIt is also possible to customise the update process with arguments like updateR(update_packages = FALSE) to skip package updates. For more details, check the documentation using the command ?updateR.\n\n\n\n\n\n\n\nDouglas, Alex, Deon Roos, Francesca Mancini & David Lusseau. 2024. An introduction to R. https://intro2r.com/.\n\n\nR Core Team. 2024. R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen‚Äôs complete novels. https://CRAN.R-project.org/package=janeaustenr.\n\n\nWinter, Bodo. 2019. Statistics for linguists: An introduction using R. Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Installing `R` and RStudio</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html",
    "href": "5_GettingStaRted.html",
    "title": "5¬† Getting staRted",
    "section": "",
    "text": "Chapter overview\nNow that you have installed and tested R and RStudio, in this chapter, you will learn how to:\nIf you are already familiar with the basics of R and are keen to learn more about doing statistics in R, you can skip most of this chapter. That said, it‚Äôs probably not a bad idea to have a go at the quiz questions and the final task to refresh your memory.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#using-the-console",
    "href": "5_GettingStaRted.html#using-the-console",
    "title": "5¬† Getting staRted",
    "section": "5.1 Using the Console",
    "text": "5.1 Using the Console\nThis is what you did in the previous chapter when you tested that RStudio was working properly (using the command: plot(1:10)).\nOne way to write R code in RStudio is to use the Console. If you set up RStudio as recommended here, the Console should be in your top-right pane. You can type a line of code immediately after the command prompt &gt; and press ‚ÄúEnter‚Äù.\nData input is the most basic operation in R. Try inputting a number by typing it out in the Console and then pressing ‚ÄúEnter‚Äù. R will interpret the number and return it. You can input both integers (whole numbers, e.g., 13) and decimal numbers (e.g., 0.5).\n\n\n\n\n\n\nFigure¬†5.1: Inputting numbers in the Console\n\n\n\nR can handle not only numbers but also text data, known as ‚Äúcharacter strings‚Äù or just ‚Äústrings‚Äù. Strings must always be enclosed in quotation marks. You can choose to use either double quotation marks \" \" or single quotation marks ' ', but it is important to be consistent. In this textbook, we will use double quotation marks throughout.\nTry first inputting a single word and then an entire sentence in the Console.\n\n\n\n\n\n\nFigure¬†5.2: Inputting strings in the Console\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\n1) What happens if you enter a word without quotation marks?\n\n\n\n\nR returns an error indicating that you probably mistyped the word.\n\n\nR returns an error message because it interprets the word as an object name or command.\n\n\nR returns an error message indicating that it expected a number.\n\n\nR automatically wraps the word in quotation marks and processes it as a string.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-Maths",
    "href": "5_GettingStaRted.html#sec-Maths",
    "title": "5¬† Getting staRted",
    "section": "5.2 Doing maths in R",
    "text": "5.2 Doing maths in R\nR can also be used as a very powerful calculator. The lines of code in Figure¬†5.3 demonstrate mathematical operations involving addition (+), subtraction (-), division (/), and multiplication (*). Try out a few yourself!\n\n\n\n\n\n\nFigure¬†5.3: Using the R Console as a calculator\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\n2) Try entering 13^2 in the Console. What does the ^ (caret) operator do?\n\n\n\n\nThe ^ operator performs an exponentiation operation, here 13 to the power of 2.\n\n\nThe ^ operator calculates the modulus of a number, here of 13 with 2 as the base.\n\n\nThe ^ operator creates a vector, here with 13 occurrences of the integer 2.\n\n\n\n\n\n\n\n¬†\n3) Compare 13*13 with 13 * 13. What is the difference in the output?\n\n\n\n\nThere is no difference.\n\n\nAdding a space generates an error.\n\n\nIt is impossible to add a space in the R Console.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-WorkingRObjects",
    "href": "5_GettingStaRted.html#sec-WorkingRObjects",
    "title": "5¬† Getting staRted",
    "section": "5.3 Working with R objects",
    "text": "5.3 Working with R objects\nSo far, we have used the Console like a calculator. It‚Äôs important to understand that, just like with a standard calculator, the output of all of our operations was not saved anywhere. If we want to store values, sequences of values, and the results of computations for later use, R allows us to store these as ‚ÄúR objects‚Äù.\n\n5.3.1 Creating objects\nWe use the assignment operator (&lt;-) to assign a value or sequence of values to an object name.\nWrite out the following line to create an object called my.favourite.number that contains your own favourite number.\n\nmy.favourite.number &lt;- 13\n\nWhen you enter this line in the Console and press ‚ÄúEnter‚Äù, it should look like nothing happened: R does not return anything in the Console. Instead, it saves the output in an object called my.favourite.number. However, if you look in your Environment pane, you should see that an object has appeared (Figure¬†5.4).\n\n\n\n\n\n\nFigure¬†5.4: Created object in the Environment pane\n\n\n\nTo save an object containing a character string, we use quotation marks. Create an object called my.favourite.word containing your favourite word (in any written language of your choice).\n\nmy.favourite.word &lt;- \"empathy\"\n\nYour Environment pane should now contain two objects. You can print the content of a stored object by entering the object name in the Console and then pressing ‚ÄúEnter‚Äù (see Figure¬†5.5).\nüí° Tip: If you‚Äôre feeling lazy or simply want to avoid making a typo, you can type just the first few letters of an object name and then press the ‚ÄúTab‚Äù key (‚Üπ or ‚á•). RStudio will then give you a drop-down menu with possible options. Select the one you want by clicking on it or pressing ‚ÄúEnter‚Äù.\n\n\n\n\n\n\nFigure¬†5.5: Calling up stored objects in the Console to view their content\n\n\n\n\n\n5.3.2 Object types\nThese two objects are of different types. We can use the class() function to find out which type of object an object is.\n\n\n\n\n\n\nFigure¬†5.6: Using the class() function\n\n\n\nHere, my.favourite.number is a numeric object, while my.favourite.word is a character object.\n\n\n5.3.3 Naming objects\nObject naming conventions in R are fairly flexible. We can use dots (.), underscores (_) and capital letters to make our object names maximally informative and easy for us humans to read. However, spaces and other symbols are not allowed. All of these options work:\n\nword2 &lt;- \"cheerful\"\nmy.second.word &lt;- \"cheerful\"\nmy_second_word &lt;- \"cheerful\"\nMySecondWord &lt;- \"cheerful\"\n\n\n\n\n\n\n\nFigure¬†5.7: Environment pane showing all of the objects currently stored in the R session environment\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\n4) Which of these object names are not allowed in R? Try to create an object with each of these names and see if you get an error message or not.\n\n\n\n\nBestWordEver!\n\n\nmy-favourite-word\n\n\n1TopWord\n\n\nTop1Word\n\n\nAgn√®s.Favourite.Word\n\n\ntop word\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n\n\nObject names should not contain spaces or symbols like !, nor should they contain hyphens as the hyphen is reserved for the mathematical operator ‚Äúminus‚Äù. Digits can be used anywhere except at the beginning of an object name. And whilst it is possible to have special characters such as accented letters like ‚Äú√®‚Äù, it is not recommended that you use them for object names.\n\n\n5.3.4 Overwriting and deleting objects\nObject names are unique. If you create a new object with an existing object name, it will overwrite the existing object with the new one. In other words, you will lose the values that you saved in the original object. Try it out by running this line and observing what happens in your Environment pane:\n\nword2 &lt;- \"surprised\"\n\nEarlier on, you created an object called word2 which contained the string ‚Äúcheerful‚Äù. But, by running this new line of code, ‚Äúcheerful‚Äù has been replaced by the string ‚Äúsurprised‚Äù - with no warning that you were about to permanently delete ‚Äúcheerful‚Äù! üò≤\nThe command to delete a single object from your environment is remove() or rm(). Hence, to permanently delete the object MySecondWord, you can use either of these commands:\n\nremove(MySecondWord)\nrm(MySecondWord)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#working-with-.r-scripts",
    "href": "5_GettingStaRted.html#working-with-.r-scripts",
    "title": "5¬† Getting staRted",
    "section": "5.4 Working with .R scripts",
    "text": "5.4 Working with .R scripts\nIf we shut down RStudio right now, we will lose all of our work so far. This is because the objects that we have created are only saved in the environment of our current R session. Whilst this might sound reckless, it is actually a good thing: In Section 4.3.1 we set our ‚ÄòGlobal Options‚Äô settings in RStudio such that, whenever we restart RStudio, we begin with a clean slate, or a perfectly clean and tidy kitchen. We don‚Äôt want any dirty dishes or stale ingredients lying around when we enter the kitchen! With this in mind, close RStudio now and open it again to start a new R session.\nYou should now have an empty history in your Console pane and an empty Environment pane. Whilst nobody wants to start cooking in a messy kitchen, it‚Äôs also true that, if we want to remember what we did in a previous cooking/baking session, we should write it down. The pages of our recipe book are .R scripts. In the following, we will see that writing scripts is much better than running everything from the Console. It allows us to save and rerun our entire analysis pipeline any time we want. It also ensures that our analyses are reproducible and saves us time as we don‚Äôt have to rewrite our code every time. Crucially, if we made a mistake at any stage, we can go back and correct it and rerun the entire corrected script at the click of a button.\n\n5.4.1 Creating a new .R script\nThere are three ways to create a new .R script in RStudio. Pick the one that you like best:\n\nNavigate to the top menu item ‚ÄúFile‚Äù, then select ‚ÄúNew File‚Äù, then click on ‚ÄúR Script‚Äù.\nClick on the icon with a white page and a green plus button in the top left corner of the tool bar.\nUse the keyboard shortcut Shift + Ctrl/Cmd + N.\n\nWhichever option you chose, RStudio should have opened an empty file in a fourth pane (see Figure¬†5.8). This is the ‚ÄúSource pane‚Äù and it should have appeared in the top-left corner of your RStudio window.\n\n\n\n\n\n\nFigure¬†5.8: RStudio window showing a new, empty .R script that has yet to be saved\n\n\n\n\n\n5.4.2 Running code from an .R script\nWe can now type our code in this empty .R script in the Source pane, just like we did in the Console. Type the following lines of code in the script (see Figure¬†5.9):\n\n13*13\nmy.favourite.number &lt;- 13\nmy.favourite.word &lt;- \"empathy\"\n\nYou will have noticed that when you pressed ‚ÄúEnter‚Äù after every line, nothing happened: Nowhere can we see the result of 13*13, nor have our two objects been saved to the environment as the Environment pane remains empty (see Figure¬†5.9). Just like a recipe for a cake is not an actual, delicious cake, but simply a set of instructions, a script is only a text file that contains lines of code as instructions. For these instructions to be executed, we need to send them to the R Console where they will be interpreted as R code.\n\n\n\n\n\n\nFigure¬†5.9: Writing code in an .R script\n\n\n\nTo send a line of code to the Console (also referred to as ‚Äúrunning‚Äù a line of code), select the line that you want to excecute, or place your mouse cursor anywhere within that line and then click on the ‚ÄúRun‚Äù button (in the top-right corner of the pane, see Figure¬†5.8) or use the keyboard shortcut Ctrl/Cmd + Enter.\nTry out these two options to run the three lines of code of your script and check that a) you are seeing the result of the mathematical operation in the Console output and b) two objects have been added to your environment.\n\n\n5.4.3 Saving an .R script\nIt is now very easy to rerun this script any time we want to redo this calculation and recreate these two R objects. However, our .R script is not yet saved! RStudio is warning us about this by highlighting the file name ‚ÄúUntitled1*‚Äù in red (see Figure¬†5.8). Just like with any unsaved computer file, if we were to shut RStudio down now, we would lose our work. So, let us save this .R script locally, that is on our own computer. To do so either:\n\nNavigate to the top menu item ‚ÄúFile‚Äù and then click on ‚ÄúSave‚Äù,\nClick on the save icon üíæ, or\nUse the keyboard shortcut Ctrl/Cmd+ S.\n\nGive your script a meaningful file name. Remember that file names should be both computer-readable and human-readable. If you navigate to the folder where you saved your .R script, you should see that its file extension is .R. You should also see that it is a tiny file because it contains nothing more than a few lines of text. If you double click on an .R file, RStudio should automatically open it. However, if you wanted, you could open .R files with any text-processing software, such as LibreOffice Writer or Microsoft Word.\n\n\n5.4.4 Writing comments in scripts\nJust like in a recipe book, in addition to writing the actual instructions, we can also write some notes, for example to remind ourselves of why we did things in a particular way or for what occasion we created a special dish. In programming, notes are called ‚Äúcomments‚Äù and they are typically preceded by the # symbol.\nThus, if a line starts with a # symbol, we say that it is ‚Äúcommented out‚Äù. RStudio helpfully displays lines that are commented out in a different colour. These lines will not be interpreted as code even if you send them to the Console. Write the following lines in your script and then try to run them.\n\n#13^13\n\n#StringObject3 &lt;- \"This line has been commented out so the object will not be saved in the environment even if you try to run it.\"\n\nAs you can see, nothing happens. You can also add comments next to a line of interpretable code. In this case, the code is interpreted up until the #. This can be helpful to make a note of what a line of code does, e.g.:\n\nsqrt(169) # Here the sqrt() function will compute the square root of 169.\n\n[1] 13\n\n\nIt is good practice to comment your code when working in an .R script. Comments are crucial for other people to understand what your code does and how it achieves that. But even if you are confident that you are the only person who will ever use your code, it is still a very good idea to use comments to make notes documenting your intentions and your reasoning as you write your script.\nFinally, writing comments in your code as you work through the examples in this book is a great way to reinforce what you are learning. From this chapter onwards, I recommend that, for each chapter, you create an .R script documenting what you have learnt, adding lots of comments to help you remember how things work. This is generally more efficient (and less error-prone!) than trying to take notes in a separate document (e.g., in a Microsoft Word file) or on paper.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-RelationalOperators",
    "href": "5_GettingStaRted.html#sec-RelationalOperators",
    "title": "5¬† Getting staRted",
    "section": "5.5 Using relational operators",
    "text": "5.5 Using relational operators\nNow that we have saved some objects in our environment, we can use them in calculations. Try out the following operations (and any other that take your fancy) with your own favourite number:\n\nmy.favourite.number / 2\n\n[1] 6.5\n\nmy.favourite.number * my.favourite.number\n\n[1] 169\n\n\nIn additional to the mathematical operations that we saw in Section 5.2, we can also use relational operators such &gt;, &lt;, &lt;=, &gt;=, == and != to make all kinds of comparisons. Try out the following commands to understand how these relational operators work and then have a go at the quiz questions.\n\nmy.favourite.number &gt; 10\nmy.favourite.number &lt; 10\nmy.favourite.number == 25\nmy.favourite.number &gt;= 13\nmy.favourite.number &lt;= -13\nmy.favourite.number != 25\n\n\n\n\n\n\n\nQuiz time!\n\n\n\n5) What is the relational operator that checks whether a value is ‚Äúmore than or equal to‚Äù another value?\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n6) What is the relational operator that checks whether a value ‚Äúis not equal to‚Äù another value?\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n\n\nThe relational operators == and != can also be used with character objects. Find out how they work by first creating a new character object with a word that was added to the 2025 edition of the popular French dictionary Petit Larousse:\n\nNew.French.Word &lt;- \"√©cogeste\"\n\nThen copy these lines of code to test how these relational operators work with string characters.\n\nNew.French.Word == \"√©cogeste\" \nNew.French.Word != \"trottinettiste\"\n\nYou will have noticed that the relational operator == tests whether two strings are the same and returns TRUE if that‚Äôs the case. In contrast, != tests whether two strings are different and will therefore return FALSE if they are not different.\n\n\n\n\n\n\nQuiz time!\n\n\n\nAbove, we created the following R character string object:\n\nNew.French.Word &lt;- \"√©cogeste\"\n\n7) Why does this line of code return FALSE even though New.French.Word was assigned the character string ‚Äú√©cogeste‚Äù?\n\nNew.French.Word == \"ecogeste\"\n\n\n\n\n\nBecause \"√©cogeste\" and \"ecogeste\" are two different strings in R.\n\n\nBecause R automatically removed the accent as object names must be in English.\n\n\nBecause == cannot be used to compare character strings in French.\n\n\n\n\n\n\n\n¬†\n8) Why does this line of code return FALSE?\n\nNew.French.Word == \" √©cogeste\"\n\n\n\n\n\nBecause string objects cannot include any special characters. This includes spaces.\n\n\nBecause R is case-sensitive.\n\n\nBecause this string includes an additional space character.\n\n\n\n\n\n\n\n¬†\n9) Why does this line of code return FALSE?\n\nNew.French.Word == \"√âcogeste\"\n\n\n\n\n\nBecause this word is not in the dictionary of the Acad√©mie Fran√ßaise.\n\n\nBecause R is case-sensitive.\n\n\nBecause strings should never start with a capital letter.\n\n\n\n\n\n\n\n¬†\n10) Why does this line of code return FALSE?\n\nNew.French.Word != \"√©cogeste\"\n\n\n\n\n\nBecause √©cogeste is no longer a new French word.\n\n\nBecause this string is in a different text encoding.\n\n\nBecause this command asks whether New.French.Word is not equal to \"√©cogeste\".",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "5_GettingStaRted.html#sec-Errors",
    "href": "5_GettingStaRted.html#sec-Errors",
    "title": "5¬† Getting staRted",
    "section": "5.6 Dealing with errors ü§¨",
    "text": "5.6 Dealing with errors ü§¨\nWhen R cannot interpret your code, the Console will display an error message in red. A large part of learning to code is really about learning how to interpret these error messages and developing an intuition for the most common reasons why errors occur.\n\n\n\n\n\n\nFigure¬†5.10: The process of fixing programming errors is called ‚Äúdebugging‚Äù and often involves an array of emotions (artwork by @allison_horst).\n\n\n\nAs you begin your journey of learning to code in R, you are very likely to encounter one problem on a regular basis. So let‚Äôs take a closer look at that error. Copy and paste this exact line of code and try to run it in your R Console:\n\nsqrt(my.favourite.number\n\nNotice that, in this erroneous line of code, we have (intentionally) forgotten to include the final bracket. As a result, after you hit ‚ÄúEnter‚Äù, the Console output shows a ‚Äú+‚Äù instead of the result of the mathematical operation. The ‚Äú+‚Äù indicates that the line is incomplete and therefore cannot be interpreted yet. R is therefore asking you to complete your line of code.\n\n\n\n\n\n\nFigure¬†5.11: Incomplete function in console\n\n\n\nThere are two ways to fix this. The first method is to complete the line of code directly in the Console. In this case, this means adding the closing bracket ‚Äú)‚Äù after the ‚Äú+‚Äù and hitting ‚ÄúEnter‚Äù. Now that the line has been completed, R is able to interpret it as an R command and will output the result of the operation.\nIf you are running a line of code just once, from the Console, this first method is fine. As we have seen above, however, most of the time, you will write your code in a script rather than in the Console. So this first on-the-fly method is only recommendable for lines of code that you will genuinely only need once. These include commands to install packages, like install.packages(\"janeaustenr\"), or to consult documentation files, e.g., help(janeaustenr).\nGiven that we will mostly be working in scripts, let‚Äôs now generate this error from an .R script. To do so, copy and paste the erroneous line of code in your .R script and try to run it by either clicking on the ‚ÄúRun‚Äù icon or using the shortcut Ctrl/Cmd + Enter:\n\nsqrt(my.favourite.number\n\nAgain, our incomplete line of code cannot be interpreted and the ‚Äú+‚Äù symbol appears in the Console. Now, correct the error in your script by adding the missing ‚Äú)‚Äù and try to run the command again:\n\nsqrt(my.favourite.number)\n\nEven though we have corrected the problem, we now get an error! ü§Ø At first sight, this does not make sense, but look carefully at what happened in the Console: The line of code that R tried to interpret is sqrt(my.favourite.number + sqrt(my.favourite.number), i.e., the combination of the incomplete version of the command plus the complete one. This is obviously nonsense and R tells us so by outputting an error message!\n\n\n\n\n\n\nFigure¬†5.12: Error message in console\n\n\n\nTo be able to enter a new line of code, we must see the command prompt &gt; in the Console. So, let‚Äôs generate the error again and learn how to fix it with the second method. Add this erroneous line to your script again and run it:\n\nsqrt(my.favourite.number\n\nThe + situation arises again, but we will now solve it using the second method. Head over to the Console and place your cursor next to the +. This time, instead of completing the line by adding a closing bracket, press ‚ÄúEsc‚Äù on your keyboard. This will cancel the incomplete line of code. Then, you can add the missing ) in your script and rerun the newly completed line of code from the Source pane.\nThis second method is the one you should use when you are documenting your code in a script. If you don‚Äôt make the changes immediately in your script, you will forget and you will run into this error again in the future. Think of it like a pastry chef who realises that they need to put a little more baking powder in a cake batter for the texture to be just right, but does not make a note of that change in their recipe book. Next time, the pastry chef will likely forget and not put the correct amount of baking powder. If it is one of their assistants who prepares the cake, they will not be able to know that the chef made that change!\nLearning to make sense of error messages is a very important skill that, like all skills, takes practice. Most errors are very easy to fix if you keep your cool. In fact, 90% of errors are simply typos.\n\n\n\n\n\n\nTask 1\n\n\n\nCopy and paste the following lines of code in a new .R script. Try to run each line individually. Each line will generate an error of some kind. In some cases, RStudio will warn you in advance that a line of code is likely wrong by displaying a red cross icon to the left of the erroneous line. If you hover over the red cross icon, RStudio will display a message that may help you to fix the error.\nCan you decode the error messages to find out what is causing these errors and fix these ten erroneous commands?\n\nmy.favourite.word &lt;- \"empathy\"\nmy.favourite.number &lt;- 13\n\n# Error 1:\nmy.favourite.number + my.favorite.number\n\n# Error 2:\nNegin-Fav-Word &lt;- \"Ach so!\" \n\n# Error 3:\nmy.favourite.numbers^2\n\n# Error 4:\n√∂mers_favourite_ number &lt;- 52\n\n# Error 5:\n    √∂mers_favorite_number =   my.favourite..number\n\n# Error 6:\nmy.favourite.number*2 -&gt; half.my.fav.number\n\n# Error 7:\nrose's.favourite.number &lt;- 5\n\n# Error 8:\nBestWordEver &lt;- \"supercalifragilisticexpialidocious\n\n# Error 9:\n2FavNumbers &lt;- my.favourite.number + √∂mers_favourite_number\n\n# Error 10:\ngood.luck &lt;- ŸÖŸàŸÅŸÇ ÿ®ÿßÿ¥ŸäÿØ\"\n\n\n\n\n\n\n\n\n\nFigure¬†5.13: Debugging is an unavoidable part of writing code. If you‚Äôre stuck and starting to feel fustrated, the best thing you can usually do is to take a short break (artwork by @allison_horst).\n\n\n\n\n\n\n\n\n\nClick here for the solutions to Task 1\n\n\n\n\n\n\nThe first error was object 'my.favorite.number' not found. This means that the object my.favorite.number is not stored in your environment. If you think it is, the problem is most likely due to a typo. Here, my.favorite.number uses American English spelling, whereas we used British English spelling when we created the object. To correct the error, you need to use exactly the same spelling as when you created the object.\nThe second error is also object 'Negin' not found. However, here we do not expect an object called Negin to be in the environment because what we are actually trying to do is create and save a new object called Negin-Fav-Word! The problem is that R interprets the hyphens in this object name as ‚Äúminus‚Äù and therefore tries to find the object Negin in order to then subtract Fav and Word from it. To correct this error, you need to remove the hyphens or replace them by dots.\nThe third error is yet another object not found error. It is another typo: the correct object name is not in the plural form.\nThe fourth error is Error: unexpected symbol in \"√∂mers_favourite_ number\". In addition, RStudio warned us that there were some ‚Äúunexpected tokens‚Äù in this line of code. The unexpected item is the space between _ and number. To fix this error, you need to remove this space character.\nThe object my.favourite..number is not found because the name of the object saved in the environment does not have two consecutive dots. Note that the error does not come from the fact that this line begins with some white space and includes multiple space characters after the = sign. These added spaces make the line more difficult for us humans to read, but R simply ignores them. Hence, to fix this error, what you need to do is remove one of the consecutive dots in the object name.\n\nIt is also worth noting that, once you‚Äôve removed the extra dot, this line of code replaces the value originally stored in √∂mers_favourite_number with the value stored in my.favourite.number. If you check your environment pane, you will see that the command has changed √∂mers_favourite_number to 13 - with no warning! In other words, here, the equal sign = behaves in the same way as the assignment operator &lt;-.\n\nIf you tried to run this line, you will have noticed that it does not actually generate an error. However, you may have noticed that the assignment operator is in the opposite direction. This means that my.favourite.number is multiplied by two and that this number is then assigned to a new object called half.my.fav.number. With this in mind, you will likely want to amend the line for the outcome to make mathematical sense (or rename the object).\nRunning this line will have caused you to run into a + situation in the console. As explained earlier in Section 5.6, to get out of it, you should first take your mouse cursor to the Console pane and then press the escape key on your keyboard to cancel this erroneous line. Whilst there is no error message to help you understand where the problem is coming from, you may find that RStudio helpfully displays a red cross icon to the left of the line; hovering over it displays a multi-line message. The first line is the relevant one: unexpected token 's.favourite.number &lt;- 5. This tells us that apostrophes are forbidden in object names. Remove the ' and the error will be fixed.\nThis line also causes a + situation. In this case, it is due to a missing quotation mark. To fix this error, first cancel the incomplete line of code by escaping it. Then, add the missing double quotation mark in your script and rerun the completed line.\nThe message Error: unexpected symbol in \"2FavNumbers\" is due to the fact that object names cannot start with a number. Change the object name to something like TwoFavNumbers or Fav2Numbers to fix the error.\nHere, too, the error message reads unexpected symbol. However, it is important to remember that the unexpected symbol is not within the character string, but rather within the command to assign the string to the object name good.luck. Hence, the problem is not that this string is in Persian, but rather that one of the quotation marks is missing. You can fix the error by ensuring that the phrase is enclosed in quotation marks.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Getting sta`R`ted</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html",
    "href": "6_ImpoRtingData.html",
    "title": "6¬† ImpoRting data",
    "section": "",
    "text": "Chapter overview\nMany introductory R textbooks postpone this section until much later by relying on datasets that are directly accessible as R data objects. In real life, however, research data rarely comes neatly packaged as an R data object. Your data will most likely be stored in a spreadsheet table or as text files of some kind. And -let‚Äôs be honest- they will be more messy than you would like to admit, making this chapter and the next crucial for learning to do data analysis in R.\nThis chapter will take you through the process of:\nIn future chapters, we will continue to work with this data. We will learn how to ‚Äúclean it up‚Äù for data analysis, before we begin to explore it using descriptive statistics and data visualisations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#accessing-data-from-a-published-study",
    "href": "6_ImpoRtingData.html#accessing-data-from-a-published-study",
    "title": "6¬† ImpoRting data",
    "section": "6.1 Accessing data from a published study",
    "text": "6.1 Accessing data from a published study\nAs we saw in Section 1.1, it is good practice to share both the data and materials associated with research studies so that others can reproduce and replicate the research.\nIn the following chapters, we will focus on data associated with the following study:\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\n\n\n\n\n\nFigure¬†6.1: Title page from the journal Language Learning\n\n\n\nFollow the DOI1 link above and read the abstract to find out what the study was about. You do not need to have institutional or paid access to the full paper to read the abstract.\n\n\n\n\n\n\nQuiz time!\n\n\n\n1) What types of data were collected as part of this study?\n\n\n\n\nSociodemographic information about the participants including their age and level of education\n\n\nParticipants' results on a grammar test\n\n\nParticipants' results on a collocations test\n\n\nParticipants' results on a nonverbal intelligence test\n\n\nParticipants' results on a vocabulary test\n\n\nParticipants' results on a language analytic ability test\n\n\n\n\n\n\n\n¬†\n2) On average, how did the English L2 speakers perform compared to the native speakers?\n\n\n\n\nOn average, both groups performed equally well on grammar and vocabulary tasks.\n\n\nThe observed differences in performance between native speakers and L2 speakers were not statistically significant.\n\n\nOn average, L2 speakers performed better on grammar tasks than native speakers\n\n\nOn average, native speakers outperformed L2 speakers on all language tasks, with the most significant difference observed in collocation tasks.\n\n\n\n\n\n\n\n¬†\n3) Did all native speakers perform better than the L2 speakers in the English vocabulary and grammar tests?\n\n\n\n\nYes, the L1 speakers clearly outperformed the L2 speakers in both grammar and vocabulary.\n\n\nThis study only looked at average trends, so no conclusive statement can be made about individual participants.\n\n\nNo, while L1 speakers generally performed better, some L2 speakers demonstrated equally high proficiency in grammar and vocabulary.\n\n\n\n\n\n\n\n¬†\n\n\nThe author, Ewa DƒÖbrowska, has made the data used in this study available on an open repository (see Section 2.4). To find out on which repository, go back to the study‚Äôs DOI link and click on the drop-down menu ‚ÄúSupporting Information‚Äù. It links to a PDF file. Click on the link and scroll to the last page which contains the following information about the data associated with this study:\n\nAppendix S4: Datasets\nDƒÖbrowska, E. (2018). L1 data [Data set]. Retrieved from https://www.iris-database.org/iris/app/home/detail?id=york:935513\nDƒÖbrowska, E. (2018). L2 data [Data set]. Retrieved from https://www.iris-database.org/iris/app/home/detail?id=york:935514\n\n\n\n\n\n\n\nQuiz time!\n\n\n\n4) On which repository/repositories can the data be found?\n\n\n\n\nZenodo\n\n\ndatabase.org\n\n\nIRIS\n\n\nResearchGate\n\n\nAll of the above.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\nYou may have noticed that the datasets were published in 2018, whereas the article (DƒÖbrowska 2019) was published in the following year. This is very common in academic publications as it can take many months or even years for an article or book to be published, by which time the author(s) may have already made the data available on a repository. This particular article was actually first published on the journal‚Äôs website on 22 October 2018 as an ‚Äúadvanced online publication‚Äù, but was not officially published until March 2019 as part of Volume 69, Issue S1 of the journal (see https://doi.org/10.1111/lang.12323). This explains the discrepancy between the publication date of the datasets and the publication date of the article recorded in the bibliographic reference.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-SavingData",
    "href": "6_ImpoRtingData.html#sec-SavingData",
    "title": "6¬† ImpoRting data",
    "section": "6.2 Saving and examining the data",
    "text": "6.2 Saving and examining the data\nClick on the two links listed in Appendix S4 and download the two datasets. Note that the URL may take a few seconds to redirect and load. Save the two datasets in an appropriate place on your computer (see Section 3.3), as we will continue to work with these two files in the following chapters.\n\n\n\n\n\n\nWhat‚Äôs a good place to save these files? ü§î\n\n\n\n\n\nIf you haven‚Äôt already done so, I suggest that you create a folder in which you save everything that you create whilst learning from this textbook. This folder could be called something along the lines of DataLiteracyTextbook, 2024_data_literacy, or LeFoll_2024_DataLiteracy (see Section 3.2). Then, within this folder, I recommend that you create another folder called Dabrowska2019 (note how I have not included the ‚ÄúƒÖ‚Äù character in the folder name as this could cause problems), and within this folder, create another folder called data. This is the folder in which you can save these two files.\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\n5) In which data format are these two files saved?\n\n\n\n\nThey are both .csv files.\n\n\nThey are both .xslx files.\n\n\nThey are both .html files.\n\n\nThey are both .txt files.\n\n\n\n\n\n\n\n¬†\n\n\nThe file L1_data.csv contains data about the study‚Äôs L1 participants. It is a delimiter-separated values (DSV) file (see Section 2.5.1). The first five lines of the file are printed below. Note that this is a very wide table as it contains many columns (you can scroll to the right to view all the columns).\nParticipant,Age,Gender,Occupation,OccupGroup,OtherLgs,Education,EduYrs,ReadEng1,ReadEng2,ReadEng3,ReadEng,Active,ObjCl,ObjRel,Passive,Postmod,Q.has,Q.is,Locative,SubCl,SubRel,GrammarR,Grammar,VocabR,Vocab,CollocR,Colloc,Blocks,ART,LgAnalysis\n1,21,M,Student,PS,None,3rd year of BA,17,1,2,2,5,8,8,8,8,8,8,6,8,8,8,78,95,48,73.33333333,30,68.75,16,17,15\n2,38,M,Student/Support Worker,PS,None,NVQ IV Music Performance,13,1,2,3,6,8,8,8,8,8,8,7,8,8,8,79,97.5,58,95.55555556,35,84.375,11,31,13\n3,55,M,Retired,I,None,No formal (City and Guilds),11,3,3,4,10,8,8,8,8,8,7,8,8,8,8,79,97.5,58,95.55555556,31,71.875,5,38,5\n4,26,F,Web designer,PS,None,BA Fine Art,17,3,3,3,9,8,8,8,8,8,8,8,8,8,8,80,100,53,84.44444444,37,90.625,20,26,15\n\n\n\n\n\n\nQuiz time!\n\n\n\n7) Which character is used to separate the values in the file L1_data.csv?\n\n\n\n\ncolon\n\n\ndouble quotation mark\n\n\ntab\n\n\nspace\n\n\ncomma\n\n\n\n\n\n\n\n¬†\n8) Which character is used to delineate the values?\n\n\n\n\nsingle quotation mark\n\n\nspace\n\n\nnone\n\n\ndouble quotation mark\n\n\ndot",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-RProject",
    "href": "6_ImpoRtingData.html#sec-RProject",
    "title": "6¬† ImpoRting data",
    "section": "6.3 Using Projects in RStudio",
    "text": "6.3 Using Projects in RStudio\nOne of the advantages of working with RStudio is that it allows us to harness the potential of RStudio Projects. Projects help us to keep our digital kitchen nice and tidy. In RStudio, each project has its own directory, environment, and history which means that we can work on multiple projects at the same time and RStudio will keep them completely separate. This means that we can easily switch between cooking different dishes, say a gluten-free egg curry and vegan pancakes, without fear of accidentally setting the wrong temperature on the cooker or contaminating either dish.\nRegardless of whether or not you‚Äôre a keen multi-tasker, RStudio Projects are a great way to help you keep together all the data, scripts, and outputs associated with a single project in an organised manner. In the long run, this will make your life much, much easier. It will also be an absolute lifesaver as soon as you need to share your work with others (e.g., your supervisor, colleagues, reviewers, etc.).\nTo create a new Project, you have two options. In RStudio, you can select ‚ÄòFile‚Äô, then ‚ÄòNew Project‚Ä¶‚Äô (see ‚Äúa‚Äù on Figure¬†6.2). Alternatively, you can click on the Project button in the top-right corner of your RStudio window and then select ‚ÄòNew Project‚Ä¶‚Äô (see ‚Äúb‚Äù on Figure¬†6.2).\n\n\n\n\n\n\nFigure¬†6.2: Create a new project in RStudio\n\n\n\nBoth options will open up a window with three options for creating a new project:\n\nNew Directory (which allows you to create an entirely new project for which you do not yet have a folder on your computer)\nExisting Directory (which allows you to create a project in an existing folder associated with your project)\nVersion Control (see Bryan 2018).\n\nIn Section 6.2, you should have already saved the data that we want to import in a dedicated folder on your computer. Here, a folder is the same as a directory. Hence, you can select the second option: ‚ÄòExisting Directory‚Äô.\nClicking on this option will open up a new window (Figure¬†6.2). Click on ‚ÄòBrowse‚Ä¶‚Äô to navigate to the folder where you intend to save all your work related to DƒÖbrowska (2019). If you followed my suggestions earlier on, this would be a folder called something along the lines of Dabrowska2019. Once you have selected the correct folder, select the option ‚ÄòOpen in a new session‚Äô and then click on ‚ÄòCreate Project‚Äô.\n\n\n\nNew project window\n\n\nCreating an RStudio project generates a new file in your project folder called Dabrowska2019.Rproj. You can see it in the Files pane of RStudio. Note that the extension of this newly created file is .Rproj. Such .Rproj files store information about your project options, which you will not need to edit. More usefully, .Rproj files can be used as shortcuts for opening your projects. To see how this works, shut down RStudio. Then, in your computer file system (e.g., using a File Explorer window on Windows and a Finder window on macOS), navigate to your project folder to locate your .Rproj file (see Figure¬†6.3 (a)). Double-click on the file. This will automatically launch RStudio with all the correct settings for this particular project. Alternatively, you can use the Project button in the top-right corner of your RStudio window to open up a project from RStudio itself (see Figure¬†6.3 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Launching a Project from the File Finder\n\n\n\n\n\n\n\n\n\n\n\n(b) Launching a Project from RStudio\n\n\n\n\n\n\n\nFigure¬†6.3: The two options to open an RProject.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-WorkingDirectories",
    "href": "6_ImpoRtingData.html#sec-WorkingDirectories",
    "title": "6¬† ImpoRting data",
    "section": "6.4 Working directories",
    "text": "6.4 Working directories\nThe folder in which the .Rproj file was created corresponds to your project‚Äôs working directory. Once you have opened a Project, you can see the path to your project‚Äôs working directory at the top of the Console pane in RStudio. The Files pane should also show the content of this directory.\n\n\n\nContents of the project folder as displayed by RStudio\n\n\nClick on the ‚ÄúNew Folder‚Äù icon in your Files pane to create a new subfolder called analysis. Your folder Dabrowska2019 should now contain an .RProj file and two subfolders called analysis and data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-ImportingDataCSV",
    "href": "6_ImpoRtingData.html#sec-ImportingDataCSV",
    "title": "6¬† ImpoRting data",
    "section": "6.5 Importing data from a .csv file",
    "text": "6.5 Importing data from a .csv file\nWe will begin by creating a new R script in which we will write the code necessary to import the data from DƒÖbrowska (2019)‚Äòs study in R. To do so, from the Files pane in RStudio, click on the analysis folder to open it and then click on the ‚ÄôNew Blank File‚Äô icon in the menu bar of the Files pane and select ‚ÄòR Script‚Äô. This will open a new, empty R script in your Source pane. It is best to always begin by saving a newly created file. Save this empty script with a computer- and human-friendly file name such as 1_DataImport.R (Section 3.2). It should now appear in your analysis folder in the Files pane.\nGiven that we want to import two .csv files, we are going to use the function read.csv(). You can find out what this function does by running the command ?read.csv or help(read.csv) in the Console to open up the documentation. This help file contains information about several base R functions used to import data. Scroll down to find the information about the read.csv() function. It reads:\nread.csv(file, header = TRUE, sep = \",\", quote = \"\\\"\",\n         dec = \".\", fill = TRUE, comment.char = \"\", ...)\nThis line from the documentation informs us that this function‚Äôs first argument is the path to the file from which we want to import the data. It also informs us that file is the only argument that does not have a default value (as it is not followed by an equal sign and a value). In this function, file is therefore the only argument that is compulsory. Hence, in theory, all we need to write to import the data is:\n\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\")\n\nIn fact, we could shorten things even further as, unless otherwise specified, R will assume that the first value listed after a function corresponds to the function‚Äôs first argument which, here, is file. In other words, this command and the one above are equivalent:\n\nL1.data &lt;- read.csv(\"data/L1_data.csv\")\n\nThe file path \"data/L1_data.csv\" informs R that the data is located in a subfolder of the project‚Äôs working directory called data and that, within this data subfolder, the file that we want to import is called L1_data.csv. Note that the file extension must be specified (see Section 2.3). Note, also, the file path is separated with a single forward slash /. In R, this should work regardless of the operating system that you are using and, in order to be able to easily share your scripts with others, it is recommended that you use forward slashes even if you are running Windows (Section 3.3).\nAlthough the command above did the job, in practice, it is often safer to spell things out further to remind ourselves of some of the default settings of the function that we are using in case they need to be checked or changed at a later stage. In this example, we will therefore import the data with the following command:\n\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\",\n                    header = TRUE,\n                    sep = \",\",\n                    quote = \"\\\"\",\n                    dec = \".\")\n\nIn the command above, header = TRUE, explicitly tells R to import the first row of the .csv table as column headers rather than values. This is not strictly necessary because, as we saw from the function‚Äôs help file, TRUE is already set as the default value for this argument, but it is good to remind ourselves of how this particular dataset is organised.\nThe arguments sep and quote specify the characters that, in this .csv file are used to separate the values on the one hand, and delineate them, on the other (see Section 2.5.1). As we saw above, DƒÖbrowska (2019)‚Äôs .csv files use the comma as the separator and the double quotation mark as the quoting character. Note that the \" character needs to be preceded by a backslash (\\) (we say it needs to be ‚Äúescaped‚Äù) because otherwise R will interpret it as part of the command syntax, which would lead to an error. Finally, the argument dec = \".\" explicitly tells R that this .csv file uses the dot as the decimal point. In some countries, e.g., Germany and France, the comma is used to represent decimal places so, if you obtain data from a German or French colleague, this setting may need to be changed to dec = \",\" for the data to be imported correctly.\n\n\n\n\n\n\n‚ÄúHell is empty, and all the devils are {here}.‚Äù üòà\n\n\n\n\n\nThis section title borrows a quote from The Tempest by William Shakespeare to reflect the fact that file paths are perhaps the most frequent source of frustration among (beginner) coders. Section 6.6 explains how to deal with the most frequent error messages. Ultimately, however, these errors are typically due to poor data management (see Chapter 3). That‚Äôs because the devil‚Äôs in the detail (remember: no spaces, special characters, differences between different operating systems, etc.). As a result, even advanced users of R and other programming languages frequently find that file path issues continue to plague their work, if they fail to take file management seriously.\nTo make your projects more robust to such issues, I strongly recommend working with the {here} package in addition to RProjects. You will first need to install the package:\n\ninstall.packages(\"here\")\n\nWhen you load the package, it automatically runs the here() function with no argument, which returns the path to your project directory, as determined by the location of the .RProj file associated with your project.\n\nlibrary(here)\n\nYou can now use the here() function to build paths relative to this directory with the following syntax:\n\nhere(\"data\", \"L1_data.csv\")\n\n[1] \"/Users/lefoll/Documents/UzK/RstatsTextbook/data/L1_data.csv\"\n\n\nAnd you can embed this path in your import command like this:\n\nlibrary(here)\n\nL1.data &lt;- read.csv(file = here(\"data\", \"L1_data.csv\"))\n\nMuch like wearing a helmet for extra safety (Figure¬†6.4), {here} makes the paths that you include in your code far more robust. In other words, they are far less likely to fail and break your code when you share your scripts with your colleagues, or run them yourself from different directories or operating systems. For more reasons to use {here}, check out the blog post ‚ÄúWhy should I use the here package when I‚Äôm already using projects?‚Äù by Barrett (2018).\n\n\n\n\n\n\nFigure¬†6.4: Although a fairly common way of working with data in R, using setwd() (see Section 6.12.1) is dangerous and will, sooner or later, cause you and/or your colleagues some nasty accidents. A combination of using RProj and {here} as described above is much safer!",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-ImportingErrors",
    "href": "6_ImpoRtingData.html#sec-ImportingErrors",
    "title": "6¬† ImpoRting data",
    "section": "6.6 Import errors and issues ü•∫",
    "text": "6.6 Import errors and issues ü•∫\nIt is crucial that you check whether your data has genuinely been correctly imported. Here‚Äôs a list of things to check (in that order):\n\nWere you able to run the import command without producing any errors? If you are getting an error, remember that this is most likely due to a typo (see Section 5.6)!\n\nIf part of the error message reads ‚ÄúNo such file or directory‚Äù, this means that either the file path or the file name is incorrect. Carefully check the path that you specified in your import command (if you‚Äôre struggling to find the correct path, you may want to try out the import method explained in Section 6.12.2). To ensure that you are not misspelling the name of the file, you can press the tab key on your keyboard to get RStudio to auto-complete the file name for you.\nIf the error message includes the statement ‚Äúcould not find function‚Äù, this means that you have either misspelled the name of the function or this is not a base R function and you have forgotten to load the library to which this function belongs (see Section 6.8).\nAs usual whenever you get an error message, also check that you have included all of the necessary brackets and quotation marks (see Section 5.6).\n\nHas the R data object appeared in your Environment pane? Does it have the expected number of rows (observations) and columns (variables)? L1.data contains 90 observations and 31 variables. If you are getting different numbers, this might be because you previously opened the .csv file with Excel or that your computer converted it to Excel format automatically. To remedy this, ensure that you have followed all the steps described in Section 2.5.2.\nTo view the entire table, use the function View() with the name of your data object as the first and only argument, e.g., View(L1.data)2. This will open up a new tab in your Source pane that displays the full table, much like in a spreadsheet programme. You can search and filter the table in this tab, but you cannot edit it in any way (and that‚Äôs a good thing because, if we want to edit things, we want to ensure that we keep track of our changes in a script!). Browse through the table and check that everything ‚Äúlooks healthy‚Äù. This is much like visually inspecting and smelling ingredients before using them in a recipe. It‚Äôs not perfect but if something is really off, you should notice it. Check that each cell appears to have one and only one value.\nFinally, use the str() function to view the structure of your data object in a more compact way. Using the command str(L1.data) will display a summary of the data.frame in the Console. The summary begins by informing us that this data object is a data.frame, that contains 90 observations and 31 variables. Then, it lists all of the variables, followed by the type of values stored in this variable (e.g., character strings or integers) and then the first few values for each variable. Especially with very wide tables that contain a lot of variables, it is often easier to check the summary of the imported data with str() than with View(), though I would always recommend taking a few seconds to do both. This is time well spent!\n\n\n\n\n\n\n\nTask 1\n\n\n\nImport both data files from DƒÖbrowska (2019) using the read.csv function as described above. Save the first as the R object L1.data (as in the example above) and the second as L2.data. Then, answer the following questions.\na) In the two data files from DƒÖbrowska (2019), each row corresponds to one participant. How many L1 participants were included in this study?\n\n\n\n\n17\n\n\n2790\n\n\n31\n\n\n221\n\n\n90\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nb) How many L2 participants were included in DƒÖbrowska (2019)‚Äôs study?\n\n\n\n\n45\n\n\n67\n\n\n306\n\n\n90\n\n\n220\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nc) Compare the data frames containing the L1 and L2 data. Which dataset contains more variables?\n\n\n\n\nL2.data\n\n\nL1.data\n\n\nThey have the same number of variables\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nd) You have saved the two datasets to your local R environment as L1.data and L2.data. What kind of R objects are L1.data and L2.data? You can find out by using the command class(). It simply takes the name of the object as its only argument.\n\n\n\n\ntibble\n\n\nlist\n\n\ntable\n\n\ninteger\n\n\ndata frame\n\n\ncharacter\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\ne) Why does the L2 dataset contain the variable NativeLg, but not the L1 dataset?\n\n\n\n\nBecause Dr. DƒÖbrowska decided not to collect this information for L1 participants.\n\n\nBecause some or all of the L1 participants did not wish to answer this question.\n\n\nBecause this variable was removed from the dataset for data protection reasons.\n\n\nBecause, in this study, all L1 participants have English as their native language.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#importing-tabular-data-in-other-formats",
    "href": "6_ImpoRtingData.html#importing-tabular-data-in-other-formats",
    "title": "6¬† ImpoRting data",
    "section": "6.7 Importing tabular data in other formats",
    "text": "6.7 Importing tabular data in other formats\nWe have seen how to load data from a .csv file into R by creating an R data frame object that contains the data extracted from a .csv file. But, as we saw in Chapter 2, not all datasets are stored as .csv files. Fear not: there are many import functions in R, with which you can import pretty much all kinds of data formats! This section introduces a few of the most useful ones for research in the language sciences.\nWe begin with the highly versatile function read.table(). The read.csv() is actually a variant of read.table(). You recall that when we called up the help file for the former using ?read.csv(), we obtained a combined help file for several functions, the first of which was read.table(). By specifying the following arguments as we did earlier, we can actually use the read.table() function to import our .csv file with exactly the same results:\n\nL1.data &lt;- read.table(file = \"data/L1_data.csv\",\n                    header = TRUE,\n                    sep = \",\",\n                    quote = \"\\\"\",\n                    dec = \".\")\n\n\n6.7.1 Tab-separated file\nIn Task 3 in Section 2.5.2, you downloaded and examined a DSV file with a .txt extension that was separated by tabs: offlinedataLearners.txt from Schimke et al. (2018).\nIf we change the separator character argument to \\t for tab, we can also import this dataset in R using the read.table() function:\n\nOfflineLearnerData &lt;- read.table(file = \"data/offlinedataLearners.txt\",\n                                        header = TRUE,\n                                        sep = \"\\t\",\n                                        dec = \".\")\n\nFor the command above to work, you will first need to save the file offlinedataLearners.txt to the folder specified in the path. Otherwise, you will get an error message informing you that there is ‚ÄúNo such file or directory‚Äù (see Section 6.6).\n\n\n6.7.2 Semi-colon-separated file\nFigure¬†6.5 displays an extract of the dataset AJT_raw_scores_L2.csv from an experimental study by Busterud et al. (2023). Although this DSV file has a .csv extension, it is actually separated by semicolons. As you can see in Figure¬†6.5, in the file AJT_raw_scores_L2.csv, the comma is used to show the decimal place.\n\n\n\n\n\n\nFigure¬†6.5: Extract of data file AJT_raw_scores_L2.csv from\n\n\n\nIf you look carefully, you will also see that this dataset has some empty cells. This data can be downloaded from https://doi.org/10.18710/JBMAPT. It is delivered with a README text file. It is good practice to include a README file when publishing datasets or code and, as the name suggests, it is always a good idea to actually read README files! üôÉ Among other things, this particular README explains that, in this dataset: ‚ÄúMissing data are represented by empty cells.‚Äù\nIf you call up the help file for the read.table() function again, you will see that there is an argument called na.strings. The default value is NA. When we import this dataset AJT_raw_scores_L2.csv from Busterud et al. (2023), we will therefore need to change this argument to ensure that empty cells are recognised as missing values.\nIn addition to the file path, the command to import this dataset specifies the separator character as the semicolon (sep = \";\"), the character used to represent decimals (dec = \",\"), and empty cells as missing values (na.strings = \"\"):\n\nAJT.raw.scores.L2 &lt;- read.table(file = \"data/AJT_raw_scores_L2.csv\",\n                          header = TRUE,\n                          sep = \";\",\n                          dec = \",\",\n                          na.strings = \"\")\n\nOnce we have run this command, we should check that the data have been correctly imported, for example by using the View() function:\n\nView(AJT.raw.scores.L2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nL3\nYears.of.L3\nGender\nL3.selfasses\nL3.grade\nL2.selfasses\nL2.grade\n\n\n\n\nBKRE452\n2\n4\n2\n2.5\n3\n4\n3\n\n\nSHEL876\n2\n4\n2\n3.0\n5\n6\n5\n\n\nSVI√ò510\n2\n4\n1\n4.0\n4\n6\n5\n\n\nEHEA194\n2\n4\n1\n2.0\n2\n6\n4\n\n\nERAO442\n2\n4\n2\n3.0\n3\n5\n4\n\n\nSEIO103\n2\n4\n1\n2.0\n3\n4\n3\n\n\nNMOI241\n2\n4\n1\n3.0\n4\n4\n4\n\n\nBBIE911/77\n2\n4\n1\nNA\n3\nNA\n4\n\n\nUUNO561\n2\n4\n1\n2.0\n3\n3\n4\n\n\nSMAO470\n2\n4\nNA\n3.0\nNA\n6\nNA\n\n\nSSID616\n2\n3\n1\n2.0\n2\n6\n4\n\n\nSHRI714\n2\n1\n2\n4.0\n3\n3\n3\n\n\nHALI620\n2\n1\n2\n5.0\n6\n4\n4\n\n\n\n\n\n\nHere, we can see that the data have been correctly imported as a table. The commas have been correctly converted to decimal points and the empty cells are now labelled NA.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#sec-readr",
    "href": "6_ImpoRtingData.html#sec-readr",
    "title": "6¬† ImpoRting data",
    "section": "6.8 Using {readr} from the {tidyverse} to import tabular files",
    "text": "6.8 Using {readr} from the {tidyverse} to import tabular files\nThe {tidyverse} is a family of packages that we will use a lot in future chapters. This family of package includes the {readr} package which features some very useful functions to import data into R. You can install and load the {readr} package either individually or as part of the {tidyverse} bundle:\n\n# Install the package individually:\ninstall.packages(\"readr\")\n\n# Or install the full tidyverse (this will take a little longer):\ninstall.packages(\"tidyverse\")\n\n# Load the library:\nlibrary(readr)\n\n\nDelimiter-separated values (DSV) files\nThe {readr} package includes functions to import DSV files that are similar, but not identical to the base R functions explained above. The main difference is that the {readr} functions load data into an R object of type ‚Äútibble‚Äù rather than ‚Äúdata frame‚Äù. In practice, this will not make a difference for our work in future chapters. Hence, the following two commands can equally be used to import L1_data.csv:\n\n# Import .csv file using the base R function read.csv():\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\", \n                    header = TRUE, \n                    quote = \"\\\"\")\n\nclass(L1.data)\n\n[1] \"data.frame\"\n\n# Import .csv file using the {readr} function read_csv():\nL1.data &lt;- read_csv(file = \"data/L1_data.csv\", \n                    col_names = TRUE, \n                    quote = \"\\\"\")\n\nclass(L1.data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nNote that instead of the argument header = TRUE, the {readr} function read_csv() takes the argument col_names = TRUE, which has the same effect.\nThere are a few more differences between the two functions that are worth noting:\n\nIf the column headers in your original data file contain spaces, these will be automatically replaced by dots (.) when you import data using base R functions. By contrast, with the {readr} functions, the spaces will, by default, be retained. As we will see later, this behaviour can represent both an advantage and disadvantage, depending on what you want to do.\nThe {readr} functions are quicker and are therefore recommended if you are importing large datasets.\nIn general, the behaviour of {readr} functions is more consistent across different operating systems and locale settings (e.g., the language in which your operating system is set).\n\nNote that, just like read.csv() was a special case of read.table, the {readr} function read_csv() is a special variant of the more general function read_delim() that can be used to import data from all kinds of DSV files. Check the help file to find out all the options using ?read_delim.\nThe help file informs us that the package includes a function specifically designed to import semi-colon separated file with the the comma as the decimal point: read_csv2(). It further states that ‚Äú[t]his format is common in some European countries.‚Äù If you scroll down the help page, you will see that its usage is summarised in the following way:\nread_csv2(\n  file,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  quote = \"\\\"\",\n  comment = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = show_progress(),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\nThis overview of the read_csv2 function shows all of the arguments of the function and their default values. For instance, with na = c(\"\", \"NA\"), it tells us that, by default, both empty cells and cells with the value NA will be interpreted by the function as NA values.\nHaving checked the default values for all of the arguments of the read_csv2 function, we may conclude that we can safely use this {readr} function to import the file AJT_raw_scores_L2.csv from Busterud et al. (2023) without changing any of these default values. Hence, all we need is:\n\nAJT.raw.scores.L2 &lt;- read_csv2(file = \"data/AJT_raw_scores_L2.csv\")\n\nNote that, whereas when we used the base R function read.table() the header for the third variable in the file was imported as Years.of.L3, using the {readr} function, the variable is entitled Years of L3.\n\n\nFixed width files\nFixed width files (with file extensions such as .gz, .bz2 or .xz) are a less common type of data source in the language sciences. In these text-based files, the values are separated not by a specific character such as the comma or the tab, but by a set amount of white/empty space other than a tab. Fixed width files can be loaded using the read_fwf() function from {readr}. Fields can be specified by their widths with fwf_widths() or by their positions with fwf_positions().",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#importing-files-from-spreadsheet-software",
    "href": "6_ImpoRtingData.html#importing-files-from-spreadsheet-software",
    "title": "6¬† ImpoRting data",
    "section": "6.9 Importing files from spreadsheet software",
    "text": "6.9 Importing files from spreadsheet software\nIf your data are currently stored in a spreadsheet software (e.g., LibreOffice Calc, Google Sheets, or Microsoft Excel), you can export them to .csv or .tsv. However, if you do not wish to do this (e.g., because your colleague wishes to maintain the spreadsheet format that includes formatting elements such as bold or coloured cells), there are functions to import these file formats directly into R.\n\n6.9.1 LibreOffice Calc\nFor LibreOffice Calc (which you should have installed in Section 1.2), you can install the {readODS} package and use its read_ods() function to import .ods files. Details about all the options can be found here https://www.rdocumentation.org/packages/readODS/versions/2.3.0.\n\n# Install from CRAN (safest option):\ninstall.packages(\"readODS\")\n\n# Or install the development version from Github:\nremotes::install_github(\"ropensci/readODS\")\n\n# Load the library:\nlibrary(readODS)\n\n# Import your .ods data:\nMyLibreOfficeData &lt;- read_ods(\"data/MyLibreOfficeTable.ods\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = NA)\n\n\n\n6.9.2 Microsoft Excel\nVarious packages can be used to import Microsoft Excel file formats, but the simplest is {readxl}, which is part of the {tidyverse}. It allows users to import data in both .xlsx and the older .xls format. You can find out more about its various options here: https://readxl.tidyverse.org/.\n\n# Install from CRAN (safest option):\ninstall.packages(\"readxl\")\n\n# Or install the development version from Github:\nremotes::install_github(\"tidyverse/readxl\")\n\n# Load the library:\nlibrary(readxl)\n\n# Import your .ods data:\nMyExcelData &lt;- read_excel(\"data/MyExcelSpreadsheet.xlsx\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = NA)\n\n\n\n6.9.3 Google Sheets\nThere are also several ways to import data from Google Sheets. The simplest is to export your tabular data as a .csv, .tsv, .xslx, or .ods file by selecting Google Sheet‚Äôs menu option ‚ÄòFile‚Äô &gt; ‚ÄòDownload‚Äô. Then, you can simply import this downloaded file in R using the corresponding function as described above.\nHowever, if you want to directly import your data from Google Sheets and be able to dynamically update the analyses that you conduct in R even as the input data is amended on Google Sheets, you can use the {googlesheets4} package (which is part of the {tidyverse}):\n\n# Install from CRAN (safest option):\ninstall.packages(\"googlesheets4\")\n\n# Or install the development version from Github:\nremotes::install_github(\"tidyverse/googlesheets4\")\n\n# Load the library:\nlibrary(googlesheets4)\n\n# Import your Google Sheets data using your (spread)sheet's URL:\nMySheetsData &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = \"NA\")\n\n# Or import your Google Sheets data using just the sheet's ID:\nMySheetsData &lt;- read_sheet(\"1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\",\n                            sheet = 1,\n                            col_names = TRUE,\n                            na = \"NA\")\n\n\n\n\n\n\n\nFigure¬†6.6: Dialogue box to consent to the {tidyverse} API Packages having access to your Google Drive to import directly from a Google Sheet. Read this carefully before clicking on ‚ÄòContinue‚Äô.\n\n\n\n\n\n6.9.4 Importing spreadsheet files with multiple sheets/tabs\nNote that, as spreadsheet software typically allow users to have several ‚Äúsheets‚Äù or ‚Äútabs‚Äù within a file that each contains separate tables, the functions read_excel(), read_ods(), and read_sheet include an argument called sheet which allows you to specify which sheet should be imported. The default value is 1, which simply means that the first one is imported. If your sheets have names, you can also use its name as the argument value, e.g.:\n\nMyExcelData &lt;- read_excel(\"data/MyExcelSpreadsheet.xlsx\",\n                            sheet = \"raw data\",\n                            col_names = TRUE,\n                            na = NA)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#importing-data-files-from-spss-sas-and-stata",
    "href": "6_ImpoRtingData.html#importing-data-files-from-spss-sas-and-stata",
    "title": "6¬† ImpoRting data",
    "section": "6.10 Importing data files from SPSS, SAS and Stata",
    "text": "6.10 Importing data files from SPSS, SAS and Stata\nIf you‚Äôve recently switched from working in SPSS, SAS, or Stata (or are collaborating with someone who uses these programmes), it might be useful to know that you can also import the data files created by programmes directly into R using the {haven} package. Details of all the options can be found here: https://haven.tidyverse.org/.\n\n# Install from CRAN (safest option):\ninstall.packages(\"haven\")\n\n# Or install the development version from Github:\nremotes::install_github(\"tidyverse/haven\")\n\n# Load the library:\nlibrary(haven)\n\n# Import an SAS file\nMySASData &lt;- read_sas(\"MySASDataFile.sas7bdat\")\n\n# Import an SPSS file\nMySPSSDataFile &lt;- read_sav(\"MySPSSDataFile.sav\")\n\n# Import a Stata file\nMyStataData &lt;- read_dta(\"MyStataDataFile.dta\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#importing-other-file-formats",
    "href": "6_ImpoRtingData.html#importing-other-file-formats",
    "title": "6¬† ImpoRting data",
    "section": "6.11 Importing other file formats",
    "text": "6.11 Importing other file formats\nIn this textbook, we will only deal with DSV files (Section 2.5.1) but, as you can imagine, there are many more R packages and functions that allow you to import all kinds of other file formats. These include .xml, .json and .html files, various database formats, and other files with complex structures (see, e.g., https://rc2e.com/inputandoutput).\nIn addition, fellow linguists are constantly developing new packages to work with file formats that are specific to our discipline. In the spirit of Open Science (see Chapter 1), many are making these packages available to the wider research community by releasing them under open licenses. For example, Linguistics M.A.¬†students Katja Wiesner and Nicolas Werner wrote an R package to facilitate the import of .eaf files generated by the annotation software ELAN (Lausberg & Sloetjes 2009) into R as part of a seminar project supervised by Dr.¬†Fahime (Fafa) Same at the University of Cologne (https://github.com/relan-package/rELAN/?tab=readme-ov-file).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#quick-and-dirty-aka-bad-ways-to-import-data-in-r",
    "href": "6_ImpoRtingData.html#quick-and-dirty-aka-bad-ways-to-import-data-in-r",
    "title": "6¬† ImpoRting data",
    "section": "6.12 Quick-and-dirty (aka bad!) ways to import data in R",
    "text": "6.12 Quick-and-dirty (aka bad!) ways to import data in R\nFeel free to skip this section if you got on just fine with the importing method introduced above as the following two methods are problematic for a number of reasons. However, they may come in useful in special cases, which is why both are briefly explained below.\n\n6.12.1 Hardcoding file paths in R scripts ü•¥\nWhilst it is certainly not recommended (see, e.g., Bryan 2017), it is nonetheless worth understanding this method of working with file paths in R as you may well come across it in other people‚Äôs code.\nInstead of creating an RProject to determine a project‚Äôs working directory (as we did in Section 6.3), it is possible to begin a script with a line of code that sets the working directory for the script using the function setwd(), e.g.:\n\nsetwd(\"/Users/lefoll/Documents/UzK/RstatsTextbook/Dabrowska2019\")\n\nAfterwards, data files can be imported using a relative path from the working directory just like we did earlier.\n\nL1.data &lt;- read.csv(file = \"data/L1_data.csv\")\n\nIf you have to work with an R script that uses this method, you will need to amend the path designated as the working directory by setwd() to the corresponding path on your own computer. This might not sound like much of an issue, but as data scientist, R expert, and statistics professor Jenny Bryan (2017) explains:\n\nThe chance of the setwd() command having the desired effect ‚Äì making the file paths work ‚Äì for anyone besides its author is 0%. It‚Äôs also unlikely to work for the author one or two years or computers from now. The project is not self-contained and portable.\n\nIn the language sciences, not everyone is aware of the severity of these issues. Hence, it is not uncommon for researchers to make their scripts even less reproducible by not setting a working directory at all and, instead, relying exclusively on absolute paths (Section 3.3). Hence, every time they want to import data (and, as we will see later on, export objects from R, too), they write out the full file path in the command like this:\n\nL1.data &lt;- read.csv(file = \"/Users/lefoll/Documents/UzK/RstatsTextbook/Dabrowska2019/data/L1_data.csv\")\n\nHaving to work with such a script is particularly laborious because it means that, if you inherit such a script from a colleague, you will have to manually change every single file path in the script to the corresponding file paths on your own computer. And, as Bryan (2017) points out, this will also apply if you change anything in your own computer directory structure! I hope I‚Äôve made clear that the potential for making errors in the process is far too important to even consider going down that route.\nHowever, should you have to use this method at some point for whatever reason, you can make use of Section 3.3 which explained how to copy full file paths from a file Explorer or Finder window. Note that if there are spaces or other special characters other than _ or - anywhere in your file path, your import command will fail (see Section 3.2 on naming conventions for folders and files). The following command, for instance, will fail and return an error (see Figure¬†6.7) because the folder ‚ÄúUni Work‚Äù contains a space.\n\nL1.data &lt;- read.csv(file = \"/Users/lefoll/Documents/Uni Work/RstatsTextbook/Dabrowska2019/data/L1_data.csv\")\n\n\n\n\n\n\n\nFigure¬†6.7: Error message due to an error in the file path\n\n\n\nThe only way to fix this issue is to remove the space in the name of the folder (in your File Finder or Navigator window) and then amend the file path in your R script accordingly.\n\n\n6.12.2 Importing data using RStudio‚Äôs GUI ü´§\nYou may have noticed that, if you click on a data file from the Files pane in RStudio (Figure¬†6.8), RStudio will offer to import the dataset for you. This looks like (and genuinely is) a very convenient way to import data in an R session using RStudio‚Äôs GUI (graphical user interface).\n\n\n\n\n\n\nFigure¬†6.8: Importing a file from RStudio‚Äôs File pane\n\n\n\nClicking on ‚ÄòImport Dataset‚Äô opens up RStudio‚Äôs ‚ÄòImport Text Data‚Äô dialogue box, which is similar to the one that we saw in LibreOffice Calc (Section 2.5.2). It allows you to select the relevant options to correctly import the file and displays a preview to check that the options that you have selected are correct. You can also specify the name of the R object to which you want to assign the imported data. By default, the name of the data file (minus the file extension and any special characters) is suggested.\n\n\n\n\n\n\nFigure¬†6.9: RStudio‚Äôs ‚ÄòImport Text Data‚Äô dialogue\n\n\n\nAs soon as you click on the ‚ÄòImport‚Äô button, the data is imported and opened using the View() function for you to check the sanity of the data.\nThis importing method works a treat, so what‚Äôs not to like? Well, the first problem is that you are not in full control. You cannot select which import function is used; RStudio decides for you. You may have noticed that it chooses to use the {readr} import functions, rather than the base R ones. There are lots of good reasons to use the {readr} functions (see Section 6.8), but it may not be what you wanted to do. When we do research, it is important for us to be in control of every step of the analysis process.\nSecond, your data import settings are not saved in an .R script as the commands were only sent to the Console: they are part of your script. This means that if you import your data in this way, do some analyses, and then close RStudio, you will have no way of knowing with which settings you imported the data to obtain the results of your analysis! This can have serious consequences for the reproducibility of your work.\nWhilst there is no way of remedying the first issue, the second can easily be fixed. After you have successfully imported your data from RStudio‚Äôs Files pane, you can (and should!) immediately copy the import commands from the Console into your .R script. In this way, the next time you want to re-run your analysis, you can begin by running these import commands directly from your .R script rather than by via RStudio‚Äôs Files pane.\nIf you are running into errors due to incorrect file paths, it can be useful to try to import your data using RStudio‚Äôs GUI to see where you are going wrong by comparing your own attempts with the import commands that RStudio generated.\n\n\n\n\nBarrett, Malcolm. 2018. Why should i use the here package when i‚Äôm already using projects? - malcolm barrett. https://malco.io/articles/2018-11-05-why-should-i-use-the-here-package-when-i-m-already-using-projects.\n\n\nBryan, Jennifer. 2018. Let‚Äôs git started | happy git and GitHub for the useR. Open Education Resource. https://happygitwithr.com/.\n\n\nBryan, Jenny. 2017. Project-oriented workflow. Tidyverse.org. https://www.tidyverse.org/blog/2017/12/workflow-vs-script/.\n\n\nBusterud, Guro, Anne Dahl, Dave Kush & Kjersti Faldet Listhaug. 2023. Verb placement in L3 french and L3 german: The role of language-internal factors in determining cross-linguistic influence from prior languages. Linguistic Approaches to Bilingualism. John 13(5). 693‚Äì716. https://doi.org/10.1075/lab.22058.bus.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nLausberg, Hedda & Han Sloetjes. 2009. Coding gestural behavior with the NEUROGES-ELAN system. Behavior Research Methods 41(3). 841‚Äì849. https://doi.org/10.3758/BRM.41.3.841.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N. Shahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced glossary of open scholarship terms. Nature Human Behaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nSchimke, Sarah, Israel de la Fuente, Barbara Hemforth & Saveria Colonna. 2018. First language influence on second language offline and online ambiguous pronoun resolution. Language Learning 68(3). 744‚Äì779. https://doi.org/10.1111/lang.12293.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "6_ImpoRtingData.html#footnotes",
    "href": "6_ImpoRtingData.html#footnotes",
    "title": "6¬† ImpoRting data",
    "section": "",
    "text": "‚ÄúDigital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films - the use of DOIs is not restricted to just scholarly or academic material. DOIs ‚Äúprovides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.‚Äù (https://doi.org/hb.html). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are Crossref and Datacite.‚Äù (Parsons et al. 2022)‚Ü©Ô∏é\nNote that, unlike the functions that we have used so far, the View() function begins with a capital letter. R is a case-sensitive programming language, which means that view() and View() are not the same thing!‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Impo`R`ting data</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html",
    "href": "7_VariablesFunctions.html",
    "title": "7¬† VaRiables and functions",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#inspecting-a-dataset-in-r",
    "href": "7_VariablesFunctions.html#inspecting-a-dataset-in-r",
    "title": "7¬† VaRiables and functions",
    "section": "7.1 Inspecting a dataset in R",
    "text": "7.1 Inspecting a dataset in R\nIn Section 6.6, we saw that we can use the View() function to display tabular data in a format that resembles that of a spreadsheet programme (see Figure¬†7.1).\nThe two datasets from DƒÖbrowska (2019) are both long and wide so you will need to scroll in both directions to view all the data. RStudio also provides a filter option and a search tool (see Figure¬†7.1). Note that both of these tools can only be used to visually inspect the data. You cannot alter the dataset in any way using these tools (and that‚Äôs a good thing!).\n\nView(L1.data)\n\n\n\n\n\n\n\nFigure¬†7.1: The L1.data object as visualised using the View() function in RStudio\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ1. The View() function is more user-friendly than attempting to examine the full table in the Console. Try to display the full L2.dataset in the Console by using the command L2.data which is shorthand for print(L2.data). What happens?\n\n\n\n\nThe R Console prints out all the data, but not the column headers.\n\n\nR produces an error message because there are too many rows and the Console window is not long enough.\n\n\nThe R Console prints the data in a randomly jumbled way.\n\n\nR only displays the first 22 rows and the columns are not aligned because the Console window is not wide enough.\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n\n\nIn practice, it is often useful to printing subsets of a dataset in the Console to quickly check the sanity of the data. To do so, we can use the function head() that prints the first six rows of a tabular dataset.\n\nhead(L1.data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant\nAge\nGender\nOccupation\nOccupGroup\nOtherLgs\nEducation\nEduYrs\nReadEng1\nReadEng2\nReadEng3\nReadEng\nActive\nObjCl\nObjRel\nPassive\nPostmod\nQ.has\nQ.is\nLocative\nSubCl\nSubRel\nGrammarR\nGrammar\nVocabR\nVocab\nCollocR\nColloc\nBlocks\nART\nLgAnalysis\n\n\n\n\n1\n21\nM\nStudent\nPS\nNone\n3rd year of BA\n17\n1\n2\n2\n5\n8\n8\n8\n8\n8\n8\n6\n8\n8\n8\n78\n95.0\n48\n73.33333\n30\n68.750\n16\n17\n15\n\n\n2\n38\nM\nStudent/Support Worker\nPS\nNone\nNVQ IV Music Performance\n13\n1\n2\n3\n6\n8\n8\n8\n8\n8\n8\n7\n8\n8\n8\n79\n97.5\n58\n95.55556\n35\n84.375\n11\n31\n13\n\n\n3\n55\nM\nRetired\nI\nNone\nNo formal (City and Guilds)\n11\n3\n3\n4\n10\n8\n8\n8\n8\n8\n7\n8\n8\n8\n8\n79\n97.5\n58\n95.55556\n31\n71.875\n5\n38\n5\n\n\n4\n26\nF\nWeb designer\nPS\nNone\nBA Fine Art\n17\n3\n3\n3\n9\n8\n8\n8\n8\n8\n8\n8\n8\n8\n8\n80\n100.0\n53\n84.44444\n37\n90.625\n20\n26\n15\n\n\n5\n55\nF\nHomemaker\nI\nNone\nO‚ÄôLevels\n12\n3\n2\n3\n8\n8\n8\n8\n8\n8\n8\n7\n8\n8\n8\n79\n97.5\n55\n88.88889\n36\n87.500\n16\n31\n14\n\n\n6\n58\nF\nRetired\nI\nNone\nO‚ÄôLevels\n12\n1\n1\n2\n4\n8\n5\n1\n8\n8\n7\n6\n7\n8\n8\n66\n65.0\n48\n73.33333\n21\n40.625\n8\n15\n3\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ2. Six is the default number of rows printed by the head() function. Have a look at the function‚Äôs help file using the command ?head to find out how to change this default setting. How would you get R to print the first 10 lines of L2.data?\n\n\n\n\nhead(L2.data, 10)\n\n\nhead(L2.data, n = 10L)\n\n\nhead(L2.data, rows = 10)\n\n\nhead(L2.data, n = 10)\n\n\nhead(L2.data n = 10L)\n\n\nhead(L2.data, L = 10)\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#working-with-variables",
    "href": "7_VariablesFunctions.html#working-with-variables",
    "title": "7¬† VaRiables and functions",
    "section": "7.2 Working with variables",
    "text": "7.2 Working with variables\n\n7.2.1 Types of variables\nIn statistics, we differentiate between numeric (or quantitative) and categorical (or qualitative) variables. Each variable type can be subdivided into different subtypes. It is very important to understand the differences between these types of data as we frequently have to use different statistics and visualisations depending on the type(s) of variable(s) that we are dealing with.\nSome numeric variables are continuous: they contain measured data that, at least theoretically, can have an infinite number of values within a range (e.g., time). In practice, however the number of possible values depends on the precision of the measurement (e.g., are we measuring time in years, as in the age of adults, or milliseconds, as in participants‚Äô reaction times in a linguistic experiment). Numeric variables for which only a defined set of values are possible are called discrete variables (e.g., number of occurrences of a word in a corpus). Most often, discrete numeric variables represent counts of something.\n\n\n\n\n\nCategorical variables can be nominal or ordinal. Nominal variables contain unordered categorical values (e.g., participants‚Äô mother tongue or nationality), whereas ordinal variables have categorical values that can be ordered meaningfully (e.g., participants‚Äô proficiency in a specific language where the values beginner, intermediate and advanced or A1, A2, B1, B2, C1 and C2 have a meaningful order). However, the difference between each category (or level) is not necessarily equal. Binary variables are a special case of nominal variable which only has two mutually exclusive outcomes (e.g., true or false in a quiz question).\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ3. Which type of variable is stored in the Occupation column in L1.data?\n\n\n\n\nDiscrete\n\n\nBinary\n\n\nContinuous\n\n\nOther\n\n\nNominal\n\n\n\n\n\n\n\n¬†\nQ4. Which type of variable is stored in the Gender column in L1.data?\n\n\n\n\nOther\n\n\nContinuous\n\n\nDiscrete\n\n\nNominal\n\n\nBinary\n\n\n\n\n\n\n\n¬†\nQ5. Which type of variable is stored in the column VocabR in L1.data?\n\n\n\n\nOther\n\n\nNominal\n\n\nContinuous\n\n\nDiscrete\n\n\nBinary\n\n\n\n\n\n\n\n¬†\n\n\n\n\n7.2.2 Inspecting variables in R\nIn tidy data tabular formats (see Chapter 8), each row corresponds to one observation and each column to a variable. Each cell, therefore, corresponds to a single data point, which is the value of a specific variable (column) for a specific observation (row). As we will see in the following chapters, this data structure allows for efficient and intuitive data manipulation, analysis, and visualisation.\nThe names() functions returns the names of all of the columns of a data frame. Given that the datasets from DƒÖbrowska (2019) are ‚Äòtidy‚Äô, this means that names(L1.data) returns a list of all the column names in the L1 dataset.\n\nnames(L1.data)\n\n [1] \"Participant\" \"Age\"         \"Gender\"      \"Occupation\"  \"OccupGroup\" \n [6] \"OtherLgs\"    \"Education\"   \"EduYrs\"      \"ReadEng1\"    \"ReadEng2\"   \n[11] \"ReadEng3\"    \"ReadEng\"     \"Active\"      \"ObjCl\"       \"ObjRel\"     \n[16] \"Passive\"     \"Postmod\"     \"Q.has\"       \"Q.is\"        \"Locative\"   \n[21] \"SubCl\"       \"SubRel\"      \"GrammarR\"    \"Grammar\"     \"VocabR\"     \n[26] \"Vocab\"       \"CollocR\"     \"Colloc\"      \"Blocks\"      \"ART\"        \n[31] \"LgAnalysis\" \n\n\n\n\n7.2.3 R data types\nA useful way to get a quick and informative overview of a large dataset is to use the function str(), which was mentioned in Section 6.6. It returns the ‚Äúinternal structure‚Äù of any R object. It is particular useful for large tables with many columns\n\nstr(L1.data)\n\n'data.frame':   90 obs. of  31 variables:\n $ Participant: chr  \"1\" \"2\" \"3\" \"4\" ...\n $ Age        : int  21 38 55 26 55 58 31 58 42 59 ...\n $ Gender     : chr  \"M\" \"M\" \"M\" \"F\" ...\n $ Occupation : chr  \"Student\" \"Student/Support Worker\" \"Retired\" \"Web designer\" ...\n $ OccupGroup : chr  \"PS\" \"PS\" \"I\" \"PS\" ...\n $ OtherLgs   : chr  \"None\" \"None\" \"None\" \"None\" ...\n $ Education  : chr  \"3rd year of BA\" \"NVQ IV Music Performance\" \"No formal (City and Guilds)\" \"BA Fine Art\" ...\n $ EduYrs     : int  17 13 11 17 12 12 13 11 11 11 ...\n $ ReadEng1   : int  1 1 3 3 3 1 3 2 1 2 ...\n $ ReadEng2   : int  2 2 3 3 2 1 2 2 1 2 ...\n $ ReadEng3   : int  2 3 4 3 3 2 3 3 1 2 ...\n $ ReadEng    : int  5 6 10 9 8 4 8 7 3 6 ...\n $ Active     : int  8 8 8 8 8 8 7 8 8 8 ...\n $ ObjCl      : int  8 8 8 8 8 5 8 4 7 5 ...\n $ ObjRel     : int  8 8 8 8 8 1 8 8 3 8 ...\n $ Passive    : int  8 8 8 8 8 8 8 8 2 8 ...\n $ Postmod    : int  8 8 8 8 8 8 7 7 6 8 ...\n $ Q.has      : int  8 8 7 8 8 7 8 1 3 0 ...\n $ Q.is       : int  6 7 8 8 7 6 7 8 7 8 ...\n $ Locative   : int  8 8 8 8 8 7 8 8 8 8 ...\n $ SubCl      : int  8 8 8 8 8 8 8 8 7 8 ...\n $ SubRel     : int  8 8 8 8 8 8 8 8 7 8 ...\n $ GrammarR   : int  78 79 79 80 79 66 77 68 58 69 ...\n $ Grammar    : num  95 97.5 97.5 100 97.5 65 92.5 70 45 72.5 ...\n $ VocabR     : int  48 58 58 53 55 48 39 48 31 42 ...\n $ Vocab      : num  73.3 95.6 95.6 84.4 88.9 ...\n $ CollocR    : int  30 35 31 37 36 21 29 33 22 29 ...\n $ Colloc     : num  68.8 84.4 71.9 90.6 87.5 ...\n $ Blocks     : int  16 11 5 20 16 8 8 10 7 9 ...\n $ ART        : int  17 31 38 26 31 15 7 10 6 6 ...\n $ LgAnalysis : int  15 13 5 15 14 3 4 5 2 6 ...\n\n\nAt the top of its output, the function str(L1.data) first informs us that L1.data is a data frame object, consisting of 90 observations (i.e.¬†rows) and 31 variables (i.e.¬†columns). Then, it returns a list of all of the variables included in this data frame. Each line starts with a $ sign and corresponds to one column. First, the name of the column (e.g.¬†Occupation) is printed, followed by the column‚Äôs R data type (e.g.¬†chr for a character string vector), and then its values for the first few rows of the table (e.g.¬†we can see that the first participant in this dataset was a ‚ÄúStudent‚Äù and the second a ‚ÄúStudent/Support Worker‚Äù).\nCompare the outputs of the str() and head() functions in the Console with that of the View() function to understand the different ways in which the same dataset can be examined in RStudio.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ6. Use the str() function to examine the internal structure of the L2 dataset. How many columns are there in the L2 dataset?\n\n\n\n\n\n\n\n\n\n\n¬†\nQ7. Which of these columns can be found in the L2 dataset, but not the L1 one?\n\n\n\n\nEngWork\n\n\nOtherLgs\n\n\nArrival\n\n\nFirstExp\n\n\nNativeLg\n\n\nEduYrs\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ8. Which type of R object is the variable Arrival stored as?\n\n\n\n\nintelligence\n\n\ndigit\n\n\nindex\n\n\ninterest\n\n\nstring character\n\n\ninteger\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ9. How old was the third participant listed in the L2 dataset when they first moved to an English-speaking country?\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ10. In both datasets, the column Participant contains anonymised participant IDs. Why is the variable Participant stored as string character vector in L1.data, but as an integer vector in L2.data?\n\n\n\n\nBecause the L1 participants' IDs only contain whole numbers with no decimal points.\n\n\nBecause there are more L1 participants than L2 participants.\n\n\nBecause some of the L1 participants' IDs contain letters as well as numbers.\n\n\nBecause the L1 participants' IDs are written out as words rather than digits.\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n\n\n\n\n7.2.4 Accessing individual columns in R\nWe can call up individual columns within a data frame using the $ operator. This displays all of the participants‚Äô values for this one variable. As shown below, this works for any type of data.\n\nL1.data$Gender\n\n [1] \"M\" \"M\" \"M\" \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"F\" \"M\" \"F\" \"M\" \"F\" \"F\"\n[20] \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"M\" \"F\" \"M\" \"F\" \"M\" \"F\" \"F\" \"F\" \"M\" \"F\" \"F\" \"M\" \"F\"\n[39] \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"M\"\n[58] \"F\" \"F\" \"M\" \"M\" \"M\" \"M\" \"F\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"F\" \"M\" \"F\" \"F\"\n[77] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n\nL1.data$Age\n\n [1] 21 38 55 26 55 58 31 58 42 59 32 27 60 51 32 29 41 57 60 18 41 60 21 25 26\n[26] 60 57 60 52 25 23 42 59 30 21 21 60 51 62 65 19 65 29 38 37 42 20 32 29 29\n[51] 27 28 29 25 33 25 25 25 52 25 53 22 65 60 61 65 65 61 30 30 32 30 39 29 55\n[76] 18 32 31 20 38 44 18 17 17 17 17 17 17 17 17\n\n\nBefore doing any data analysis, it is crucial to carefully visually examine the data to spot any problems. Ask yourself:\n\nDo the values look plausible?\nAre there any missing values?\n\nLooking at the Gender and Age variables, we can see that all the L1 participants declared being either ‚Äòmale‚Äô (\"M\") or ‚Äòfemale‚Äô (\"F\"), that the youngest were 17 years old, and that no participant was improbably old. A single improbable value is likely to be the result of a data entry error, e.g.¬†a participant or researcher entered 188 as an age, instead of 18. If you spot lots of improbable or outright weird values (e.g.¬†C, I and PS as age values!), something is likely to have gone wrong during the data import process (see Section 6.6).\nJust like we can save individual numbers and words as R objects to our R environment, we can also save individual variables as individual R objects. As we saw in Section 5.3, in this case, the values of the variable are not printed in the Console, but rather saved to our R environment.\n\nL1.Occupation &lt;- L1.data$Occupation\n\nIf we want to display the content of this variable, we must print our new R object by calling it up with its name, e.g.¬†L1.Occupation. Try it out! As listing all of the all of the L1 participant‚Äôs jobs makes for a very long list, below, we only display the first six values using the head() function.\n\nhead(L1.Occupation)\n\n[1] \"Student\"                \"Student/Support Worker\" \"Retired\"               \n[4] \"Web designer\"           \"Homemaker\"              \"Retired\"",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#sec-SquareBrackets",
    "href": "7_VariablesFunctions.html#sec-SquareBrackets",
    "title": "7¬† VaRiables and functions",
    "section": "7.3 Accessing individual data points in R",
    "text": "7.3 Accessing individual data points in R\nWe can also access individual data points from a variable using the index operator, the square brackets ([]). For example, we can access the Occupation value for the fourth L1 participant by specifying that we only want the fourth element of the R object L1.Occupation.\n\nL1.Occupation[4]\n\n[1] \"Web designer\"\n\n\nWe can also do this from the L1.data data frame object directly. To this end, we use a combination of the $ and the [] operators.\n\nL1.data$Occupation[4]\n\n[1] \"Web designer\"\n\n\nWe can access a continuous range of data points using the : operator.\n\nL1.data$Occupation[10:15]\n\n[1] \"Housewife\"             \"Admin Assistant\"       \"Content Editor\"       \n[4] \"School Crossing Guard\" \"Carer/Cleaner\"         \"IT Support\"           \n\n\nOr, if they are not continuous, we can list the numbers of the values that we are interesting in using the combine function (c()) and commas separating each index value.\n\nL1.data$Occupation[c(11,13,29,90)]\n\n[1] \"Admin Assistant\"       \"School Crossing Guard\" \"Dental Nurse\"         \n[4] \"Student\"              \n\n\nIt is also possible to access data points from a table by specifying both the number of the row and the number of the column of the relevant data point(s) using the following pattern:\n[row,column]\nFor example, given that we know that Occupation is stored in the fourth column of L1.data, we can find out the occupation of the L1 participant in the 60th row of the dataset like this:\n\nL1.data[60,4]\n\n[1] \"Train Driver\"\n\n\nAll of these approaches can be combined. For example, here we access the values of the second, third, and fourth columns for the 11th, 13th, 29th, and 90th L1 participants.\n\nL1.data[c(11,13,29,90),2:4]\n\n   Age Gender            Occupation\n11  32      F       Admin Assistant\n13  60      M School Crossing Guard\n29  52      F          Dental Nurse\n90  17      M               Student\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nThe following two quiz questions focus on the NativeLg variables from the L2 dataset (L2.data).\nQ11. Use the index operators to find out the native language of the 26th L2 participant.\n\n\n\n\nCantonese\nChinese\nGerman\nItalian\nGreek\nLithuanian\nMandarin\nPolish\nRussian\nSpanish\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ12. Which command(s) can you use to display only the Gender, Occupation, Native language, and Age of the last participant listed in the L2 dataset?\n\n\n\n\nL2.data[-1:c(2,3,5,9)]\n\n\nL2.data[67,c(2:3,5,9)]\n\n\nL2.data[67,c(2,3,5,9)]\n\n\nL2.data[90,c(2:3,5,9)]\n\n\nL2.data[67 , c(2,3,5,9)]\n\n\nL2.data[67:c(2,3,5,9)]\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#sec-RFunctions",
    "href": "7_VariablesFunctions.html#sec-RFunctions",
    "title": "7¬† VaRiables and functions",
    "section": "7.4 Using built-in R functions",
    "text": "7.4 Using built-in R functions\nWe know from our examination of the L1 dataset from DƒÖbrowska (2019) that it includes 90 English native speaker participants. To find out their mean average age, we could add up all of their ages and divide the sum by 90 (see Section 8.1 for more ways to report the central tendency of a variable).\n\n(21 + 38 + 55 + 26 + 55 + 58 + 31 + 58 + 42 + 59 + 32 + 27 + 60 + 51 + 32 + 29 + 41 + 57 + 60 + 18 + 41 + 60 + 21 + 25 + 26 + 60 + 57 + 60 + 52 + 25 + 23 + 42 + 59 + 30 + 21 + 21 + 60 + 51 + 62 + 65 + 19 + 65 + 29 + 38 + 37 + 42 + 20 + 32 + 29 + 29 + 27 + 28 + 29 + 25 + 33 + 25 + 25 + 25 + 52 + 25 + 53 + 22 + 65 + 60 + 61 + 65 + 65 + 61 + 30 + 30 + 32 + 30 + 39 + 29 + 55 + 18 + 32 + 31 + 20 + 38 + 44 + 18 + 17 + 17 + 17 + 17 + 17 + 17 + 17 + 17) / 90\n\n[1] 37.54444\n\n\nOf course, we would much rather not write all of this out! Especially, as we are very likely to make errors in the process. Instead, we can use the base R function sum() to add up all of the L1 participant‚Äôs ages and divide that by 90.\n\nsum(L1.data$Age) / 90\n\n[1] 37.54444\n\n\nThis already looks much better, but it‚Äôs still less than ideal: What if we decided to exclude some participants (e.g., because they did not complete all of the experimental tasks)? Or decided to add data from more participants? In both these cases, 90 will no longer be the correct denominator to calculate their average age! That‚Äôs why it is better to work out the denominator by computing the total number of values in the variable of interest. To this end, we can use the length() function, which returns the number of values in any given vector.\n\nlength(L1.data$Age)\n\n[1] 90\n\n\nWe can then combine the sum() and the length() functions to calculate the participants‚Äô average age.\n\nsum(L1.data$Age) / length(L1.data$Age)\n\n[1] 37.54444\n\n\nBase R includes lots of useful functions, especially to do statistics. Hence, it will come as no surprise to find that there is a built-in function to calculate mean average values. It is called mean() and is very simple to use.\n\nmean(L1.data$Age)\n\n[1] 37.54444\n\n\nIf you save the values of a variable to your R session environment, you do not need to use the name of the dataset and the $ sign to calculate its mean. Instead, you can directly apply the mean() function to the stored R object.\n\n# Saving the values of the Age variable to a new R object called L1.Age:\nL1.Age &lt;- L1.data$Age\n\n# Applying the mean() function to this new R object:\nmean(L1.Age)\n\n[1] 37.54444\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ13. How does the average age of the L2 participants in DƒÖbrowska (2019) compare to that of the L1 participants?\n\n\n\n\nAge is not comparable across two different datasets.\n\n\nOn average, the L2 participants are younger than the L1 participants.\n\n\nOn average, the L2 participants are older than the L1 participants.\n\n\nOn average, the L2 participants are the same age than the L1 participants.\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nFor this task, you first need to check that you have saved the following two variables from the L1 dataset to your R environment.\n\nL1.Age &lt;- L1.data$Age\nL1.Occupation &lt;- L1.data$Occupation\n\n1) Below is a list of useful base R functions. Try them out with the variable L1.Age. What does each function do? Make a note by writing a comment next to each command (see Section 5.4.4). The first one has been done for you.\n\nmean(L1.Age) # The mean() function returns the mean average of a set of number.\nmin()\nmax()\nsort()\nlength()\nmode()\nclass()\ntable()\nsummary()\n\n2) Age is a numeric variable. What happens if you try these same functions with a character string variable? Find out by trying them out with the variable L1.Occupation which contains words rather than numbers.\n¬†\n\n\n\n\n\n\n\n\nClick here for the solutions to Task 1\n\n\n\n\n\nAs you will have seen, often the clue is in the name of the function - but not always! üòâ\n\nmean(L1.Age) # The mean() function returns the mean average of a set of number.\nmean(L1.Occupation) # It does not make sense to calculate a mean average value of a set of words, therefore R returns an 'NA' (not applicable) and a warning in red explaining that the mean() function expects a numeric or logical argument.\n\nmin(L1.Age) # For a numeric variable, min() returns the lowest numeric value.\nmin(L1.Occupation) # For a string variable, min() returns the first value sorted alphabetically.\n\nmax(L1.Age) # For a numeric variable, min() returns the highest numeric value.\nmax(L1.Occupation) # For a string variable, max() returns the last value sorted alphabetically.\n\nsort(L1.Age) # For a numeric variable, sort() returns all of the values of the variable ordered from the smallest to the largest.\nsort(L1.Occupation) # For a string variable, sort() returns of all of the values of the variable in alphabetical order.\n\nlength(L1.Age) # The function length() returns the number of values in the variable.\nlength(L1.Occupation) # The function length() returns the number of values in the variable.\n\nmode(L1.Age) # The function mode() returns the R data type that the variable is stored as.\nmode(L1.Occupation) # The function mode() returns the R data type that the variable is stored as.\n\nclass(L1.Age) # The function mode() returns the R object class that the variable is stored as.\nclass(L1.Occupation) # The function mode() returns the R object class that the variable is stored as.\n\ntable(L1.Age) # For a numeric variable, the function table() outputs a table that tallies the number of occurrences of each unique value in a set of values and sorts them in ascending order.\ntable(L1.Occupation) # For a string variable, the function table() outputs a table that tallies the number of occurrences of each unique value in a set of values and sorts them alphabetically.\n\nsummary(L1.Age) # For a numeric variable, the function summary() outputs six values that, together, summarise the set of values contained in this variable: the minimum and maximum values, the first and third quartiles (more on this in Chapter *), and the mean and median (more on this in Chapter *).\nsummary(L1.Occupation) # For a string variable, the summary() function only outputs the length of the string vector, its object class and data mode. \n\n\n\n\n\n7.4.1 Function arguments\nAll of the functions that we have looked at this chapter so far work with just a single argument: either a vector of values (e.g.¬†a variable from our dataset as in mean(L1.data$Age)) or an entire tabular dataset (e.g.¬†str(L1.data)). When we looked at the head() function, we saw that, per default, it displays the first six rows but that we can change this by specifying a second argument in the function. In R, arguments within a function are always separated by a comma.\n\nhead(L1.Age, n = 6)\n\n[1] 21 38 55 26 55 58\n\n\nThe names of the argument can be specified but do not have to be if they are listed in the order specified in the documentation. You can check the ‚ÄòUsage‚Äô section of a function‚Äôs help file (e.g.¬†using help(head) function or ?head) to find out the order of the arguments. Run the following commands and compare their output:\n\nhead(x = L1.Age, n = 6)\nhead(L1.Age, 6)\nhead(n = 6, x = L1.Age)\nhead(6, L1.Age)\n\nWhilst the first three return exactly the same output, the fourth returns an error because the argument names are not specified and are not in the order specified in the function‚Äôs help file. To avoid making errors and confusing your collaborators and/or future self, it‚Äôs good practice to explicitly name all the arguments except the most obvious ones.\n\n\n\n\n\n\nQuiz time!\n\n\n\nLook at the two lines of code and their outputs below.\n\nL1.data$Vocab\n\n [1] 73.333333 95.555556 95.555556 84.444444 88.888889 73.333333 53.333333\n [8] 73.333333 35.555556 60.000000 40.000000 95.555556 86.666667 53.333333\n[15] 88.888889 46.666667 86.666667 84.444444 86.666667 77.777778 93.333333\n[22] 91.111111 68.888889 82.222222 75.555556 80.000000 86.666667 88.888889\n[29] 75.555556 57.777778 88.888889 95.555556 60.000000 77.777778 55.555556\n[36] 80.000000 88.888889 93.333333 93.333333 95.555556 75.555556 77.777778\n[43] 82.222222 80.000000 44.444444 62.222222 57.777778 93.333333 57.777778\n[50] 66.666667 48.888889 77.777778 51.111111 68.888889 80.000000 80.000000\n[57] 55.555556 77.777778 80.000000 82.222222 91.111111 71.111111 28.888889\n[64] 82.222222 80.000000 62.222222 95.555556 68.888889 13.333333  8.888889\n[71] 26.666667 37.777778 55.555556 82.222222 86.666667 40.000000 86.666667\n[78] 71.111111 46.666667 64.444444 60.000000 22.222222 64.444444 48.888889\n[85] 42.222222 60.000000 53.333333 42.222222 51.111111 68.888889\n\nround(L1.data$Vocab)\n\n [1] 73 96 96 84 89 73 53 73 36 60 40 96 87 53 89 47 87 84 87 78 93 91 69 82 76\n[26] 80 87 89 76 58 89 96 60 78 56 80 89 93 93 96 76 78 82 80 44 62 58 93 58 67\n[51] 49 78 51 69 80 80 56 78 80 82 91 71 29 82 80 62 96 69 13  9 27 38 56 82 87\n[76] 40 87 71 47 64 60 22 64 49 42 60 53 42 51 69\n\n\nQ14. Based on your observations, what does the round() function do?\n\n\n\n\nThe round() function rounds off numbers to the nearest whole number.\n\n\nThe round() function is designed to save screen space for smaller displays.\n\n\nThe round() function displays just the first two digits of any number.\n\n\nThe round() function displays fewer values for ease of reading.\n\n\n\n\n\n\n\n¬†\nQ15. Check out the ‚ÄòUsage‚Äô section of the help file on the round() function to find out how to round the Vocab values in the L1 dataset to two decimal places. How can this be achieved?\n\n\n\n\nround(L1.data$Vocab, 2)\n\n\nround(L1.data$Vocab, digits = -4)\n\n\nround(L1.data$Vocab: digits = 2)\n\n\nround(L1.data$Vocab, digits = 2)\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#combining-functions-in-r",
    "href": "7_VariablesFunctions.html#combining-functions-in-r",
    "title": "7¬† VaRiables and functions",
    "section": "7.5 Combining functions in R",
    "text": "7.5 Combining functions in R\nCombining functions is where the real fun starts with programming! In Section 7.4, we already combined two functions using a mathematical operator (/). But what if we want to compute L1 participant‚Äôs average age to two decimal places? To do this, we need to combine the mean() function and the round() function. We can do this in two steps.\n\n# Step 1:\nL1.mean.age &lt;- mean(L1.Age)\n# Step 2:\nround(L1.mean.age, digits = 2)\n\n[1] 37.54\n\n\nIn step 1, we compute the mean value and save it as an R object and, in step 2, we pass this object through the round() function with the argument digits = 2. There is nothing wrong with this method, but it often require lots of intermediary R objects, which can get rather tiresome.\nIn the following, we will look at two further ways to combine functions in R: nesting and piping.\n\n7.5.1 Nested functions\nThe first method involves lots of brackets (also known as ‚Äòparentheses‚Äô). This is because in nested functions, one function is placed inside another function. The inner function is evaluated first, and its result is passed to the next outer function. Here‚Äôs an example:\n\nround(mean(L1.Age))\n\n[1] 38\n\n\nIn this example, the mean() function is nested inside the round() function. The mean() function calculates the mean of L1.Age, and the result is passed to the round() function, which rounds the result to the nearest integer.\nYou can also pass additional arguments to any of the functions, but you must make sure that you place the arguments within the correct set of brackets.\n\nround(mean(L1.Age), digits = 2)\n\n[1] 37.54\n\n\nIn this example, the argument digits = 2 belongs to the outer function round(); hence it must be placed within the outer set of brackets.\nIn theory, you can nest as many functions as you like, but things can get quite chaotic after more than a couple of functions. You need to make sure that you can trace back which arguments and which brackets belong to which function (see Figure¬†7.2).\n\n\n\n\n\n\nFigure¬†7.2: A schematic representations of a) one function with two arguments, b) two nested functions each with two arguments, and c) three nested functions each with two arguments\n\n\n\n\n\n\n\n\n\nTime to think!\n\n\n\nConsider the three lines of code below. Without running them, can you tell which of the three lines of code will output the square root of L1 participant‚Äôs average age to two decimal places?\n\nround(sqrt(mean(L1.Age) digits = 2))\n\nsqrt(round(mean(L1.Age), digits = 2))\n\nround(sqrt(mean(L1.Age)), digits = 2)\n\nThe first line will return an ‚Äúunexpected symbol‚Äù error because it is missing a comma before the argument digits = 2. The second line actually outputs 6.126989, which has more than two decimal places! This is because R interprets the functions from the inside out: first, it calculates the mean value, then it rounds that off to two decimal places, and only then does it compute the square root of that rounded off value. The third line, in contrast, does the rounding operation as the last step. Note that, in the two lines of code that do not produce an error, the brackets around the argument digits = 2 are also located in different places.\nIt is very easy to make bracketing errors when writing code and especially so when nesting functions (see Figure¬†7.2). Watch your commas and brackets (see also Section 5.6)!\n\n\n\n\n7.5.2 Piped Functions\nIf you found all these brackets overwhelming: fear not! There is a second method for combining functions in R, which is often more convenient and almost always easier to decipher. It involves the pipe operator, which in R is |&gt;.1\nThe |&gt; operator passes the output of one function on to the first argument of the next function. This allows us to chain multiple functions together in a much more intuitive way.\n\nL1.Age |&gt; \n mean() |&gt; \n round()\n\n[1] 38\n\n\nIn this example, the object L1.Age is passed on to the first argument of the mean() function. This calculates the mean of L1.Age. Next, this result is passed to the round() function, which rounds the mean value to the nearest integer.\nIf we want to pass additional arguments to any function in the pipeline, we simply at it in the brackets corresponding to the function in question.\n\nL1.Age |&gt; \n mean() |&gt; \n round(digits = 2)\n\n[1] 37.54\n\n\nLike many of the relational operators we saw in Section 5.5, the R pipe is a combination of two symbols, the computer pipe symbol | and the right angle bracket &gt;. Don‚Äôt worry if you‚Äôre not sure where these two symbols are on your keyboard as RStudio has a handy shortcut for you: Ctrl/Cmd + Shift + M2 (see Figure¬†7.3). I strongly recommend that you write this shortcut on a prominent post-it and learn it asap, as you will need it a lot when you are working in R!\n\n\n\n\n\n\nFigure¬†7.3: Remix of Ren√© Magritte‚Äôs ‚ÄúLa Trahison des images‚Äù (1928-1929) with the native R pipe and its RStudio shortcut (based on an image from Wikiart.org). This image is licensed under CC-BY Elen Le Foll3.\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ16. Using the R pipe operator, calculate the average mean age of the L2 participants and round off this value to two decimal places. What is the result?\n\n\n\n\n\n\n\n\n\n\n¬†\nQ17. Unsurprisingly, in DƒÖbrowska (2019)‚Äòs study, English L1 participants, on average, scored higher in an English vocabulary test than L2 participants. Calculate the difference between L1 and L2 participants‚Äô mean Vocab test results and round off this means difference to two decimal places.\n\n\n\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\nClick here for a detailed answer to Q16\n\n\n\n\n\nThey are lots of ways to tackle question 16. Here is one approach:\n\n(mean(L1.data$Vocab) - mean(L2.data$Vocab)) |&gt; \n  round(digits = 2)\n\n[1] 16.33\n\n\nNote that this approach requires a set of brackets around the first subtraction operation, otherwise only the second mean value is rounded off to two decimal places. Compare the following lines of code:\n\nmean(L1.data$Vocab) - mean(L2.data$Vocab)\n\n[1] 16.33315\n\n(mean(L1.data$Vocab) - mean(L2.data$Vocab)) |&gt; \n  round(digits = 2)\n\n[1] 16.33\n\nmean(L1.data$Vocab) - round(mean(L2.data$Vocab), digits = 2)\n\n[1] 16.3358\n\n\nAnother solution would be to store the difference in means as an R object and pass this object to the round() function.\n\nmean.diff.vocab &lt;- mean(L1.data$Vocab) - mean(L2.data$Vocab)\nround(mean.diff.vocab, digits = 2)\n\n[1] 16.33\n\n\nOr, if you want to use the pipe:\n\nmean.diff.vocab &lt;- mean(L1.data$Vocab) - mean(L2.data$Vocab)\nmean.diff.vocab |&gt; \n  round(digits = 2)\n\n[1] 16.33\n\n\n\n\n\n\n\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "7_VariablesFunctions.html#footnotes",
    "href": "7_VariablesFunctions.html#footnotes",
    "title": "7¬† VaRiables and functions",
    "section": "",
    "text": "This is the native R pipe operator, which was introduced in May 2021 with R version 4.1.0. As a result, you will not find it in code written in earlier versions of R. Previously, piping required an additional R library, the {magrittr} library. The {magrittr} pipe looks like this: %&gt;%. At first sight, they appear to work is in the same way, but there are some important differences. If you are familiar with the {magrittr} pipe and want to understand how it differs from the native R pipe, I recommend this excellent blog post by Isabella Vel√°squez: https://ivelasq.rbind.io/blog/understanding-the-r-pipe/.‚Ü©Ô∏é\nIf, in your version of RStudio, this shortcut produces %&gt;% instead of |&gt;, you have probably not activated the native R pipe option in your RStudio global options (see instructions in Section 4.3.1).‚Ü©Ô∏é\nI would appreciate you referencing this textbook or textbook chapter when reusing this image. Thank you!‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Va`R`iables and functions</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html",
    "href": "8_DescriptiveStats.html",
    "title": "8¬† DescRiptive statistics",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#sec-CentralTendency",
    "href": "8_DescriptiveStats.html#sec-CentralTendency",
    "title": "8¬† DescRiptive statistics",
    "section": "8.1 Measures of central tendency",
    "text": "8.1 Measures of central tendency\nIn Section 7.4, we calculated the mean average age of L1 and L2 participants. Averages are a very useful way to describe the central tendency of a numeric variable - both in science and everyday life. For example, it is useful for me to know if a particular bus journey lasts, on average, 12 minutes or 45 minutes. As it‚Äôs an average value, I am not expecting it to last exactly 12 or 45 minutes, but the average duration is nonetheless helpful to plan my schedule.\nIn science, we use averages to describe the central tendency of numeric variables that are too large for us to be able to examine every single data point. With very small datasets, averages are unnecessary. Imagine that a Breton1 language class in Fiji has five students. Their teacher hardly needs to calculate an average of the students‚Äô vocabulary test results to get an understanding of how her students are doing. She can simply examine all five results!\nNot only are averages of very small datasets unnecessary, they can, in fact, be misleading. Imagine that the five Breton learners got the following results (out of 100) on their vocabulary test:\n\n5, 82, 86, 89, 91\n\nIf we calculate the average result of the class, we get:\n\nmean(c(5, 82, 87, 89, 91))\n\n[1] 70.8\n\n\nThis average grade does not describe very well how any of the students did: Four did much better than that, while one did considerably worse! The results of quantitative studies, however, typically involve much larger datasets so that averages can be a very useful way to describe central tendencies within the data. But it‚Äôs important to understand that, depending on the data, different measures of central tendency make sense. Later on, we will also see that measures of central tendency do not suffice to describe numeric variables: measures of variability (Section 8.3) and good data visualisation (Chapter 10) are also crucial.\n\n8.1.1 Mean\nThe measure of central tendency that we have looked at so far is the arithmetic mean. When people speak of averages, they typically mean mean values.\nIn Section 7.4, we saw that means are calculated by adding up all the values and dividing the sum by the total of values.\n\nsum((c(5, 82, 87, 89, 91))) / 5\n\n[1] 70.8\n\n\nMeans are useful because they are commonly reported and widely understood. Their disadvantage is that they are very susceptible to outliers and skew (which far fewer people actually understand, see Section 8.2). As we saw in the example above, the fact that one ‚Äòoutlier‚Äô learner did very poorly in her Breton vocabulary test led to a much lower average grade than we would expect considering that the other four test-takers did much better than the mean.\nMeans are also frequently misinterpreted as ‚Äúmost likely value‚Äù. This is rarely the case. For example, in this example, 71.2 is not a score that any of the five students obtained!\n\n\n8.1.2 Median\nAnother way to report the central tendency of a set of numeric values like test results is to look for its ‚Äúmiddle value‚Äù. If we order our five Breton learners‚Äô test results from the lowest to the highest value, we can see that the middle value is 86. This is the median.\n\n5, 82, 86, 89, 91\n\nFor datasets with an even number of values (e.g., 2, 4, 6, 8, etc.), we take the mean of the two middle values. Hence, in the following extended dataset with six Breton learners, the median test score is 86.5 because the two middle test results are 86 and 87 and (86¬†+¬†87)¬†/¬†2¬†=¬†86.5.\n\n5, 82, 86, 87, 89, 91\n\nBy now, you will probably not be surprised to learn that there is an R function called median(), which allows us to easily calculate the median value of any set of numbers.\n\nmedian(c(5, 82, 86, 89, 91))\n\n[1] 86\n\nmedian(c(5, 82, 86, 87, 89, 91))\n\n[1] 86.5\n\n\nNow, it‚Äôs time to turn to real data!\n\n\n8.1.3 Mode\nThe mean and median are measures of central tendency that only work with numeric variables. However, data in the language sciences frequently also includes categorical data (see Section 7.2.1). In the data from DƒÖbrowska (2019), this includes variables such as Gender, NativeLg, OtherLgs, and Occupation. We also need to be able to describe these variables as part of our data analysis. For such categorical variables, the only available measure of central tendency is the mode, which corresponds to the most frequent value in a variable.\nThe table() function outputs how often each unique value occurs in a variable.\n\ntable(L1.data$Gender)\n\n\n F  M \n48 42 \n\n\nFrom this output, we can tell that the mode of the Gender variable in the L1 dataset is F, which stands for ‚Äúfemale‚Äù.\nWhen there are many different unique values (or levels), it makes sense to order them according to their frequency. To do so, we can pipe the output of the table() function into the sort() function (piping was covered in Section 7.5.2). Note that, by default, R sorts by ascending order (decreasing = FALSE). We can change this default to TRUE.\n\ntable(L1.data$Occupation) |&gt; \n  sort(decreasing = TRUE)\n\n\n                  Retired                   Student                Unemployed \n                       14                        14                         4 \n                Housewife            Shop Assistant                   Teacher \n                        3                         3                         3 \n          Admin Assistant            Factory Worker                 Policeman \n                        2                         2                         2 \n        Quantity Surveyor             Admin Officer             Administrator \n                        2                         1                         1 \n              Boilermaker             Care Assisant             Carer/Cleaner \n                        1                         1                         1 \n       Catering Assistant             Civil Servant                   Cleaner \n                        1                         1                         1 \n                    Clerk            Content Editor    Creative Writing Tutor \n                        1                         1                         1 \n Customer Service Advisor              Dental Nurse         Finance Assistant \n                        1                         1                         1 \n   Functions Co-ordinator                 Homemaker           Human Resources \n                        1                         1                         1 \n               IT Support             Manual worker           Nail Technician \n                        1                         1                         1 \nOffice Admin Co-Ordinator         P/T Administrator         Personal Searcher \n                        1                         1                         1 \n      Project Coordinator              Receptionist                    Roofer \n                        1                         1                         1 \n          Sales Assistant     School Crossing Guard    School Crossing Patrol \n                        1                         1                         1 \n          Senior Lecturer                    Singer         Student (college) \n                        1                         1                         1 \n   Student/Support Worker     Supermarket Assistant            Support Worker \n                        1                         1                         1 \n             Train Driver                 Unemploed       University lecturer \n                        1                         1                         1 \n                 Waitress       Warehouse Operative              Web designer \n                        1                         1                         1 \n\n\nWe can see that, among the L1 participants, there were as many ‚ÄúRetired‚Äù participants as there were ‚ÄúStudent‚Äù participants.2 Hence, we have two modes. In general, modal values rarely make good summaries of variables with many different possible values or levels. This is why the mode is not suitable for numeric variables, unless there are only a few possible discrete numeric values (e.g., the values of a five or seven-point Likert scale3).\nCross-tabulations of more than one categorical variable (or numeric variable with just a few unique values) can easily be generated using the table() function. In the following, we cross-tabulate the additional languages that the L1 participants speak with their gender. This allows us to see that most male and female L1 participants did not speak another language other than English. Hence, for both the male and female subsets of L1 participants the mode of the variable OtherLgs is ‚ÄúNone‚Äù.\n\ntable(L1.data$OtherLgs, L1.data$Gender)\n\n         \n           F  M\n  French   1  1\n  German   2  1\n  None    44 40\n  Spanish  1  0\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nIn comparative studies, it is important to ensure that comparisons are fair and meaningful. For example, it would probably not be very meaningful to compare the linguistic knowledge of a group of undergraduate student learners of English with a group of retired native speakers. In this quiz, you will examine how similar the L1 and the L2 participants in DƒÖbrowska (2019) were in terms of age.\nQ1. What was the mean age of the L1 participants in DƒÖbrowska (2019)? Use the round() function to round off the mean value to two decimal places (see Section 7.5.2 for a reminder as to how to combine two functions).\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ2. On average, were the L1 participants in DƒÖbrowska (2019) older or younger than the L2 participants?\n\n\n\n\nOn average, the L2 participants were older.\n\n\nIt's impossible to tell based on the available data.\n\n\nIt depends whether you base the comparison on mean or median values.\n\n\nOn average, the L1 participants were older.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ3. Which of the following statements is true about the L1 and L2 participants in DƒÖbrowska (2019)?\n\n\n\n\nThe difference between the average ages of the two groups is greater when comparing mean than median ages.\n\n\nThe difference between the average ages of the two groups is greater when comparing median than mean ages.\n\n\nThe difference remains the same no matter what type of central tendency measure is used.\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#sec-Distributions",
    "href": "8_DescriptiveStats.html#sec-Distributions",
    "title": "8¬† DescRiptive statistics",
    "section": "8.2 Distributions",
    "text": "8.2 Distributions\nData analysis typically begins with the description of individual variables from a dataset. This is referred to as univariate descriptive statistics and is all about describing the distribution of the variables. A distribution is a way to summarise how the values of a variable are dispersed. It tells us things like the variable‚Äôs most frequent values, its range of values, and how the values are clustered or spread out. Examining the shapes and patterns of distributions can help us understand the typical values of the variables of our data, identify outliers, and make informed decisions about how to analyse and visualise our data.\n\n8.2.1 Distributions of categorical variables\nTables can be an effective way to examine the distribution of categorical variables. The table() function outputs the frequency of each level of a categorical variable. By default, the levels are ordered alphabetically.\n\ntable(L1.data$OtherLgs)\n\n\n French  German    None Spanish \n      2       3      84       1 \n\n\nWe saw that we can use the sort() function to change this behaviour.\n\ntable(L1.data$OtherLgs) |&gt; \n    sort(decreasing = TRUE)\n\n\n   None  German  French Spanish \n     84       3       2       1 \n\n\nThe proportions() function allows us to describe the frequency of each level of a categorical variable as a proportion of all data points. This is especially useful if we want to compare the distribution of a categorical variable across different (sub)datasets of different sizes.\n\ntable(L1.data$OtherLgs) |&gt; \n  sort(decreasing = TRUE) |&gt; \n  proportions()\n\n\n      None     German     French    Spanish \n0.93333333 0.03333333 0.02222222 0.01111111 \n\n\nWhen computing proportions, 0 corresponds to 0% and 1 to 100%. If we want to obtain percentages, we therefore need to multiply these numbers by 100. We can therefore see that more than 90% of L1 participants reported not being competent in any language other than English, their native language.\n\nOtherLgs.prop &lt;- \n  table(L1.data$OtherLgs) |&gt; \n  sort(decreasing = TRUE) |&gt; \n  proportions()*100\n\nOtherLgs.prop\n\n\n     None    German    French   Spanish \n93.333333  3.333333  2.222222  1.111111 \n\n\nTo round the values to two decimal places, we can pipe the R object that we created in the previous chunk (OtherLgs.prop) into the round() function.\n\nOtherLgs.prop |&gt; \n  round(digits = 2)\n\n\n   None  German  French Spanish \n  93.33    3.33    2.22    1.11 \n\n\nIn addition to using frequency tables, we can visualise data distributions graphically. Bar plots allow us to easily compare the distribution of categorical variables across different datasets and subsets of data. For example, in Figure¬†8.1, we can see that the distribution of additional languages spoken by the L1 participants is very similar in both the female and the male subset of participants.\n\n\n\n\n\n\n\n\nFigure¬†8.1: Additional languages spoken by L1 participants in DƒÖbrowska (2019)\n\n\n\n\n\nIn Chapter 10, you will learn how to make plots like Figure¬†8.1 in R using the {ggplot2} package.\n\n\n8.2.2 Distributions of numeric variables\nIn DƒÖbrowska (2019), on average, the L2 participants were younger than the L1 participants.\n\nmean(L1.data$Age) - mean(L2.data$Age)\n\n[1] 4.828027\n\n\nThe difference in mean age was more than four years. But are these two mean values good summaries of the central tendencies of participants‚Äô ages? To check, it is important that we examine the full distribution of participants‚Äô ages. We begin with the distribution of L2 participants‚Äô ages.\nWe first use the table() function to tally L2 participants‚Äô ages.\n\ntable(L2.data$Age)\n\n\n20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42 46 47 48 51 \n 1  1  5  3  2  3  2  2  6  3  4  5  2  3  2  2  3  2  5  1  1  1  2  1  1  1 \n52 55 62 \n 1  1  1 \n\n\nAs the above table contains a lot of different values, it‚Äôs easier to visualise these numbers in the form of a bar chart (also called bar plot). The mode (28) has been highlighted in black.\n\n\nShow the R code to produce the plot below (but note that data visualisation is covered in Chapter 11).\nbarplot.mode &lt;- \n  \n  # Take the L2.data data frame and pipe it into the ggplot function:\n  L2.data |&gt;\n\n  # Start a ggplot, mapping Age to the x-axis:\n  ggplot(mapping = aes(x = Age)) + \n\n  # Add a bar plot layer, conditionally fill the bars; bars representing 28 years of age will have a different colour:\n  geom_bar(aes(fill = (Age == 28))) +    \n\n  # Manually control the colours of the bar fill: set the bar representing Age == 28 to \"#0000EE\", and remove the legend:\n  scale_fill_manual(values = c(\"TRUE\" = \"black\"), guide = \"none\") +  \n  \n    # Apply ggplot2's classic theme:\n  theme_classic() +\n\n  # Ensure that there are tick marks for every single whole number and do not extend the limits of y-scale to avoid white space on the plot:\n  scale_y_continuous(name = \"Number of L1 participants\",\n                     breaks = scales::breaks_width(1), \n                     expand = c(0, 0)) +\n\n  # Set the x-axis breaks and remove white space:\n  scale_x_continuous(breaks = scales::breaks_width(1),\n                     expand = c(0, 0)) +\n  \n  # Add label for mode:\n  annotate(\"text\", \n           x = 25, \n           y = 5.8, \n           label = \"mode\", \n           colour = \"black\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 25,\n    y = 5.65, \n    xend = 27.2,\n    yend = 5, \n    curvature = 0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"black\")\n\n# Print the plot\nbarplot.mode\n\n\n\n\n\n\n\n\n\nThanks to this bar chart, it‚Äôs much easier to see that the second most frequent ages after the mode of 28 are 22, 31 and 39. How do these ages compare to the median age?\n\n\nShow R code to generate the plot below.\nbarplot.mode.median &lt;- \n  barplot.mode +\n  geom_bar(aes(fill = ifelse(Age == 31, \"31\", ifelse(Age == 28, \"28\", \"Other\")))) +\n  scale_fill_manual(values = c(\"28\" = \"black\", \"31\" = \"darkred\"), guide = \"none\") +\n  # Add label for median:\n  annotate(\"text\", \n           x = 30.5, \n           y = 5.5, \n           label = \"median\", \n           colour = \"darkred\",\n           family = \"mono\") +\n  \n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 30.4,\n    y = 5.35, \n    xend = 30.4,\n    yend = 4.9, \n    curvature = 0.6,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"darkred\")\n\nbarplot.mode.median\n\n\n\n\n\n\n\n\n\nWe can compare the mode (28) and median (31) to the mean (32.72), which, on the following bar chart, is represented as a blue dashed line.\n\n\nShow R code to generate the plot below.\nbarplot.mode.median +\n  geom_vline(aes(xintercept = mean(Age)), \n             color = \"#0000EE\", \n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  \n  # Add label for mean:\n  annotate(\"text\", \n           x = 36, \n           y = 5.3, \n           label = \"mean\", \n           colour = \"#0000EE\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 36,\n    y = 5.15, \n    xend = 33.2,\n    yend = 4.4, \n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"#0000EE\")\n\n\n\n\n\n\n\n\n\nNext, we can reduce the number of bars by adding together the number of L2 participants aged 20-22, 22-24, 24-26, etc. This is what we call a histogram. Histograms are used to visualise distributions and their bars are called bins because they ‚Äúbin together‚Äù a number of values.\n\n\nShow R code to generate the plot below.\nage.histo &lt;- \n  L2.data |&gt;\n  ggplot(mapping = aes(x = Age)) + \n    geom_vline(aes(xintercept = mean(Age)), \n             color = \"#0000EE\", \n             linetype = \"dashed\",\n             linewidth = 0.8) +\n    geom_vline(aes(xintercept = 28), \n             color = \"black\", \n             linewidth = 0.8) +\n    geom_vline(aes(xintercept = 31), \n             color = \"darkred\", \n             linewidth = 0.8) +  \n  \n  # Add label for mode:\n  annotate(\"text\", \n           x = 24, \n           y = 2.9, \n           label = \"mode\", \n           colour = \"black\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 24,\n    y = 2.6, \n    xend = 27.2,\n    yend = 2, \n    curvature = 0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"black\") +\n  \n  # Add label for mean:\n  annotate(\"text\", \n           x = 36, \n           y = 1.4, \n           label = \"mean\", \n           colour = \"#0000EE\",\n           family = \"mono\") +\n  \n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 36,\n    y = 1.1, \n    xend = 33.2,\n    yend = 0.4, \n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"#0000EE\") +\n  \n    # Add label for median:\n  annotate(\"text\", \n           x = 25, \n           y = 1.4, \n           label = \"median\", \n           colour = \"darkred\",\n           family = \"mono\") +\n  \n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 25,\n    y = 1.1, \n    xend = 30.7,\n    yend = 0.4, \n    curvature = 0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")), \n    colour = \"darkred\") +\n  \n  theme_classic() +\n  scale_y_continuous(name = \"Number of L2 participants\", \n                     breaks = scales::breaks_width(1), \n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = scales::breaks_width(2), \n                     expand = c(0, 0))\n\nage.histo +\n    geom_histogram(position = \"identity\", \n                 binwidth = 2,\n                 fill = \"black\",\n                 alpha = 0.4)\n\n\n\n\n\n\n\n\n\nIf we reduce the number of bins by having them cover three years instead of two, the histogram looks like this.\n\n\nShow R code to generate the plot below.\nage.histo +\n    geom_histogram(position = \"identity\", \n                 binwidth = 3,\n                 fill = \"black\",\n                 alpha = 0.4) +\n    scale_x_continuous(breaks = scales::breaks_width(3), \n                     expand = c(0, 0))\n\n\n\n\n\n\n\n\n\nAlternatively, we can apply a density function to smooth over the bins of the histogram to generate a density plot of L2 participants‚Äô ages (see purple curve in Figure¬†8.2). Such smoothed curves allow for a better comparison of distribution shapes across different groups and datasets. Note that, in a density plot, the values on the y-axis are no longer counts, but rather density probabilities. We will not use any fancy formulae to work this out mathematically, but you should understand that the total area under the curve (in purple) will always equal to 1, which corresponds to 100% probability. In this dataset, this is because there is a 100% probability that an L2 participant‚Äôs age is between 20 and 62.\n\n\nShow R code to generate the plot below.\n# There is no in-built function in R to calculate the mode of a numeric vector but we can define one ourselves:\nget_mode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\nL2.data |&gt;\n  ggplot(mapping = aes(x = Age)) + \n  geom_histogram(aes(x = Age, y = after_stat(density)),\n                 binwidth = 3,\n                 fill = \"black\",\n                 alpha = 0.4) + \n  geom_density(colour = \"purple\",\n               fill = \"purple\",\n               alpha = 0.2,\n               linewidth = 0.8) +\n  geom_vline(aes(xintercept = mean(Age)),\n             color = \"#0000EE\",\n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = get_mode(Age)),\n             color = \"black\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = median(Age)),\n             color = \"darkred\",\n             linewidth = 0.8) +  \n  # Add label for mode:\n  annotate(\"text\",\n           x = 25,\n           y = 0.029,\n           label = \"mode\",\n           colour = \"black\",\n           family = \"mono\") +\n\n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 25,\n    y = 0.028,\n    xend = 27.5,\n    yend = 0.025,\n    curvature = 0.6,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"black\") +\n\n  # Add label for mean:\n  annotate(\"text\",\n           x = 36,\n           y = 0.014,\n           label = \"mean\",\n           colour = \"#0000EE\",\n           family = \"mono\") +\n\n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 36,\n    y = 0.011,\n    xend = 33.2,\n    yend = 0.004,\n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"#0000EE\") +\n  \n  # Add label for median:\n  annotate(\"text\",\n           x = 25.5,\n           y = 0.01,\n           label = \"median\",\n           colour = \"darkred\",\n           family = \"mono\") +\n\n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 25.5,\n    y = 0.008,\n    xend = 30.8,\n    yend = 0.004,\n    curvature = 0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"darkred\") +  \n\n  theme_classic() +\n  scale_y_continuous(name = \"Density\",\n                     breaks = scales::breaks_width(0.01),\n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = scales::breaks_width(3),\n                     expand = c(0, 0))\n\n\n\n\n\n\n\n\nFigure¬†8.2: Density plot showing the distribution of L2 participants‚Äô ages\n\n\n\n\n\nIf we wanted to work out the probability of an L2 participant being between 42 and 62 years old, we would have to calculate the area under the curve between these two points on the x-axis. Even without doing any maths, you can see that this area is considerably smaller than between the ages of 22 and 42. This means that, in this dataset, participants are considerably more likely to be between 22 and 42 than between 42 and 62 years old.4\nThe density plot of L2 participants‚Äô ages features a characteristic bell-shaped curve, which indicates that the distribution resembles a normal distribution. However, it also features a long tail towards the older years. We are therefore dealing with a skewed distribution. Skewness is a measure of asymmetry in the a distribution. Skewed distributions occur when one tail end of the bell is longer than the other. Here, the asymmetry is due to the fact that DƒÖbrowska (2019)‚Äôs L2 data includes quite a few participants who were older than 40 at the time of the study, whereas there were none who were younger than 20. As the tail is to the right of the plot, this is a right skewed (or positive) distribution.\nThe median is usually better than the mean for describing the central tendency of a skewed distribution because it is less susceptible to the outlier(s) contained in the tail of a skewed distribution (see Section 8.1.2). Figure¬†8.2 confirms that the median is a better approximation of L2 participants‚Äô ages than the mean.\n\n\n8.2.3 Normal (or Gaussian) distributions\nIn a perfectly normally distributed variable, the mean and the median are exactly the same. They are both found at the centre of the distribution and the bell shape of the distribution is perfectly symmetrical. Hence, the skewness of a normal distribution is near zero.\nPerfectly normal distributions, however, are very rarely found in real life! Here is what the normal distribution of 10,000 participants‚Äô age might look like in real life (using numbers randomly generated from a perfectly normal distribution thanks to the R function rnorm()).\n\n\nShow R code to generate the plot below.\n# The {truncnorm} package contains density, probability, quantile and random number generation functions for the truncated normal distribution:\n#install.packages(\"truncnorm\")\nlibrary(truncnorm)\n\nset.seed(42)\nnormal.age.sd8 &lt;- round(rtruncnorm(mean = 35, sd = 8, n = 10000, a = 10, b = 100))\n#get_mode(normal.age.sd8) \n\nggplot(mapping = aes(x = normal.age.sd8)) + \n    geom_vline(aes(xintercept = mean(normal.age.sd8)),\n             color = \"#0000EE\",\n             linetype = \"dashed\",\n             linewidth = 0.8) +\n    geom_vline(aes(xintercept = median(normal.age.sd8)),\n             color = \"darkred\",\n             #linetype = \"dotted\",\n             linewidth = 0.6) +\n    # geom_vline(aes(xintercept = get_mode(normal.age.sd8)),\n    #          color = \"black\",\n    #          linewidth = 0.8) +  \n  \n  # Add label for mean:\n  annotate(\"text\",\n           x = 30.6,\n           y = 0.014,\n           label = \"mean\",\n           colour = \"#0000EE\",\n           family = \"mono\") +\n\n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 30.5,\n    y = 0.013,\n    xend = 34.7,\n    yend = 0.008,\n    curvature = 0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"#0000EE\") +  \n  \n  # Add label for median:\n  annotate(\"text\",\n           x = 38,\n           y = 0.007,\n           label = \"median\",\n           colour = \"darkred\",\n           family = \"mono\") +\n\n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 38,\n    y = 0.006,\n    xend = 35.2,\n    yend = 0.004,\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"darkred\") +\n  theme_classic() +\n  scale_y_continuous(name = \"Density\",\n                     breaks = scales::breaks_width(0.01),\n                     expand = c(0, 0)) +\n  scale_x_continuous(name = \"Age\",\n                     breaks = scales::breaks_width(2),\n                     expand = c(0, 0)) +\n  geom_histogram(aes(x = normal.age.sd8, y = after_stat(density)),\n                 binwidth = 2,\n                 fill = \"black\",\n                 alpha = 0.4) + \n  geom_density(colour = \"purple\",\n               linewidth = 0.8,\n               fill = \"purple\",\n               alpha = 0.3)\n\n\n\n\n\n\n\n\nFigure¬†8.3: A normal distribution of the age of a fictitious group of participants\n\n\n\n\n\nIn Figure¬†8.3, the mean (34.9235) and the median (35) age of our 10,000 fictitious learners are very close to each other. So close that, on the plot, the lines overlap. The skew is near zero, hence the density curve forms near-symmetrical bell shape. This means that the purple area under the curve to the left of the central tendency is pretty much the same as the area to the right of the line. In other words, there are as many people whose age is below the central tendency (50%) as there are people whose age is above the central tendency (50%). These are the characteristics of a normal distribution. Understanding these is important as many statistical tests assume that the variables entered in these tests are (approximately) normally distributed (see Chapter 12).\n\n\n8.2.4 Non-normal (or non-parametric) distributions\nWe saw that the distribution of L2 participants‚Äô ages in DƒÖbrowska (2019) was close to a normal distribution but with a right skew towards older years. This meant that the mean age was higher than median age.\nDoes the distribution of L1 participants‚Äô ages follow a similar shape? Are we dealing with a distribution that is normal, left or right skewed, or something entirely different?\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ4. What were the modal ages of the L1 and L2 participants in DƒÖbrowska (2019)?\n\n\n\n\n17 (for L1 participants) and 28 (L2 participants)\n\n\n29 (for L1 participants) and 28 (L2 participants)\n\n\n28 (for L1 participants) and 29 (L2 participants)\n\n\n32 (for L1 participants) and 31 (L2 participants)\n\n\n\n\n\n\n\nüòá Hover for a hint\n\n\n\n\n¬†\nQ5. Which of the following statements is true?\n\n\n\n\nThe mean and median age of the L1 group are almost the same.\n\n\nThe difference between mean and median age is smaller in the L1 group than in the L2 group.\n\n\nThe difference between the mean and median age is greater in the L1 group than in the L2 group.\n\n\n\n\n\n\n\n¬†\nQ6. Which measure of central tendency best describes L1 participants‚Äô ages?\n\n\n\n\nThe mean.\n\n\nThe median.\n\n\nThe mode.\n\n\nNone of them.\n\n\n\n\n\n\n\n¬†\n\n\nFigure¬†8.4 shows the distribution of L1 participants‚Äô ages both as a histogram (in grey) and a density plot (in purple). Both of these visualisations make quite clear that this distribution of ages is not at all normal! It is non-normal or non-parametric. This is because it does not consist of one more or less symmetrical bell shape. Instead, we can see several small bell shapes.\n\n\nShow R code to generate the plot below.\nL1.data |&gt;\n  ggplot(mapping = aes(x = Age)) + \n  geom_histogram(aes(x = Age, y = after_stat(density)),\n                 binwidth = 2,\n                 fill = \"black\",\n                 alpha = 0.4) + \n  geom_density(colour = \"purple\",\n               fill = \"purple\",\n               alpha = 0.2,\n               linewidth = 0.8) +\n  geom_vline(aes(xintercept = mean(Age)),\n             color = \"#0000EE\",\n             linetype = \"dashed\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = get_mode(Age)),\n             color = \"black\",\n             linewidth = 0.8) +\n  geom_vline(aes(xintercept = median(Age)),\n             color = \"darkred\",\n             linewidth = 0.8) + \n  \n  # Add label for mode:\n  annotate(\"text\",\n           x = 20.5,\n           y = 0.064,\n           label = \"mode\",\n           colour = \"black\",\n           family = \"mono\") +\n\n  # Add curved arrow for mode:\n  annotate(\n    geom = \"curve\",\n    x = 20.5,\n    y = 0.062,\n    xend = 18,\n    yend = 0.058,\n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"black\") +\n\n  # Add label for mean:\n  annotate(\"text\",\n           x = 40,\n           y = 0.03,\n           label = \"mean\",\n           colour = \"#0000EE\",\n           family = \"mono\") +\n\n  # Add curved arrow for mean:\n  annotate(\n    geom = \"curve\",\n    x = 40,\n    y = 0.028,\n    xend = 37.8,\n    yend = 0.025,\n    curvature = -0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"#0000EE\") +\n  \n  # Add label for median:\n  annotate(\"text\",\n           x = 28,\n           y = 0.01,\n           label = \"median\",\n           colour = \"darkred\",\n           family = \"mono\") +\n\n  # Add curved arrow for median:\n  annotate(\n    geom = \"curve\",\n    x = 28,\n    y = 0.008,\n    xend = 31.7,\n    yend = 0.004,\n    curvature = 0.4,\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    colour = \"darkred\") +  \n\n  theme_classic() +\n  scale_y_continuous(name = \"Density\",\n                     breaks = scales::breaks_width(0.01),\n                     expand = c(0, 0)) +\n  scale_x_continuous(breaks = scales::breaks_width(2),\n                     expand = c(0, 0))\n\n\n\n\n\n\n\n\nFigure¬†8.4: Density plot showing the distribution of L1 participants‚Äô ages\n\n\n\n\n\nThe histogram also shows that the most frequent age (the mode) corresponds to the lowest age in the dataset (17) and that both the median and mean are far removed from most participants‚Äô ages. This distribution of L1 participants‚Äô ages points to the limits of measures of central tendency. They are useful to describe approximately normally distributed variables, but are far less informative when it comes to other types of distributions.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ7. What can we reasonably deduce from the distribution of L1 participants‚Äô ages visualised in Figure¬†8.4?\n\n\n\n\nPeople in their 40s are most likely to be willing to participate in linguistic studies.\n\n\nThe researcher excluded 34 year-olds from this study.\n\n\nIt was easier to recruit teenagers to participate in this study.\n\n\nIt was easier to recruit adults in their 50s and 60s to participate in this study.\n\n\nFor this study, the researcher specifically targeted 17 and 60 year-olds.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ8. What are the pros of reporting the median rather than the mean to describe the central tendency of a variable?\n\n\n\n\nThe median is the most widely used measure of central tendency.\n\n\nAs the middle value, the median is fairly easy to interpret.\n\n\nThe median is less susceptible to outliers.\n\n\nThe median is less susceptible to skew.\n\n\nThe median has the highest probability of being the true central value.\n\n\nThe median also works well with nominal variables.\n\n\nThe median is ideal for very small datasets.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ9. What are the cons of reporting the median rather than the mean to describe the central tendency of a variable?\n\n\n\n\nThe median is more susceptible to outliers.\n\n\nThe median is often less meaningful with small sample sizes.\n\n\nThe median does not take all values into account.\n\n\nThe median can be misleading if it is not a value in the dataset.\n\n\nThe median is more susceptible to skew.\n\n\nThe median only works with odd numbers of values.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#sec-Variability",
    "href": "8_DescriptiveStats.html#sec-Variability",
    "title": "8¬† DescRiptive statistics",
    "section": "8.3 Measures of variability",
    "text": "8.3 Measures of variability\nMeasures of central tendency should never be reported alone. As we saw with L1 participants‚Äô age in Section 8.2.4, measures of central tendency are not always very informative and can even be misleading. But, even when they are informative, they only tell us part of the story: the average value of a dataset, but not the spread or variability of the data. For example, a mean age of 25 might suggest a group of young adults, but without a measure of variability, we can‚Äôt tell if the group is relatively homogeneous (e.g., all students in their twenties) or heterogeneous (e.g., with some participants in their teens and others in their thirties or older). Therefore, it is essential to report measures of central tendency in conjunction with measures of variability, such as the range, interquartile range, or standard deviation, to provide a more complete picture of the data.\nConsider the three distributions of ages presented in Figure¬†8.5. As you can tell from their shapes, these are three normal distributions. They each have exactly the same mean and median of 35; yet they evidently correspond to very different groups of people!\n\n\nShow R code to generate the plot below.\n# The {truncnorm} package contains density, probability, quantile and random number generation functions for the truncated normal distribution. You will need to install it before you can load it.\n\n#install.packages(\"truncnorm\")\nlibrary(truncnorm)\n\nset.seed(42)\nnormal.age.A &lt;- rtruncnorm(mean = 35, sd = 1, n = 10000, a = 10, b = 64)\nnormal.age.B &lt;- rtruncnorm(mean = 35, sd = 5, n = 10000, a = 10, b = 64)\nnormal.age.C &lt;- rtruncnorm(mean = 35, sd = 10, n = 10000, a = 10, b = 64)\n\nnormal.density &lt;- function(ages) {\n  ggplot(mapping = aes(x = ages)) + \n      geom_vline(aes(xintercept = mean(ages)),\n               color = \"#0000EE\",\n               linetype = \"dashed\",\n               linewidth = 0.8) +\n    theme_classic() +\n    scale_y_continuous(name = \"Density\",\n                       #breaks = scales::breaks_width(0.01),\n                       expand = c(0, 0)) +\n    scale_x_continuous(name = \"Age\",\n                       breaks = scales::breaks_width(2),\n                       limits = c(10,66),\n                       expand = c(0, 0)) +\n    geom_histogram(aes(x = ages, y = after_stat(density)),\n                   binwidth = 1,\n                   fill = \"black\",\n                   alpha = 0.4) + \n    geom_density(colour = \"purple\",\n                 linewidth = 0.8,\n                 fill = \"purple\",\n                 alpha = 0.3)\n}\n\n# The three plots are printed as one figure using the {patchwork} library. You will need to install this library before you can load it and use it.\n\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nnormal.density(normal.age.A) / normal.density(normal.age.B) / normal.density(normal.age.C) +\n    # Add captions A, B, C\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\nFigure¬†8.5: Three normal distributions of ages with a mean/median of 35 years.\n\n\n\n\n\nWhereas Group A only includes adults aged 32 to 39, the Group C includes children as young as 10 as well as adults well into their 50s and early 60s - even though they both have the same mean/median on 35. This is why both measures of central tendency and variability are important when describing numeric variables! Measures of variability help us to understand how far each data point is from the central tendency. Hence, for Group A in Figure¬†8.5, we can say that all data points are pretty close to the mean/median of 35. In Group B, participants‚Äô ages are, on average, more ‚Äòspread out‚Äô to the left and right of the central tendency. And this is even more notable in Group C.\n\n8.3.1 Range\nThe most basic measure of variability is one that you will already be familiar with: range. It is easily calculated by subtracting the highest value of a variable from its lowest value. For example, in DƒÖbrowska (2019), the range of results obtained by the L1 participants in the English grammar comprehension test is:\n\nmax(L1.data$GrammarR) - min(L1.data$GrammarR)\n\n[1] 22\n\n\nBy contrast, the range of results in this same test among the L2 participants is:\n\nmax(L2.data$GrammarR) - min(L2.data$GrammarR)\n\n[1] 40\n\n\nIn practice, the range is usually reported by explicitly mentioning a variable‚Äôs lowest and highest values as this is usually much more informative than the range itself. Here is how DƒÖbrowska (2019) reports the age of the participants in the published article:\n\nThe L1 participants were all born and raised in the United Kingdom and were selected to ensure a range of ages, occupations, and educational backgrounds. The age range was from 17 to 65 years [‚Ä¶]. The nonnative participants ranged in age from 20 to 62 years [‚Ä¶] (DƒÖbrowska 2019: 6).\n\n\n\n\n\n\n\nTask 1\n\n\n\nComplete the description of the GrammarR variable in L1.data and L2.data below.\nCopy and paste the following paragraph into a text processor (e.g., LibreOffice Writer or Microsoft Word) and fill in the six blanks using figures that you calculated in R. If necessary, round off values to two decimal places.\n\nOn average, English native speakers performed only marginally better in the English grammatical comprehension test (median = ______) than English L2 learners (median = ______). However, L1 participants‚Äô grammatical comprehension test results ranged from ______to ______, whereas L2 participants‚Äô results ranged from ______to ______. ¬†\n\n\n\n\n\n\n\n\n\nClick here for the solution to Task 1\n\n\n\n\n\nYour paragraph should read as follows:\n\nOn average, English native speakers performed only marginally better in the English grammatical comprehension test (median = 76) than English L2 learners (median = 75). L1 participants‚Äô grammatical comprehension test results ranged from 58 to 80. In this same test, L2 participants‚Äô results ranged 40 to 80.\n\nThe following lines of R code can be used to obtain these numbers.\n\nmedian(L1.data$GrammarR)\n\n[1] 76\n\nmedian(L2.data$GrammarR)\n\n[1] 75\n\nmin(L1.data$GrammarR)\n\n[1] 58\n\nmax(L1.data$GrammarR)\n\n[1] 80\n\nmin(L2.data$GrammarR)\n\n[1] 40\n\nmax(L2.data$GrammarR)\n\n[1] 80\n\n\n\n\n\n\n\n8.3.2 Interquartile range\nWe saw that the median is a measure of central tendency that represents the middle value. This means that 50% of the data falls below the median and 50% falls above the median. Going back to the test results of our six learners of Breton in Fiji, this means that half of the class scored below 86.5 and the other half above 86.5.\n\nmedian(c(5, 82, 86, 87, 89, 91))\n\n[1] 86.5\n\n\nWe can further subdivide the distribution into chunks of 25% of the data, or quartiles (see Figure¬†8.6).\n\nThe first quartile (Q1) is the value below which 25% of the data falls. In other words, the first quartile corresponds to a value that lies above one-quarter of the values in the data set.\nThe second quartile (Q2) is the median and, as we know, half of the data (25% + 25% = 50%) are below this value, the other half are above.\nThe third quartile (Q3) is the value below which 75% of the data falls. In other words, it is also the value above which the upper 25% of the data are.\nThe interquartile range (IQR) is the range between the second and the third quartile: it therefore covers the middle 50% of the data. This is illustrated below with a growing number of imaginary Breton learners.\n\n\n\n\n\n\n\nFigure¬†8.6: Animation showing the interquartile range of five different sets of values (CC-BY Elen Le Foll)\n\n\n\nThe easiest way to examine a variable‚Äôs IQR in R is to use the handy summary() function which, when applied to a numeric variable, returns a number of useful descriptive statistics including its first and third quartiles.5\n\nsummary(L1.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  58.00   71.25   76.00   74.42   79.00   80.00 \n\n\nFrom the output of the summary() function, we can easily calculate the IQR, which we know is equal to the range between the first and the third quantile.\n\n79 - 71.25\n\n[1] 7.75\n\n\nAlternatively, we can compute the IQR directly using the IQR() function.\n\nIQR(L1.data$GrammarR)\n\n[1] 7.75\n\n\nThe reason that the summary() function is probably more useful than IQR() is that, like the full range, the interquantile range is not usually reported as the difference between Q3 and Q1. This is because it is more informative to consider the first quartile (Q1), the median (Q2), and the third quartile (Q3) together to grasp both the central tendency of a set of numbers and the amount of variability there is around this central tendency.\nIn practice, quartiles are rarely reported as numbers. Instead, they are usually visualised as boxplots. Boxplots present a visual summary of a numeric variable‚Äôs central tendency and variability around this central tendency. On a boxplot, the box represents the IQR. Its dividing line is the median. The whiskers and any outlier points represent the rest of the distribution (see Figure¬†8.7). In other words, the lower whisker roughly covers the lower 25% of the data and the upper whisper the top 25% of the data. Boxplots are most often displayed vertically and are used to visually compare the main characteristics of distributions of numeric values across different groups (e.g., grammar comprehension across different language proficiency groups).\n\n\n\n\n\n\nFigure¬†8.7: Animation showing the making of a boxplot (CC-BY Elen Le Foll)\n\n\n\nRemember that, in a perfectly normal distribution, the mean and median are equal. When a variable follows a normal distribution, its box is divided into two equal halves and the whiskers are of equal length (see Figure¬†8.8). This symmetry comes from the fact that the values are equally distributed to the left and right of the median/mean. For the same reason, the bells of the normal distributions in Figure¬†8.5 were all (almost) symmetrical, although they had very different heights and widths.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ10. Examine the boxplots displayed in Figure¬†8.8.\n\n\n\n\n\n\n\n\nFigure¬†8.8: Three boxplots\n\n\n\n\n\nWhich of the following statements accurately describe the age distributions displayed in Figure¬†8.8?\n\n\n\n\nAges in Group 1 are normally distributed.\n\n\nAges in Group 1 are not normally distributed.\n\n\nAges in Group 2 are normally distributed.\n\n\nAges in Group 2 are not normally distributed.\n\n\nAges in Group 3 are normally distributed.\n\n\nIn Group 3, all participants were born within 12 months of each other.\n\n\nOn average, participants were older in Group 1 and younger in Group 3.\n\n\nIn all three groups, the median age is approximately the same.\n\n\nIn all three groups, the mean age is approximately the same.\n\n\nIn all three groups, the IQR is approximately the same.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a first hint.\n\n\n\n\nü¶â Hover over the owl for a second hint.\n\n\n\n\n¬†\nQ11. The boxplots in Figure¬†8.8 are based on the same data as the three density plots in Figure¬†8.5. Compare the two figures. Which distribution corresponds to which boxplot?\n\n\n\n\nDistribution C is visualised in boxplot 1.\n\n\nDistribution C is visualised in boxplot 3.\n\n\nDistribution A is visualised in boxplot 3.\n\n\nDistributions A, B and C are visualised in boxplot 2.\n\n\nDistribution B is visualised in boxplot 1.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ12. Examine the following distribution of scores on the grammatical comprehension test administered as part of DƒÖbrowska (2019).\n\n\n\n\n\n\n\n\nFigure¬†8.9: Density plot of participants‚Äô scores on the English comprehension grammar test\n\n\n\n\n\n¬†\nAre the scores visualised in Figure¬†8.9 normally distributed?\n\n\n\n\nNo, they are far from normally distributed.\n\n\nYes, they are approximately normally distributed, but with a slight positive skew.\n\n\nYes, they are approximately normally distributed.\n\n\nIt's impossible to tell from the plot alone.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ13. Compare the following outputs of the summary() function.\n\nsummary(L1.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  58.00   71.25   76.00   74.42   79.00   80.00 \n\nsummary(L2.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40.00   58.50   75.00   67.76   78.00   80.00 \n\n\nBased on the outputs of the summary() function, what does Figure¬†8.9 display?\n\n\n\n\nThe distribution of GrammarR scores among L1 participants.\n\n\nThe distribution of GrammarR scores among L2 participants.\n\n\nThe distribution of GrammarR scores among both L1 and L2 participants.\n\n\nNone of the above.\n\n\n\n\n\n\n\n¬†\nQ14. Compare the following boxplots which summarise the distribution of scores on the grammatical comprehension test (GrammarR) administered as part of DƒÖbrowska (2019).\n\n\n\n\n\n\n\n\nFigure¬†8.10: Boxplots showing L1 and L2 participants‚Äô English grammar comprehension test scores\n\n\n\n\n\nWhy do the two boxplots look so different?\n\n\n\n\nBecause the IQR of scores was much larger among L2 participants than among L1 participants.\n\n\nBecause the two groups were not of equal size (there were more L1 than L2 participants).\n\n\nBecause more than a quarter of L2 participants scored below 60, whereas only one L1 participant scored below 60.\n\n\nBecause the range of scores was much larger among L2 participants than among L1 participants.\n\n\nBecause the median L2 score is much lower than the median L1 score.\n\n\nBecause proportionally more L2 participants scored below the L2 median score than L1 participants did below the L1 median.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nü¶â Hover over the owl for a second hint.\n\n\n\n\n\n\n\n\n8.3.3 Standard deviation\nIn the language sciences and in many other disciplines, standard deviation is the most common reported measure of variability. Whereas the interquartile range (IQR) is a measure of variability around the median, standard deviation (SD) measures variability around the mean. In other words, if you report a mean value as a measure of central tendency, you should report the standard deviation along side it. However, if you report the median, than it makes more sense to report the IQR in the form of a boxplot (see Section 8.3.2).\n\nIn a nutshell, the standard deviation tells us how far away, on average, each data point is from the mean.\n\n¬†Considering the test scores of our five Breton learners, we already know that the standard deviation is likely to be large because the mean (70.6) is quite far away from all five data points.\n\n5, 82, 86, 89, 91\n\nTo calculate how far exactly, we first measure how far each point is from the mean, e.g.¬†for the first data point we calculate 5 - 70.6, for the second 82 - 70.6, etc.\n\nBreton.scores &lt;- c(5, 82, 86, 89, 91)\n\nBreton.scores - mean(Breton.scores)\n\n[1] -65.6  11.4  15.4  18.4  20.4\n\n\nAs you can see, some of the differences between the data points and the mean value are negative, whilst others are positive. For standard deviation, we are not interested in whether data points are above or below the mean, but rather in how far removed they are from the mean. To remove any negative sign, we therefore square all these distances. The squaring operation (^2) also has the effect making large differences even larger.\n\n(Breton.scores - mean(Breton.scores))^2\n\n[1] 4303.36  129.96  237.16  338.56  416.16\n\n\nRemember that standard deviation is a measure of how different, on average, a set of numbers are from one another, with respect to the mean. We have just calculated the sum of the squared differences from the mean and we now need to calculate the average of these squared differences. To calculate the mean squared difference, we sum the differences and divide them by the number of data points.\n\nsum((Breton.scores - mean(Breton.scores))^2) / 5\n\n[1] 1085.04\n\n\nThis is the variance. The problem with the variance is that it is not in the original scale of our variable, but rather in squared units, i.e., here, in squared test scores, which is rather difficult to interpret! This is why we more commonly report the standard deviation, which is the square root of the variance. The square root function in R is sqrt().\n\nsqrt(sum((Breton.scores - mean(Breton.scores))^2) / 5)\n\n[1] 32.93995\n\n\nFrom the above result, we can deduce that, on average, learners‚Äô test scores are 32 points away from the group mean of 70.6 points.\nOf course, there is a base R function to calculate the standard deviation. It is called sd(). However, if we use the sd() function to calculate the standard deviation of our five Breton learners‚Äô test scores, we get a slightly different result.\n\nsd(Breton.scores)\n\n[1] 36.82798\n\n\nThis is because, in practice, we almost always divide the sum of squares not by the total number of data points (N ), but by the total number minus one (N-1). This is the difference between the population standard deviation and the sample standard deviation. The population standard deviation is used when we have access to the entire population (e.g.¬†all L2 English users worldwide!), which is rare in real-world scenarios. In most cases, we work with samples (e.g., as in DƒÖbrowska 2019, a sample of 67 L2 English users). Dividing by N-1 gives us a more accurate estimate of the population‚Äôs standard deviation based on our sample. It helps to reduce the bias in our estimate, making it a more reliable measure of variability around the mean.\nIn R, the sd() function calculates the sample standard deviation.\n\nsqrt(sum((Breton.scores - mean(Breton.scores))^2) / 4)\n\n[1] 36.82798\n\n\nWith a normal distribution, the standard deviation informs us about the width of the bell around the central tendency. In Figure¬†8.5 we saw that three normal distributions, all with a median/mean of 35 could have very different bell shapes. This is because they have very different standard deviations around that central tendency. Let us compare the distribution shapes of these three distributions in detail.\nDistribution A (Figure¬†8.11) is a normal distribution with a mean of 35 years (xÃÖ = 35) and a standard deviation of one year (sd = 1).\n\n\n\n\n\n\n\n\nFigure¬†8.11: Density plot of Distribution A\n\n\n\n\n\nDistribution B (Figure¬†8.12) is a normal distribution with a mean of 35 years (xÃÖ = 35) and a standard deviation of 5 years (sd = 5).\n\n\n\n\n\n\n\n\nFigure¬†8.12: Density plot of Distribution B\n\n\n\n\n\nDistribution C (Figure¬†8.13) is a normal distribution with a mean of 35 years (xÃÖ = 35) and a standard deviation of 10 years (sd = 10).\n\n\n\n\n\n\n\n\nFigure¬†8.13: Density plot of Distribution C\n\n\n\n\n\nThe standard deviation provides a single metric of the variability around the mean. This means that knowing the mean and standard deviation of a numeric variable is not enough to tell whether a distribution is (approximately) normal or skewed. Like the range and the IQR, a large standard deviation value indicates greater variability within a variable, but tells us nothing more. For instance, comparing the following two SDs tells us that there is more variability around the mean in L2 participants‚Äô grammar comprehension test scores than in that of the L1 participants, but nothing more about the distribution of the test scores in either group.\n\nsd(L1.data$GrammarR) |&gt;\n  round(digits = 2)\n\n[1] 5.01\n\nsd(L2.data$GrammarR) |&gt; \n  round(digits = 2)\n\n[1] 13.48\n\n\nIn this respect, boxplots are more informative (compare the above SDs with Figure¬†8.10). To evaluate the full shape of a numeric variable‚Äôs distribution, however, there is no alternative to plotting it as a histogram or density plot.\nIn sum, remember that, when describing variables, it is important to report both an appropriate measure of central tendency and an appropriate measure of variability. In addition, it is good practice to visualise the full distribution of a variable‚Äôs values in the form of a table, histogram, or density plot (see Chapter 11 on data visualisation). This is because any combination of a single measure of central tendency and a single measure of variability can correspond to an array of different distribution shapes.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ15. What is the standard deviation of L1 participants‚Äô age in DƒÖbrowska (2019)? Calculate the sample standard deviation to two decimal places.\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ16. Compare the standard deviation of the Age variable in the L1 and L2 datasets. What can you conclude on the basis of this comparison?\n\n\n\n\nAge is not normally distributed in both the L1 and the L2 data.\n\n\nThere is greater variability around the mean age in the L1 data than in the L2 data.\n\n\nThere is greater variability around the mean age in the L2 data than in the L1 data.\n\n\nL2 participants are more likely to be older than L1 participants.\n\n\nThere is almost twice as much variability in L2 participants' ages than in L1's.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\nFurther reading\n\n\n\nAs a follow-up, I highly recommend reading this short and highly accessible article by Fahd Alhazmi (2020), who provides a wonderful visual guide to understanding standard deviation: https://towardsdatascience.com/a-visual-interpretation-of-the-standard-deviation-30f4676c291c.\n\n\n\n\nCheck your progress üåü\nYou have successfully completed 0 out of 13 quiz questions in this chapter.\nAre you confident that you can‚Ä¶?\n\nUse and interpret different measures of central tendency (Section 8.1)\nCalculate the mode, mean, median of a numeric variable in R (Section 8.1.1 - Section 8.1.2)\nInterpret histograms and density plots (Section 8.2)\nRecognise the characteristics of a normal distribution (Section 8.2.3)\nInterpret and calculate the interquartile range in R (Section 8.3.2)\nInterpret boxplots (Section 8.3.2)\nInterpret and calculate the standard deviation (Section 8.3.3)\n\nIn Chapter 10, we will cover the basics of data visualisation and learn how to create a range of informative and elegant plots (including histograms and density plots) using the popular R package ggplot2. But, first, we need to learn about data wrangling (Chapter 9) to prepare our data for data visualisation and multivariable analyses. Are you ready? ü§ì\n\n\n\n\nAlhazmi, Fahd. 2020. A visual interpretation of the standard deviation. Medium. https://towardsdatascience.com/a-visual-interpretation-of-the-standard-deviation-30f4676c291c.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "8_DescriptiveStats.html#footnotes",
    "href": "8_DescriptiveStats.html#footnotes",
    "title": "8¬† DescRiptive statistics",
    "section": "",
    "text": "Breton is the Celtic language of Brittany (now in North-West France). With around 216,000 active speakers (Wikipedia, 26/08/2024), Breton is classified as ‚Äòseverely endangered‚Äô in the UNESCO‚Äôs Atlas of the World‚Äôs Languages in Danger. It would presumably be quite a feat to put together a class of five Breton learners in Fiji, an island country far removed from Brittany in the South Pacific Ocean with fewer than one million inhabitants (Wikipedia, 26/08/2024)!‚Ü©Ô∏é\nWe also see that this data needs cleaning before we can do any serious data analysis. There are also a few typos (e.g., Unemploed) and synonyms (School Crossing Guard and School Crossing Patrol) that we will need to standardise. This process is part of data wrangling and we will cover how to do this in a reproducible way in R in the following chapter.‚Ü©Ô∏é\nA Likert scale is a type of rating scale used to measure attitudes, opinions, or feelings. It typically consists of a series of statements or questions with a range of possible responses, often on a scale from ‚Äústrongly disagree‚Äù to ‚Äústrongly agree‚Äù. For example, in a study on language attitudes, participants might be asked to rate their agreement with the statement ‚ÄúI think it‚Äôs important to speak standard English in formal situations‚Äù on a scale from ‚Äú1 (strongly disagree)‚Äù to ‚Äú5 (strongly agree)‚Äù. The resulting variable will therefore consist of numbers ranging between 1 and 5. Note also that, strictly speaking, Likert scales are not numeric variables, but rather ordinal variables (see Section 7.2.1). The numbers refer to different categories that describe an order of responses, rather than a quantity.‚Ü©Ô∏é\nOf course, there is an R function to help you do the maths! The edcf() function allows us to calculate the area under the curve between the ages of 42 and 62.\n\necdf(L2.data$Age)(62) - ecdf(L2.data$Age)(42)\n\n[1] 0.119403\n\n\nIn other words, there is a 11.94 % probability of any L2 participant in this study being aged between 42 and 62 (corresponding to the light purple area in plot A). Compare this to the probability of a participant being between 22 and 42 years old.\n\necdf(L2.data$Age)(42) - ecdf(L2.data$Age)(22)\n\n[1] 0.7761194\n\n\nThis is, indeed, a much higher probability (ca. 78 %), as depicted by the much larger area highlighted in plot B.\n\n\n\n\n\n\n\n\n\n‚Ü©Ô∏é\nQuartiles can also be computed using the quantile() function, which takes two arguments: the variable and a value between 0 and 1 corresponding to our quantile of interest. We are interested in the first and third quartiles, therefore in the values below which lie one quarter (0.25) and three-quarters (0.75) of all the data.\nTo compute the first quantile (Q1), we therefore enter:\n\nquantile(L1.data$GrammarR, 0.25)\n\n  25% \n71.25 \n\n\nFor the third quantile (Q3), we need:\n\nquantile(L1.data$GrammarR, 0.75)\n\n75% \n 79 \n\n\n‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Desc`R`iptive statistics</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html",
    "href": "9_DataWrangling.html",
    "title": "9¬† Data wRangling",
    "section": "",
    "text": "Chapter overview\nIn this chapter, you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-tidyverse",
    "href": "9_DataWrangling.html#sec-tidyverse",
    "title": "9¬† Data wRangling",
    "section": "9.1 Welcome to the tidyverse! ü™ê",
    "text": "9.1 Welcome to the tidyverse! ü™ê\nThis chapter explains how to examine, clean, and manipulate data mostly using functions from the {tidyverse}: a collection of useful R packages increasingly used for all kinds of data analysis projects. Tidyverse functions are designed to work with tidy data (see Figure¬†9.1) and, as a result, they are often easier to combine.\n\n\n\n\n\n\nFigure¬†9.1: Tidy data illustration from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Horst & Lowndes (2020).\n\n\n\nLearning to manipulate data and conduct data analysis in R ‚Äúthe tidyverse-way‚Äù can help make your workflows more efficient.\n\nIf you ensure that your data is tidy, you‚Äôll spend less time fighting with the tools and more time working on your analysis. (Wickham, Vaughan & Girlich)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-Conflicts",
    "href": "9_DataWrangling.html#sec-Conflicts",
    "title": "9¬† Data wRangling",
    "section": "9.2 Base R vs.¬†tidyverse functions",
    "text": "9.2 Base R vs.¬†tidyverse functions\nNovice R users may find it confusing that many operations can be performed using either a base R function or a tidyverse one. For example, in Chapter 6, we saw that both the base R function read.csv() and the tidyverse function read_csv() can be used to import CSV files. The functions have slightly different arguments and default values, which can be annoying, even though they are fundamentally designed to perform the same task. But don‚Äôt fret over this too much: it‚Äôs fine for you to use whichever function you find most convenient and intuitive and it‚Äôs also absolutely fine to combine base R and tidyverse functions!\nYou will no doubt have noticed that the functions read.csv() and read_csv() have very similar but not exactly identical names. This is helpful to differentiate between the two functions. Unfortunately, some function names are found in several packages, which can lead to confusion and errors! For example, you may have noticed that when you load the tidyverse library the first time in a project, a message similar to Figure¬†9.2 is printed in the Console.\n\n\n\n\n\n\nFigure¬†9.2: Screenshot of the R Console after having loaded the {tidyverse} library\n\n\n\nFirst, the error message reproduced in Figure¬†9.2 confirms that loading the {tidyverse} package has led to the successful loading of a total of nine packages and that these are now ready to use. Crucially, the message also warns us about conflicts between some {tidyverse} packages and base R packages. These conflicts are due to the fact that two functions from the {dplyr} package have exactly the same name as functions from the base R {stats} package. The warning informs us that, by default, the {dplyr} functions will be applied.\nTo force R to use a function from a specific package, we can use the package::function() syntax. Hence, to force R to use the base R {stats} filter() function rather than the tidyverse one, we would use stats::filter(). On the contrary, if we want to be absolutely certain that the tidyverse one is used, we can use dplyr::filter().\n\n\n\n\n\n\nFigure¬†9.3: A galaxy of tidyverse-related hex stickers (artwork by @allison_horst).\n\n\n\nIn this chapter, we will explore functions from {dplyr}, {stringr}, and {tidyr}. The popular {ggplot2} tidyverse library for data visualisation following the ‚ÄúGrammar of Graphics‚Äù approach will be introduced in Chapter 10. Make sure that you have loaded the tidyverse packages before proceeding with the rest of this chapter.\n\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#checking-data-sanity",
    "href": "9_DataWrangling.html#checking-data-sanity",
    "title": "9¬† Data wRangling",
    "section": "9.3 Checking data sanity",
    "text": "9.3 Checking data sanity\nBefore beginning any data analysis, it is important to always check the sanity of our data. In the following, we will use tables and descriptive statistics to do this. In Chapter 10, we will learn how to use data visualisation to check for outliers and other issues that may affect our analyses.\n\n\n\n\n\n\nPrerequisites\n\n\n\nIn this chapter and the following chapters, all examples, tasks, and quiz questions are based on data from:\n\nDƒÖbrowska, Ewa. 2019. Experience, Aptitude, and Individual Differences in Linguistic Attainment: A Comparison of Native and Nonnative Speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\nYou will only be able to reproduce the analyses and answer the quiz questions from this chapter if you have successfully imported the two datasets from DƒÖbrowska (2019). To import the datasets, follow the instructions from Section 6.3 to Section 6.5 and complete Task 1.\n\n\n\n9.3.1 Numeric variables\nIn Section 8.3.2, we used the summary() function to obtain some useful descriptive statistics on a single numeric variable, namely the range, mean, median, and interquartile range (IQR).\n\nsummary(L1.data$GrammarR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  58.00   71.25   76.00   74.42   79.00   80.00 \n\n\nTo check the sanity of a dataset, we can use this same function on an entire data table (provided that the data is in the tidy format, see Section 9.1). Thus, the command summary(L1.data)1 outputs summary statistics on all the variables of the L1 dataset - in other words, on all the columns of the data frame L1.data.\n\nsummary(L1.data)\n\n\n\n Participant             Age           Gender           Occupation       \n Length:90          Min.   :17.00   Length:90          Length:90         \n Class :character   1st Qu.:25.00   Class :character   Class :character  \n Mode  :character   Median :32.00   Mode  :character   Mode  :character  \n                    Mean   :37.54                                        \n                    3rd Qu.:55.00                                        \n                    Max.   :65.00                                        \n  OccupGroup          OtherLgs          Education             EduYrs     \n Length:90          Length:90          Length:90          Min.   :10.00  \n Class :character   Class :character   Class :character   1st Qu.:12.00  \n Mode  :character   Mode  :character   Mode  :character   Median :13.00  \n                                                          Mean   :13.71  \n                                                          3rd Qu.:14.00  \n                                                          Max.   :21.00  \n    ReadEng1        ReadEng2        ReadEng3        ReadEng      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 5.000  \n Median :2.000   Median :2.000   Median :2.000   Median : 7.000  \n Mean   :2.522   Mean   :2.433   Mean   :2.233   Mean   : 7.189  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 9.750  \n Max.   :5.000   Max.   :5.000   Max.   :4.000   Max.   :14.000  \n\n\nFor the numeric variables in the dataset, the summary() function provides us with many useful descriptive statistics to check the sanity of the data. For example, we can check whether the minimum values include improbably low values (e.g., a five-year-old participant in a written language exam) or outright impossible ones (e.g., a minus 18-year old participant!). Equally, if we know that the maximum number of points that could be obtained in the English grammar test is 100, a maximum value of more than 100 would be highly suspicious and warrant further investigation.\nAs far as we can see from the output of summary(L1.data) above, the numeric variables in DƒÖbrowska (2019)‚Äôs L1 dataset do not appear to feature any obvious problematic values.\n\n\n9.3.2 Categorical variables as factors\nHaving examined the numeric variables, we now turn to the non-numeric, categorical ones (see Section 7.2.1). For these variables, the descriptive statistics returned by summary(L1.data) are not as insightful. They only tell us that they each include 90 values, which corresponds to the 90 participants in the L1 dataset. As we can see from the output of the str() function, these categorical variables are stored in R as character string vectors (abbreviated in the str() output to ‚Äúchr‚Äù).\n\nstr(L1.data$Gender)\n\n chr [1:90] \"M\" \"M\" \"M\" \"F\" \"F\" \"F\" \"F\" \"M\" \"M\" \"F\" \"F\" \"M\" \"M\" \"F\" \"M\" \"F\" ...\n\n\nCharacter string vectors are a useful R object type for text but, in R, categorical variables are best stored as factors. Factors are a more efficient way to store character values because each unique character value is stored only once. The data itself is stored as a vector of integers. Let‚Äôs look at an example.\nFirst, we convert the categorical variable Gender from L1.data that is currently stored as a character string vector to a factor vector called L1.Gender.fct.\n\nL1.Gender.fct &lt;- factor(L1.data$Gender)\n\nWhen we now inspect its structure using str(), we can see that L1.Gender.fct is a factor with two levels ‚ÄúF‚Äù and ‚ÄúM‚Äù. The values themselves, however, are no longer listed as ‚ÄúM‚Äù ‚ÄúM‚Äù ‚ÄúM‚Äù ‚ÄúF‚Äù ‚ÄúF‚Äù‚Ä¶, but rather as integers: 2 2 2 1 1 1‚Ä¶.\n\nstr(L1.Gender.fct)\n\n Factor w/ 2 levels \"F\",\"M\": 2 2 2 1 1 1 1 2 2 1 ...\n\n\nBy default, the levels of a factor are ordered alphabetically, hence in L1.Gender.fct, 1 corresponds to ‚ÄúF‚Äù and 2 to ‚ÄúM‚Äù.\nThe summary output of factor vectors are far more insightful than of character variables (and look rather like the output of the table() function that we used in Section 8.1.3).\n\nsummary(L1.Gender.fct)\n\n F  M \n48 42 \n\n\nThe tidyverse package {forcats} has a lot of very useful functions to manipulate factors. They all start with fct_.\n\n\n\n\n\n\n\n\nFigure¬†9.4: Hex sticker of the {forcats} package\n\n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ1. Type ?fct_ in an R script or directly in the Console and then press the tab key (‚Üπ or ‚á• on your keyboard). A list of all loaded functions that start with fct_ should pop up. Which of these is not listed?\n\n\n\n\nfct_count\n\n\nfct_mutate\n\n\nfct_na_level_to_value\n\n\nfct_rev\n\n\nfct_reorder\n\n\n\n\n\n\n\n¬†\nQ2. In the factor object L1.Gender.fct (which we created above), the first level is ‚ÄúF‚Äù because it comes first in the alphabet. Which of these commands will make ‚ÄúM‚Äù the first level instead? Check out the help files of the following {forcats} functions to understand what they do and try them out.\n\n\n\n\nfct_rev(L1.Gender.fct)\n\n\nfct_recode(L1.Gender.fct, first = \"M\", second = \"F\")\n\n\nfct_reorder(L1.Gender.fct, c(\"M\", \"F\"))\n\n\nfct_relevel(L1.Gender.fct, \"M\")\n\n\nfct_match(L1.Gender.fct, c(\"M\", \"F\"))\n\n\n\n\n\n\n\nüòá Hover for a hint",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#pre-processing-data",
    "href": "9_DataWrangling.html#pre-processing-data",
    "title": "9¬† Data wRangling",
    "section": "9.4 Pre-processing data",
    "text": "9.4 Pre-processing data\n\n9.4.1 Using mutate() to add and replace columns\nIn the previous section, we stored the factor representing L1 participants‚Äô gender as a separate R object called L1.Gender.fct. If, instead, we want to add this factor as an additional column to our dataset, we can use the mutate() function from {dplyr}.\n\n\n\n\n\nHex sticker of the {dplyr} package\n\n\n\nL1.data &lt;- L1.data |&gt; \n  mutate(Gender.fct = factor(L1.data$Gender))\n\nThe mutate() function allows us to add new columns to a dataset. By default, it also keeps all the existing ones (to control which columns are retained, check the help file and read about the ‚Äú.keep =‚Äù argument).\n\n\n\n\n\n\nFigure¬†9.5: Artwork explaining the dplyr::mutate() function by @allison_horst.\n\n\n\nWe can use the colnames() function to check that the new column has been correctly appended to the table. Alternatively, you can use the View() function to display the table in full in a new RStudio tab. In both cases, you should see that the new column is now the last column in the table (column number 32).\n\ncolnames(L1.data)\n\n [1] \"Participant\" \"Age\"         \"Gender\"      \"Occupation\"  \"OccupGroup\" \n [6] \"OtherLgs\"    \"Education\"   \"EduYrs\"      \"ReadEng1\"    \"ReadEng2\"   \n[11] \"ReadEng3\"    \"ReadEng\"     \"Active\"      \"ObjCl\"       \"ObjRel\"     \n[16] \"Passive\"     \"Postmod\"     \"Q.has\"       \"Q.is\"        \"Locative\"   \n[21] \"SubCl\"       \"SubRel\"      \"GrammarR\"    \"Grammar\"     \"VocabR\"     \n[26] \"Vocab\"       \"CollocR\"     \"Colloc\"      \"Blocks\"      \"ART\"        \n[31] \"LgAnalysis\"  \"Gender.fct\" \n\n\nWatch out: if you add a new column to a table using an existing column name, mutate() will overwrite the entire content of the existing column with the new values! In the following code chunk, we are therefore overwriting the character vector Gender with a factor vector also called Gender. We should only do this if we are certain that we won‚Äôt need to compare the original values with the new ones!\n\nL1.data &lt;- L1.data |&gt; \n  mutate(Gender = factor(L1.data$Gender))\n\n\n\n9.4.2 Using across() to transform multiple columns\nIn addition to Gender, there are quite a few more character vectors in L1.data that represent categorical variables and that would therefore be better stored as factors. We could use mutate() and factor() to convert them one by one like we did for Gender above, but that would require several lines of code in which we could easily make a silly error or two. Instead, we can use a series of neat tidyverse functions to convert all character vectors to factor vectors in one go.\n\nL1.data.fct &lt;- L1.data |&gt; \n  mutate(across(where(is.character), factor))\n\nAbove, we use mutate() to convert across() the entire dataset all columns where() there are character vectors to factor() vectors (using the is.character() function to determine which columns contain character vectors).\n\n\n\n\n\n\nFigure¬†9.6: Artwork explaining the across() function by @allison_horst.\n\n\n\nWe can check that the correct variables have been converted by comparing the output of summary(L1.data) (partially printed in Section 9.3.1) with the output of summary(L1.data.fct) (partially printed below).\n\nsummary(L1.data.fct)\n\n\n\n  Participant      Age        Gender          Occupation OccupGroup\n 1      : 1   Min.   :17.00   F:48   Retired       :14   C  :22    \n 100    : 1   1st Qu.:25.00   M:42   Student       :14   I  :23    \n 101    : 1   Median :32.00          Unemployed    : 4   M  :20    \n 104    : 1   Mean   :37.54          Housewife     : 3   PS :24    \n 106    : 1   3rd Qu.:55.00          Shop Assistant: 3   PS : 1    \n 107    : 1   Max.   :65.00          Teacher       : 3             \n (Other):84                          (Other)       :49             \n    OtherLgs                                  Education      EduYrs     \n French : 2   student                              : 8   Min.   :10.00  \n German : 3   A level                              : 5   1st Qu.:12.00  \n None   :84   BA                                   : 5   Median :13.00  \n Spanish: 1   GCSEs                                : 5   Mean   :13.71  \n              NVQ                                  : 4   3rd Qu.:14.00  \n              Northern Counties School Leaving Exam: 3   Max.   :21.00  \n              (Other)                              :60                  \n    ReadEng1        ReadEng2        ReadEng3        ReadEng      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.: 5.000  \n Median :2.000   Median :2.000   Median :2.000   Median : 7.000  \n Mean   :2.522   Mean   :2.433   Mean   :2.233   Mean   : 7.189  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 9.750  \n Max.   :5.000   Max.   :5.000   Max.   :4.000   Max.   :14.000  \n                                                                 \n\n\n\n\n\n\n\n\nTask 1\n\n\n\nIn this task, you will do some data wrangling on the L2 dataset from DƒÖbrowska (2019).\na. Which of these columns from L2.data represent categorical variables and therefore ought to be converted to factors?\n\n\n\n\nOccupGroup\n\n\nUseEngC\n\n\nNativeLg\n\n\nArrival\n\n\nEdNative\n\n\nFirstExp\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\nb. Convert all character vectors of L2.data to factors and save the new table as L2.data.fct. Use the str() function to check that your conversion has worked as planned. How many different factor levels are there in the categorical variable Occupation?\n\n\n\n\n27\n\n\n48\n\n\n44\n\n\n67\n\n\n4\n\n\n45\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse to view R code to help you answer question c.\nL2.data.fct &lt;- L2.data |&gt; \n  mutate(across(where(is.character), factor))\n\nstr(L2.data.fct)\n\n\nc. Use the summary() and str() functions to inspect the sanity of L2 dataset now that you have converted all the character vectors to factors. Have you noticed that there three factor levels in the Gender variable of the L2 dataset whereas there are only two in the L1 dataset? What is the most likely reason for this?\n\n\n\n\nBecause sometimes \"female\" was recorded as lower-case \"f\" rather than upper-case \"F\".\n\n\nBecause these six participants declined to answer the question about their gender.\n\n\nBecause these six participants identify as non-binary.\n\n\nBecause these six participants were under 18 at the time of data collection.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#data-cleaning",
    "href": "9_DataWrangling.html#data-cleaning",
    "title": "9¬† Data wRangling",
    "section": "9.5 Data cleaning üßº",
    "text": "9.5 Data cleaning üßº\nBy closely examining the data, we noticed that the values of the categorical variables were not always entered in a consistent way, which may lead to incorrect analyses. For example, in the L2 dataset, most female participants‚Äô gender is recorded as F except for six participants, where it is f. As R is a case-sensitive language, these two factor levels are treated as two different levels of the Gender variable. This means that any future analyses on the effect of Gender on language learning will compare participants across these three groups.\n\nsummary(L2.data.fct$Gender)\n\n f  F  M \n 6 40 21 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo ensure that our analyses are reproducible from the beginning to the end, it is crucial that we document all of our corrections in a script. This ensures that if we need to go back on any data pre-processing decision that we made or if we need to make any additional corrections, we can do so without having to re-do our entire analyses. In addition, it means that our corrections and other data pre-processing steps are transparent and can be inspected and challenged by our peers.\n\n\n\n9.5.1 Using {stringr} functions\nTo convert all of the lower-case ‚Äúf‚Äù in the Gender variable to upper-case ‚ÄúF‚Äù, we can combine the mutate() with the str_to_upper() function. This ensures that all values in the new Gender.corrected column are in capital letters.\n\nL2.data.cleaned &lt;- L2.data.fct |&gt; \n  mutate(Gender.corrected = str_to_upper(Gender))\n\nWe should check that our correction has gone to plan by comparing the original Gender variable with the new Gender.corrected. To this end, we display them side by side using the select() function from {dplyr}.\n\nL2.data.cleaned |&gt; \n  select(Gender, Gender.corrected)\n\n\n\n  Gender Gender.corrected\n1      F                F\n2      f                F\n3      F                F\n4      F                F\n5      M                M\n6      F                F\n\n\nLike mutate() and select(), str_to_upper() also comes from a tidyverse package2. All functions that begin with str_ come from the {stringr} package, which features lots of useful functions to manipulate character string vectors. These include:\n\n\n\n\n\n\n\n\nFigure¬†9.7: Hex sticker of the {stringr} package\n\n\n\n\nstr_to_upper() converts to string upper case.\nstr_to_lower() converts to string lower case.\nstr_to_title() converts to string title case (i.e.¬†only the first letter of each word is capitalised).\nstr_to_sentence() converts string to sentence case (i.e.¬†only the first letter of each sentence is capitalised).\n\nFor more useful functions to manipulate character strings, check out the {stringr} cheatsheet: https://github.com/rstudio/cheatsheets/blob/main/strings.pdf.\nNote that in the code chunk above, we did not save the output to a new R object. We merely printed the output in the Console. Once we have checked that our data wrangling operation went well, we can overwrite the original Gender variable with the cleaned version by using the original variable name as the name of the new column.\n\nL2.data.cleaned &lt;- L2.data.fct |&gt; \n  mutate(Gender = str_to_upper(Gender))\n\nUsing summary() or class(), we can see that manipulating the Gender variable with a function from {stringr} has resulted in the factor variable being converted back to a character variable.\n\nsummary(L2.data.cleaned$Gender)\n\n   Length     Class      Mode \n       67 character character \n\nclass(L2.data.cleaned$Gender)\n\n[1] \"character\"\n\n\nWe therefore need to add a line of code to reconvert it to a factor. We can do this within a single mutate() command.\n\nL2.data.cleaned &lt;- L2.data.fct |&gt; \n  mutate(Gender = str_to_upper(Gender),\n         Gender = factor(Gender))\n\nclass(L2.data.cleaned$Gender)\n\n[1] \"factor\"\n\n\nNow the summary() function provides a tally of male and female participants that corresponds to the values reported in DƒÖbrowska (2019: 5).\n\nsummary(L2.data.cleaned$Gender)\n\n F  M \n46 21 \n\n\n\n\n\n\n\n\nTask 2\n\n\n\nThis task focuses on the OccupGroup variable, which is found in both the L1 and L2 datasets.\nOccupGroup is a categorical variable that groups participants‚Äô professional occupations into different categories. In the L2 dataset, there are four occupational categories.\n\nL2.data.fct |&gt; \n  count(OccupGroup)\n\n  OccupGroup  n\n1          C 10\n2          I  3\n3          M 21\n4         PS 33\n\n\nDƒÖbrowska (2019: 6) explains that these abbreviations correspond to:\n\nC: Clerical positions\nI: Occupationally inactive (i.e., unemployed, retired, or homemakers)\nM: Manual jobs\nPS: Professional-level jobs or studying for a degree\n\na. Examine the OccupGroup variable in the L1 dataset (L1.data). What do you notice? Why are L1 participants grouped into five rather than four occupational categories?\n\n\n\n\nBecause this study has more L1 participants than L2 participants.\n\n\nBecause one L1 participant had an occupation that did not fit any of the other four categories.\n\n\nBecause an extra space character was accidentally added after one \"PS\" value.\n\n\nBecause the original data file was saved in a format incompatible with R.\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse to view R code to help you answer question a.\nsummary(L1.data.fct$OccupGroup)\n##   C   I   M  PS PS  \n##  22  23  20  24   1\n\nL1.data.fct |&gt; \n  count(OccupGroup)\n##   OccupGroup  n\n## 1          C 22\n## 2          I 23\n## 3          M 20\n## 4         PS 24\n## 5        PS   1\n\n\n¬†\nb. Which {stringr} function removes trailing spaces from character strings? Find the appropriate function on the {stringr} cheatsheet.\n\n\n\n\nstr_extract()\n\n\nstr_glue()\n\n\nstr_ends()\n\n\nstr_squish()\n\n\nstr_trim()\n\n\nstr_flatten()\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow R code to use the function and check that it worked as expected.\nL1.data.cleaned &lt;- L1.data.fct |&gt; \n  mutate(OccupGroup = str_trim(OccupGroup)) # Apply the str_trim() function to the OccupGroup variable\n\nL1.data.cleaned |&gt; \n  count(OccupGroup)\n\n\n¬†\nc. Following the removal of trailing whitespaces, what percentage of L1 participants have a professional-level jobs/are studying for a degree?\n\n\n\n\n20%\n\n\n22.22%\n\n\n24.44%\n\n\n25%\n\n\n26%\n\n\n28%\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nShow R code to answer question c.\nL1.data.cleaned |&gt; \n  count(OccupGroup) |&gt; \n  mutate(percent = n / sum(n),\n         percent = percent*100, \n         percent = round(percent, digits = 2)\n         )\n\n\n\n\nSo far, we have looked at rather simple data cleaning cases. Let‚Äôs now turn to a slightly more complex one: In the L2 dataset, the variable NativeLg contains character string values that correspond to the L2 participants‚Äô native language. Using the base R function unique(), we can see that there are a total of 22 unique values in this variable. However using sort() to order these 22 values alphabetically, we can easily see that there are, in fact, fewer unique native languages in this dataset due to different spellings and the inconsistent use of upper-case letters.\n\nL2.data$NativeLg |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"           \"Cantonese/Hokkein\"   \"chinese\"            \n [4] \"Chinese\"             \"french\"              \"German\"             \n [7] \"greek\"               \"Italian\"             \"Lithuanian\"         \n[10] \"Lithunanina\"         \"Lituanian\"           \"Mandarin\"           \n[13] \"Mandarin Chinese\"    \"Mandarin/ Cantonese\" \"mandarin/malaysian\" \n[16] \"Mandarine Chinese\"   \"polish\"              \"Polish\"             \n[19] \"Polish/Russian\"      \"russian\"             \"Russian\"            \n[22] \"Spanish\"            \n\n\nIf we convert all NativeLg values to title case, we can reduce the number of unique languages to 19.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"           \"Cantonese/Hokkein\"   \"Chinese\"            \n [4] \"French\"              \"German\"              \"Greek\"              \n [7] \"Italian\"             \"Lithuanian\"          \"Lithunanina\"        \n[10] \"Lituanian\"           \"Mandarin\"            \"Mandarin Chinese\"   \n[13] \"Mandarin/ Cantonese\" \"Mandarin/Malaysian\"  \"Mandarine Chinese\"  \n[16] \"Polish\"              \"Polish/Russian\"      \"Russian\"            \n[19] \"Spanish\"            \n\n\nSecond, to facilitate further analyses, we may decide to only retain the first word/language from each entry as this will further reduce the number of different levels in this categorical variable. To abbreviate ‚ÄúMandarin Chinese‚Äù to ‚ÄúMandarin‚Äù, we can use the word() function from the {stringr} package.\nBelow is an extract of the help page for the word() function (accessed with the command ?word). Can you work out how to extract the first word of a character string?\n\n\n\n\nword {stringr}\nR Documentation\n\n\n\n9.6 Extract words from a sentence\n9.6.1 Description\nExtract words from a sentence\n9.6.2 Usage\nword(string, start = 1L, end = start, sep = fixed(\" \"))\n9.6.3 Arguments\n\n\n\n\n\n\n\nstring\nInput vector. Either a character vector, or something coercible to one.\n\n\nstart, end\nPair of integer vectors giving range of words (inclusive) to extract. If negative, counts backwards from the last word.\nThe default value select the first word.\n\n\nsep\nSeparator between words. Defaults to single space.\n\n\n\n\nThe help file tells us that ‚ÄúThe default value select the first word‚Äù. In our case, this means that we can simply use the word() function with no specified argument as this will automatically retain only the first word of every entry.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word() |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"          \"Cantonese/Hokkein\"  \"Chinese\"           \n [4] \"French\"             \"German\"             \"Greek\"             \n [7] \"Italian\"            \"Lithuanian\"         \"Lithunanina\"       \n[10] \"Lituanian\"          \"Mandarin\"           \"Mandarin/\"         \n[13] \"Mandarin/Malaysian\" \"Mandarine\"          \"Polish\"            \n[16] \"Polish/Russian\"     \"Russian\"            \"Spanish\"           \n\n\nAlternatively, we can choose to specify the ‚Äústart‚Äù argument as a reminder of what we did and to better document our code. The output is exactly the same.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word(start = 1) |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"          \"Cantonese/Hokkein\"  \"Chinese\"           \n [4] \"French\"             \"German\"             \"Greek\"             \n [7] \"Italian\"            \"Lithuanian\"         \"Lithunanina\"       \n[10] \"Lituanian\"          \"Mandarin\"           \"Mandarin/\"         \n[13] \"Mandarin/Malaysian\" \"Mandarine\"          \"Polish\"            \n[16] \"Polish/Russian\"     \"Russian\"            \"Spanish\"           \n\n\nAs you can tell from the output above, the word() function uses white space to identify word boundaries. In this dataset, however, some of the participants‚Äô native languages are separated by forward slashes (/) rather than or in addition to spaces. The ‚ÄúUsage‚Äù section of the help file for the word() function (see ?word and Section 9.6) also confirms that the default word separator symbol is a space and shows us the syntax for changing the default separator. Below we change it to a forward slash.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word(start = 1, sep = fixed(\"/\")) |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"         \"Chinese\"           \"French\"           \n [4] \"German\"            \"Greek\"             \"Italian\"          \n [7] \"Lithuanian\"        \"Lithunanina\"       \"Lituanian\"        \n[10] \"Mandarin\"          \"Mandarin Chinese\"  \"Mandarine Chinese\"\n[13] \"Polish\"            \"Russian\"           \"Spanish\"          \n\n\nNow we can combine these two word extraction methods using the pipe operator (|&gt;) so that ‚ÄúCantonese/Hokkein‚Äù is abbreviated to ‚ÄúCantonese‚Äù and ‚ÄúMandarin/ Cantonese‚Äù to ‚ÄúMandarin‚Äù.\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  word(start = 1) |&gt; # Extracts the first word before the first space\n  word(start = 1, sep = fixed(\"/\")) |&gt; # Extracts the first word before the first forward slash\n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"   \"Chinese\"     \"French\"      \"German\"      \"Greek\"      \n [6] \"Italian\"     \"Lithuanian\"  \"Lithunanina\" \"Lituanian\"   \"Mandarin\"   \n[11] \"Mandarine\"   \"Polish\"      \"Russian\"     \"Spanish\"    \n\n\n\n\n\n\n\n\nGoing further: Using regular expressions (regex) ü§ì\n\n\n\n\n\nMany functions of the {stringr} package involve regular expressions (short: regex). The second page of the {stringr} cheatsheet provides a nice overview of how regular expressions can be used to manipulate character strings in R.\nUsing the str_extract() function together with the regex \\\\w+, it is possible to extract the first word of each NativeLg value with just one line of code:\n\nL2.data$NativeLg |&gt;\n  str_to_title() |&gt; \n  str_extract(\"\\\\w+\") |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"Cantonese\"   \"Chinese\"     \"French\"      \"German\"      \"Greek\"      \n [6] \"Italian\"     \"Lithuanian\"  \"Lithunanina\" \"Lituanian\"   \"Mandarin\"   \n[11] \"Mandarine\"   \"Polish\"      \"Russian\"     \"Spanish\"    \n\n\nRegular expressions provide incredibly powerful and versatile ways to work with text in all kinds of programming languages. When conducting corpus linguistics research, they also allow us to conduct complex corpus queries.\nEach programming language/software has a slightly different flavour of regex but the basic principles are the same across all languages/software and are well worth learning. To get started, I highly recommend this beautifully designed interactive regex tutorial for beginners: https://regexlearn.com/learn/regex101. Have fun! ü§ì\n\n\n\n\n\n9.6.4 Using case_when()\nWe have now reduced the number of levels in the NativeLg variable to just 14 unique languages. But we still have some typos to correct, e.g., ‚ÄúLithunanina‚Äù and ‚ÄúLituanian‚Äù.\nWe can correct these on a case-by-case basis using case_when(). This is a very useful tidyverse function from the {dplyr} package that is easy to use once you have gotten used to its syntax. Figure¬†9.8 illustrates the syntax with a toy example dataset about the dangerousness of dragons (df). In this annotated line of code in Figure¬†9.8, mutate() is used to add a new column called danger whose values depend on the type of dragon that we are dealing with. The first argument of case_when() determines that, when the dragon type is equal to ‚Äúkraken‚Äù, then the danger value is set to ‚Äúextreme‚Äù, otherwise the danger value is set to ‚Äúhigh‚Äù. You can see the outcome in the appended danger column.\n\n\n\n\n\n\nFigure¬†9.8: Artwork explaining the case_when() function by @allison_horst).\n\n\n\nApplying case_when() to fix the typos in the NativeLg variable in L2.data, we determine that:\n\nif the shortened NativeLg value is ‚ÄúMandarine‚Äù, we replace it with ‚ÄúMandarin‚Äù, and\nif the shortened NativeLg value corresponds to either ‚ÄúLithunanina‚Äù or ‚ÄúLituanian‚Äù, we replace it with ‚ÄúLithuanian‚Äù.\n\nUsing mutate(), we save this cleaned-up version of the NativeLg variable as a new column in our L2.data table, which we call NativeLg.cleaned.\n\nL2.data &lt;- L2.data |&gt;\n  mutate(\n    NativeLg.cleaned = str_to_title(NativeLg) |&gt; \n      word(start = 1) |&gt; \n      word(start = 1, sep = fixed(\"/\")),\n    NativeLg.cleaned = case_when(\n      NativeLg.cleaned == \"Mandarine\" ~ \"Mandarin\",\n      NativeLg.cleaned %in% c(\"Lithunanina\", \"Lituanian\") ~ \"Lithuanian\",\n      TRUE ~ NativeLg.cleaned)\n    )\n\nWhenever we do any data wrangling, it is crucial that we take the time to carefully check that we have not made any mistakes in the process. To this end, we display the original NativeLg and the new NativeLg.cleaned variables side by side using the select() function.\n\nL2.data |&gt; \n  select(NativeLg, NativeLg.cleaned)\n\n\n\n    NativeLg NativeLg.cleaned\n1 Lithuanian       Lithuanian\n2     polish           Polish\n3     Polish           Polish\n4    Italian          Italian\n5  Lituanian       Lithuanian\n6     Polish           Polish\n\n\nAs you can see, only the first six rows of the table are printed above. Run the code yourself to check all the other rows.\n\n\n\n\n\n\nUsing base R functions instead\n\n\n\n\n\nThis chapter focuses on {tidyverse} functions, however all of the above data wrangling and cleaning operations can equally be achieved using base R functions. For example, the mutate() code chunk above could be replaced by the following lines of base R code.\n\nL2.data$NativeLg.cleaned.base &lt;- gsub(\"([a-zA-Z]+).*\", \"\\\\1\", L2.data$NativeLg)\nL2.data$NativeLg.cleaned.base &lt;- tools::toTitleCase(L2.data$NativeLg.cleaned.base)\nL2.data$NativeLg.cleaned.base[L2.data$NativeLg.cleaned.base == \"Mandarine\"] &lt;- \"Mandarin\"\nL2.data$NativeLg.cleaned.base[L2.data$NativeLg.cleaned.base %in% c(\"Lithunanina\", \"Lituanian\")] &lt;- \"Lithuanian\"\n\n\nWith the first line, we extract the first string of letters before any space or slash in NativeLg and save this to a new variable called NativeLg.cleaned.base.\nThe second line converts all the values of the new variable to title case using a base R function from the {tools} package. The {tools} package comes with R so you don‚Äôt need to install it separately but, if you haven‚Äôt loaded it earlier in your R session, you need to call the function with the prefix tools:: so that R knows where to find the toTitleCase() function.\nThe third line corrects a typo with a direct replacement, whilst the fourth replaces two typos with a single correction.\n\nIf we now compare the variable created with the tidyverse code (NativeLg.cleaned) vs.¬†the one created using base R functions only (NativeLg.cleaned.base), we can see that they are exactly the same.\n\nL2.data |&gt; \n  select(NativeLg.cleaned, NativeLg.cleaned.base) \n\n\n\n  NativeLg.cleaned NativeLg.cleaned.base\n1       Lithuanian            Lithuanian\n2           Polish                Polish\n3           Polish                Polish\n4          Italian               Italian\n5       Lithuanian            Lithuanian\n6           Polish                Polish\n\n\nAn undeniable advantage of sticking to base R functions is that your code is more portable as it does not require the installation of any additional packages, keeping dependencies on external packages to the minimum. However, base R lacks the consistency of the tidyverse framework, which can make certain data transformation tasks considerably more tricky and code less readable (and therefore less transparent) to yourself and others.\nAs we don‚Äôt need two versions of the cleaned NativLg variable, we will now remove the NativeLg.cleaned.base column from L2.data. To do so, we use the select() function combined with the - operator to ‚Äúunselect‚Äù the column we no longer need.\n\nL2.data &lt;- L2.data |&gt; \n  select(- NativeLg.cleaned.base)\n\n\n\n\n\n\n\n\n\n\nTask 3\n\n\n\nFor some analyses, it may be useful to group together participants whose native languages come from the same family of languages. For example, French, Spanish and Italian L1 speakers, may be considered as a one group of participants whose native language is a Romance language.\nUse mutate() and case_when() to add a new variable to L2.data that corresponds to the L2 participant‚Äôs native language family. Call this new variable NativeLgFamily. Use the following language family categories:\n\nBaltic\nChinese\nGermanic\nHellenic\nRomance\nSlavic\n\nIf you‚Äôre not sure which language family a language belongs to, look it up on Wikipedia (e.g.¬†the Wikipedia page on the German language informs us in a text box at the top of the article that German is a Germanic language).\na. Which language family is the second most represented among L2 participants‚Äô native languages in DƒÖbrowska (2019)?\n\n\n\n\nBaltic\n\n\nChinese\n\n\nGermanic\n\n\nHellenic\n\n\nRomance\n\n\nSlavic\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nb. How many L2 participants are native speakers of a language that belongs to the family of Romance languages?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nc. What percentage of L2 participants have a Slavic native language? Round your answer to the nearest percent.\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nd. If you check the output of colnames(L2.data) or View(L2.data), you will see that the new variable that you created is now the last column in the table. Consult the help file of the {dplyr} function relocate() to work out how to place this column immediately after NativeLg.\n\n\n\n\nL2.data &lt;- L2.data |&gt; relocate(f, .after = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, .after = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, .after = NULL)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, after = NativeLg)\nL2.data &lt;- L2.data |&gt; relocate(NativeLgFamily, .before = NativeLg)\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\n\nClick here for solutions to Task 3\n\n\n\n\n\nAs always, there are several solutions to solving this task. Here is one solution based on what we have covered so far in this chapter.\na. Note that the following code will only work if you followed the instructions in the section above to create the NativeLg.cleaned variable as it relies on this variable to create the new NativeLgFamily variable.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(NativeLgFamily = case_when(\n    NativeLg.cleaned == \"Lithuanian\" ~ \"Baltic\",\n    NativeLg.cleaned %in% c(\"Cantonese\", \"Mandarin\", \"Chinese\") ~ \"Chinese\",\n    NativeLg.cleaned == \"German\" ~ \"Germanic\",\n    NativeLg.cleaned == \"Greek\" ~ \"Hellenic\",\n    NativeLg.cleaned %in% c(\"French\", \"Italian\", \"Spanish\") ~ \"Romance\",\n    NativeLg.cleaned %in% c(\"Polish\", \"Russian\") ~ \"Slavic\"))\n\nAs always, it is important to check that things have gone to plan.\n\nL2.data |&gt; \n  select(NativeLg.cleaned, NativeLgFamily)\n\n\n\n  NativeLg.cleaned NativeLgFamily\n1       Lithuanian         Baltic\n2           Polish         Slavic\n3           Polish         Slavic\n4          Italian        Romance\n5       Lithuanian         Baltic\n6           Polish         Slavic\n\n\nb. We can display the distribution of language families using either the base R table() function or the {tidyverse} count() function.\n\ntable(L2.data$NativeLgFamily)\n\n\n  Baltic  Chinese Germanic Hellenic  Romance   Slavic \n       5       15        1        1        6       39 \n\nL2.data |&gt; \n  count(NativeLgFamily)\n\n  NativeLgFamily  n\n1         Baltic  5\n2        Chinese 15\n3       Germanic  1\n4       Hellenic  1\n5        Romance  6\n6         Slavic 39\n\n\nc. We can add a column to show the distribution in percentages by adding a new ‚Äúpercent‚Äù column to the count() table using mutate().3 The steps are the following:\n\nWe start with the dataset that contains the new NativeLgFamily variable.\nWe pipe it into the count() function. As shown above, this function produces a frequency table with counts stored in the variable n.\nWe divide the number of participant with each native language (n) by the total number of participants (sum(n)). We obtain proportions ranging from 0 to 1.\nWe multiply these by 100 to get percentages.\nWe round the percentages to two decimal places.\nWe reorder the table so that the most represented group is at the top. To do so, we pipe our table into the dplyr::arrange(). By default, arrange() orders values in ascending order (from smallest to largest); hence, we add the desc() function to sort the table in descending order of frequency.\n\n\nL2.data |&gt; \n  count(NativeLgFamily) |&gt; \n  mutate(percent = n / sum(n),\n         percent = percent*100, \n         percent = round(percent, digits = 0)\n         ) |&gt; \n  arrange(desc(n))\n\n  NativeLgFamily  n percent\n1         Slavic 39      58\n2        Chinese 15      22\n3        Romance  6       9\n4         Baltic  5       7\n5       Germanic  1       1\n6       Hellenic  1       1\n\n\nd. At the time of writing, the help file of the relocate() function still featured examples using the {magrittr} pipe (%&gt;%) rather than the native R pipe (|&gt;) (see Section 7.5.2), but the syntax remains the same. The first argument is the data which we are piping into the function, the second argument is the column that we want to move. Then, we need to specify where to with either the ‚Äú.after‚Äù or the ‚Äú.before‚Äù argument.\n\nL2.data &lt;- L2.data |&gt; \n  relocate(NativeLgFamily, .after = NativeLg)\n\n\n\n\n\n\n\nFigure¬†9.9: Artwork explaining the dplyr::relocate() function by @allison_horst.\n\n\n\nNote that the help file specifies that both ‚Äú.after‚Äù and ‚Äú.before‚Äù begin with a dot. If you leave the dot out, the function will not work as expected! Can you see what‚Äôs happened here?\n\nL2.data |&gt; \n  relocate(NativeLgFamily, after = NativeLg) |&gt; \n  str()\n\n\n\n'data.frame':   67 obs. of  6 variables:\n $ NativeLgFamily: chr  \"Baltic\" \"Slavic\" \"Slavic\" \"Romance\" ...\n $ after         : chr  \"Lithuanian\" \"polish\" \"Polish\" \"Italian\" ...\n $ Participant   : int  220 244 46 221 222 230 247 237 243 213 ...\n $ Gender        : chr  \"F\" \"f\" \"F\" \"F\" ...\n $ Occupation    : chr  \"Student\" \"student\" \"Cleaner\" \"Student\" ...\n $ OccupGroup    : chr  \"PS\" \"PS\" \"M\" \"PS\" ...\n\n\nThe relocate() function has moved NativeLgFamily to the first column (the function‚Äôs default position) and has also moved NativeLg to the second position, but it has renamed the column after.\nThis is a reminder to always check whether your data wrangling operations have gone as planned. Just because you didn‚Äôt get an error message doesn‚Äôt mean that your code did what you wanted! ‚ö†Ô∏è",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#sec-CombiningDatasets",
    "href": "9_DataWrangling.html#sec-CombiningDatasets",
    "title": "9¬† Data wRangling",
    "section": "9.7 Combining datasets",
    "text": "9.7 Combining datasets\nSo far, we have analysed the L1 and L2 datasets individually. In the following chapters, however, we will conduct comparative analyses, comparing the performance of the L1 and L2 participants in the various language-related tests conduced as part of DƒÖbrowska (2019). To this end, we need to create a combined table that includes the data of all participants from DƒÖbrowska (2019).\nRemember that both tables, L1.data and L2.data, are in a tidy data format. This means that:\n\neach row represents an observation (i.e., here, a participant),\neach cell represents a measurement, and\neach variable forms a column.\n\nTo combine the two datasets, therefore, we need to combine the rows of the two tables. However, we cannot simply add the rows of the L2.data table to the bottom of L1.data table because, as shown below, the two tables do not have the same number of columns and the shared columns are not in the same position! We therefore need to ensure that, when the two datasets are combined, the shared columns are aligned.\n\ncolnames(L1.data)\n\n [1] \"Participant\" \"Age\"         \"Gender\"      \"Occupation\"  \"OccupGroup\" \n [6] \"OtherLgs\"    \"Education\"   \"EduYrs\"      \"ReadEng1\"    \"ReadEng2\"   \n[11] \"ReadEng3\"    \"ReadEng\"     \"Active\"      \"ObjCl\"       \"ObjRel\"     \n[16] \"Passive\"     \"Postmod\"     \"Q.has\"       \"Q.is\"        \"Locative\"   \n[21] \"SubCl\"       \"SubRel\"      \"GrammarR\"    \"Grammar\"     \"VocabR\"     \n[26] \"Vocab\"       \"CollocR\"     \"Colloc\"      \"Blocks\"      \"ART\"        \n[31] \"LgAnalysis\"  \"Gender.fct\" \n\n\n\ncolnames(L2.data)\n\n [1] \"Participant\"      \"Gender\"           \"Occupation\"       \"OccupGroup\"      \n [5] \"NativeLg\"         \"NativeLgFamily\"   \"OtherLgs\"         \"EdNative\"        \n [9] \"EdUK\"             \"Age\"              \"EduYrsNat\"        \"EduYrsEng\"       \n[13] \"EduTotal\"         \"FirstExp\"         \"Arrival\"          \"LoR\"             \n[17] \"EngWork\"          \"EngPrivate\"       \"ReadEng1\"         \"ReadOth1\"        \n[21] \"ReadEng2\"         \"ReadOth2\"         \"ReadEng3\"         \"ReadOth3\"        \n[25] \"ReadEng\"          \"ReadOth\"          \"Active\"           \"ObjCl\"           \n[29] \"ObjRel\"           \"Passive\"          \"Postmod\"          \"Q.has\"           \n[33] \"Q.is\"             \"Locative\"         \"SubCl\"            \"SubRel\"          \n[37] \"GrammarR\"         \"Grammar\"          \"VocabR\"           \"Vocab\"           \n[41] \"CollocR\"          \"Colloc\"           \"Blocks\"           \"ART\"             \n[45] \"LgAnalysis\"       \"UseEngC\"          \"NativeLg.cleaned\"\n\n\nNote, also, that participants‚Äô total number of years in education is stored in the EduYrs column in L1.data, whereas the corresponding column in L2.data is called EduTotal. Hence, we first use the {dplyr} function rename() to rename EduYrs in L1.data as EduTotal before we merge the two tables.\n\nL1.data &lt;- L1.data |&gt; \n  rename(EduTotal = EduYrs)\n\nThe {dplyr} package boasts an array of useful functions to combine tables (see Figure¬†9.10). For our purposes, bind_rows() appears to be the perfect function.4\n\n\n\n\n\n\nFigure¬†9.10: Extract of the data transformation with {dplyr} cheatsheet (CC BY SA Posit Software, PBC)\n\n\n\nHowever, when we try to combine L1.data and L2.data using bind_rows(), we get an error message‚Ä¶ üò¢ Does this error remind you of Q10 from Section 7.2.3 by any chance?\n\ncombined.data &lt;- bind_rows(L1.data, L2.data)\n\nError in `bind_rows()`:\n! Can't combine `..1$Participant` &lt;character&gt; and `..2$Participant` &lt;integer&gt;.\nWhat this error message tells us is that the bind_rows() function cannot combine the two Participant columns because in L1.data it is a string character vector, whereas in L2.data it is an integer vector. However, to avoid data loss, bind_rows() can only match columns of the same data type!\nWe must therefore first convert the Participant variable in L2.data to a character vector.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Participant = as.character(Participant))\n\nNow, we can combine the two data frames using bind_rows().\n\ncombined.data &lt;- bind_rows(L1.data, L2.data)\n\nThe problem is that now that we have merged our two datasets into one, it‚Äôs not obvious which rows correspond to L1 participants and which to L2 participants! There are various ways to solve this, but here‚Äôs a simple three-step solution that relies exclusively on functions that you are already familiar with.\nStep 1: We add a new column to L1.data called Group and fill this column with the value ‚ÄúL1‚Äù for all rows.\n\nL1.data &lt;- L1.data |&gt; \n  mutate(Group = \"L1\")\n\nStep 2: We add a new column to L2.data also called Group and fill this column with the value ‚ÄúL2‚Äù for all rows.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Group = \"L2\")\n\nStep 3: We use bind_rows() as above to combine the two datasets that now both include the extra Group column.5.\n\ncombined.data &lt;- bind_rows(L1.data, L2.data)\n\nVerification step: The combined.data table now includes the column Group, which we can use to easily identify the observations that belong to L1 and L2 participants. As expected, our combined dataset includes 90 participants from the L1 group and 67 from the L2 group:\n\ncombined.data |&gt; \n  count(Group)\n\n  Group  n\n1    L1 90\n2    L2 67\n\n\nOur combined dataset contains all the columns that appear in either L1.data or L2.data. Check that this is the case by examining the structure of the new dataset with str(combined.data).\nYou will have noticed that, in some columns, there are lots of NA (‚ÄúNot Available‚Äù) values. These represent missing data. R has inserted these NA values in the columns that only appear in one of the two datasets. For example, the L1 dataset does not include an Arrival variable (indicating the age when participants first arrived in an English-speaking country), presumably because they were all born in an English-speaking country! We only have this information for the L2 participants and this explains the 90 NA values in the Arrival column of the combined dataset.\n\ncombined.data$Arrival\n\n  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 17 18 19 19 19 19 20 22 22 23\n[101] 24 25 26 26 28 28 30 44 45 49 17 23 23 24 19 22 23 26 16 18 24 24 25 29 30\n[126] 32 33 22 25 30 44 27 18 20 21 33 38 47 16 19 20 24 25 28 33 20 22 25 28 26\n[151] 26 16 24 32 20 16 20\n\n\nWe can also check this by cross-tabulating the Group and the Arrival variables.\n\ncombined.data |&gt; \n  count(Group, Arrival)\n\n\n\n  Group Arrival  n\n1    L1      NA 90\n2    L2      16  4\n3    L2      17  2\n4    L2      18  3\n5    L2      19  6\n6    L2      20  6\n\n\nRun View(combined.data) to inspect the combined dataset and check in which other columns there are NA values.\n\n\n\n\n\n\nWhat if my data is not yet in tidy format? ü§®\n\n\n\n\n\nCombining the two datasets from DƒÖbrowska (2019) was relatively easy because the data was already in tidy format. But, fear not: if you need to first convert your data to tidy format, the {tidyr} package has got you covered! üòé\nThe pivot_longer() and pivot_wider() functions allow you to easily convert tables from ‚Äúlong‚Äù to ‚Äúwide‚Äù format and vice versa (see Figure¬†9.11).\n\n\n\n\n\n\nFigure¬†9.11: Extract of the data tidying with tidyr cheatsheet (CC BY SA Posit Software, PBC)\n\n\n\n\nRemember to carefully check the output of any data manipulation that you do before moving on to doing any analyses! To this end, the View() function is particularly helpful.\n\n\n\n\n\n\n\n\n\n\nFigure¬†9.12: Hex sticker of the {tidyr} package\n\n\n\n\n\n\n\n\n\nUsing AI tools for coding ‚ö†Ô∏è\n\n\n\nNote that older textbooks/tutorials, and especially AI tools such as ChatGPT that have been trained on older web data, will frequently suggest superseded (i.e.¬†outdated) functions for data manipulation such as spread(), gather(), select_all(), and mutate_if(). If you use superseded functions, your code will still work, but R will print a warning in the Console and usually suggest a modern alternative.\nAI tools may also suggest using functions that are deprecated. As with superseded functions, you will get a warning message with a recommended alternative. In this case, however, you must follow the advice of the warning, as writing new code with deprecated functions is really asking for trouble! Deprecated functions are scheduled for removal, which means that your code will eventually no longer run on up-to-date R versions.\n\n\n\n\n\n\nFigure¬†9.13: The four main stages of the lifecycle of R packages, functions, function arguments: experimental developments can become stable and stable can eventually become deprecated or superseded (image by Henry and Wickham 2023 for Posit Software, PBC, https://lifecycle.r-lib.org/articles/stages.html).\n\n\n\nIn sum, to ensure the future compatibility of your code, do not ignore warnings about deprecated functions and, in general, never ever blindly trust the output of AI tools!",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#a-pre-processing-pipeline",
    "href": "9_DataWrangling.html#a-pre-processing-pipeline",
    "title": "9¬† Data wRangling",
    "section": "9.8 A pre-processing pipeline",
    "text": "9.8 A pre-processing pipeline\nSo far in this chapter, we have learnt how to pre-process data for future statistical analyses and data visualisation. In the process, we have learnt about lots of different functions, mostly from the tidyverse environment (see Section 9.1 ü™ê). Now it‚Äôs time to put everything together and save our pre-processed combined dataset for future use.\nBut, first, let‚Äôs recap all of the data wrangling operations that we performed in this chapter and combine them into one code chunk. Before running this code, we first reload the original data from DƒÖbrowska (2019) to overwrite any changes that were made during this chapter. This will ensure that we all have exactly the same version of the dataset for the following chapters.\n\nlibrary(here)\n\nL1.data &lt;- read.csv(file = here(\"data\", \"L1_data.csv\"))\nL2.data &lt;- read.csv(file = here(\"data\", \"L2_data.csv\"))\n\nThen, run the following lines of code to create a new R object called combined.data that contains the wrangled data.\n\nL2.data &lt;- L2.data |&gt; \n  mutate(Participant = as.character(Participant)) |&gt; \n  mutate(Group = \"L2\")  \n\nL1.data &lt;- L1.data |&gt; \n  mutate(Group = \"L1\") |&gt; \n  rename(EduTotal = EduYrs)\n\ncombined.data &lt;- bind_rows(L1.data, L2.data) |&gt;\n  mutate(across(where(is.character), str_to_title)) |&gt;\n  mutate(across(where(is.character), str_trim)) |&gt;\n  mutate(OccupGroup = str_to_upper(OccupGroup)) |&gt; \n  mutate(\n    NativeLg = word(NativeLg, start = 1),\n    NativeLg = word(NativeLg, start = 1, sep = fixed(\"/\")),\n    NativeLg = case_when(\n      NativeLg == \"Mandarine\" ~ \"Mandarin\",\n      NativeLg %in% c(\"Lithunanina\", \"Lithunanina\", \"Lituanian\") ~ \"Lithuanian\",\n      TRUE ~ NativeLg)) |&gt; \n  mutate(NativeLgFamily = case_when(\n    NativeLg == \"Lithuanian\" ~ \"Baltic\",\n    NativeLg %in% c(\"Cantonese\", \"Mandarin\", \"Chinese\") ~ \"Chinese\",\n    NativeLg == \"German\" ~ \"Germanic\",\n    NativeLg == \"Greek\" ~ \"Hellenic\",\n    NativeLg %in% c(\"French\", \"Italian\", \"Spanish\") ~ \"Romance\",\n    NativeLg %in% c(\"Polish\", \"Russian\") ~ \"Slavic\")) |&gt; \n  mutate(across(where(is.character), factor))\n\nDon‚Äôt forgot to check the result by examining the output of View(combined.data) and str(combined.data).\n\nsummary(combined.data)\n\n\n\n  Participant       Age        Gender             Occupation OccupGroup\n 1      :  1   Min.   :17.00   F:94   Student          :27   C :32     \n 100    :  1   1st Qu.:25.00   M:63   Retired          :15   I :26     \n 101    :  1   Median :31.00          Product Operative: 5   M :41     \n 104    :  1   Mean   :35.48          Teacher          : 5   PS:58     \n 106    :  1   3rd Qu.:42.00          Cleaner          : 4             \n 107    :  1   Max.   :65.00          Unemployed       : 4             \n (Other):151                          (Other)          :97             \n              OtherLgs    Education     EduTotal        ReadEng1    \n None             :98   Student: 8   Min.   : 8.50   Min.   :0.000  \n No               :11   A Level: 5   1st Qu.:13.00   1st Qu.:1.000  \n English          : 8   Ba     : 5   Median :14.00   Median :3.000  \n German           : 6   Gcses  : 5   Mean   :14.62   Mean   :2.599  \n English At School: 3   Nvq    : 4   3rd Qu.:17.00   3rd Qu.:4.000  \n English, German  : 2   (Other):63   Max.   :24.00   Max.   :5.000  \n (Other)          :29   NA's   :67                                  \n    ReadEng2        ReadEng3        ReadEng      \n Min.   :0.000   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 5.000  \n Median :2.000   Median :2.000   Median : 7.000  \n Mean   :2.465   Mean   :2.019   Mean   : 7.083  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:10.000  \n Max.   :5.000   Max.   :4.000   Max.   :14.000  \n                                                 \n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ3. The following operations describe the steps performed by the data wrangling code chunk above. In which order are the operations performed?\n\n\n\n\n\n\nMerge data from the two datasets into one.\n\n\n\n‚áÖ\n\n\n\nConvert all the values of one variable to upper case.\n\n\n\n‚áÖ\n\n\n\nConvert one variable to a character variable.\n\n\n\n‚áÖ\n\n\n\nConvert all character string vectors to factors.\n\n\n\n‚áÖ\n\n\n\nConvert the values of all character variables to title case.\n\n\n\n‚áÖ\n\n\n\nAdd two new variables that each have all the same values.\n\n\n\n‚áÖ\n\n\n\nChange the name of one variable\n\n\n\n‚áÖ\n\n\n\nRemove whitespace at the start and end of all values in all character variables.\n\n\n\n‚áÖ\n\n\n\nAdd a new variable based on another variable.\n\n\n\n‚áÖ\n\n\n\nShorten and correct typos in the values of one variable.\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ4. In the combined dataset, how many participants have a clerical occupation?\n\n\n\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n¬†\nQ5. Of the participants who have a clerical occupation, how many were over 50 years old at the time of the data collection?\n\n\n\n\nnone\n\n\n1\n\n\n2\n\n\n6\n\n\nall of them\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for R code to answer {#sec-filter} Q5.\n\n\n\n\n\nThere are various ways to find the answer to Q5. Sticking to a function that we have looked at so far, you could cross-tabulate Age and OccupGroup using the count() function.\n\ncombined.data |&gt; \n  count(OccupGroup, Age)\n\n\n\n   OccupGroup Age n\n1           C  20 1\n2           C  25 6\n3           C  27 2\n4           C  28 3\n5           C  29 4\n6           C  30 3\n7           C  32 3\n8           C  37 1\n9           C  38 1\n10          C  39 1\n11          C  41 1\n12          C  51 2\n13          C  52 1\n14          C  53 1\n15          C  57 1\n16          C  60 1\n\n\nAnd then add up the frequencies listed in the rows that correspond to participants with clerical jobs who are 50.\n\n2 + 1 + 1 + 1 +1 \n\n[1] 6\n\n\nBut, of course, this is method is rather error-prone! Instead, we can use dplyr::filter() (see Figure¬†9.14) to filter the combined dataset according to our two criteria of interest and then count the number of rows (i.e., participants) remaining in the dataset once the filter has been applied.\n\ncombined.data |&gt;\n  filter(OccupGroup == \"C\" & Age &gt; 50) |&gt; \n  nrow()\n\n[1] 6\n\n\n\n\n\n\n\n\nFigure¬†9.14: Artwork explaining the dplyr::filter() function by @allison_horst.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#saving-and-exporting-r-objects",
    "href": "9_DataWrangling.html#saving-and-exporting-r-objects",
    "title": "9¬† Data wRangling",
    "section": "9.9 Saving and exporting R objects",
    "text": "9.9 Saving and exporting R objects\nAs a final step, we want to save the R object combined.data to a local file on our computer so that, when we continue our analyses in a new R session, we can immediately start working with the wrangled dataset. We can either save the wrangled dataset as an R object (.rds) or export it as a DSV file (e.g.¬†.csv, see Section 2.5.1). The pros and cons of the two solutions are summarised in Table¬†9.1.\n\n\n\nTable¬†9.1: Pros and cons of saving DSV and R data files.\n\n\n\n\n\n\n\n\n\nDSV files (e.g., .csv, .tsv, .tab)\nR data files (.rds)\n\n\n\n\n‚úÖ Highly portable (i.e., can be opened in all standard spreadsheet software and text editors).\n‚ùå Specific to R and cannot be opened in standard spreadsheet software or text editors.\n\n\n‚ùå Inefficient for very large datasets.\n‚úÖ Efficient memory usage for more compact data storage and faster loading times in R.\n\n\n‚úÖ Universal, language-independent format and therefore suitable for long-term archiving.\n‚ùå No guarantee that older .rds files will be compatible with newer versions of R and therefore unsuitable for long-term archiving.\n\n\n‚ùå Loss of metadata.\n‚úÖ Preserve R data structures (e.g., factor variables remain stored as factors).\n\n\n\n\n\n\nWe will save both a .csv and an .rds version of the wrangled data but in the following chapters, we will use the .rds file.\nWe will save both files to a subfolder of our project ‚Äúdata‚Äù folder called ‚Äúprocessed‚Äù. If we try to save the file to this subfolder before it has been created at this location we get an error message.\n\nsaveRDS(combined.data, file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n\nError in gzfile(file, mode) : cannot open the connection\nWe first need to create the ‚Äúprocessed‚Äù subfolder before we can save to this location! There are two ways of doing this:\n\nEither in the Files pane of RStudio or in a File Navigator/Explorer window, navigate to the ‚Äúdata‚Äù folder and, from there, click on the ‚ÄúCreate a new folder‚Äù icon to create a new subfolder called ‚Äúprocessed‚Äù.\nAlternatively, we can use the dir.create() function to create the subfolder from R itself. If the folder already exists at this location, we will get a warning.\n\n\ndir.create(file.path(here(\"data\", \"processed\")))\n\nNow that the subfolder exists, we can save combined.data as an .rds file. We will work with this file in the following chapters.\n\nsaveRDS(combined.data, file = here(\"data\", \"processed\", \"combined_L1_L2_data.rds\"))\n\nIf you want to share your wrangled dataset with a colleague who does not (yet? üòâ) use R, you can use the tidyverse function write_csv().6 Your colleague will be able to open this file in any standard spreadsheet programme or text editor (but do warn them about the dangers of opening .csv file in spreadsheets, see Section 2.6!).\n\nwrite_csv(combined.data, file = here(\"data\", \"processed\", \"combined_L1_L2_data.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#check-your-progress",
    "href": "9_DataWrangling.html#check-your-progress",
    "title": "9¬† Data wRangling",
    "section": "Check your progress üåü",
    "text": "Check your progress üåü\nYou have successfully completed 0 out of 15 quiz questions in this chapter.\nThat was a lot of data wrangling, but we are now ready to proceed with some comparative analyses of L1 and L2 English speakers‚Äô language skills!\nAre you confident that you can‚Ä¶?\n\nDefine tidy data\nCheck the sanity of a dataset\nConvert character vectors representing categorical data to factors\nAdd and replace columns in a table\nTransform several columns at once\nUse {stringr} functions to manipulate text values\nInterpret R package cheatsheets\nGain insights from the help file of R functions\nUse tidyverse functions to pre-process data in an readable and reproducible way\nSave R objects as .rds and .csv files on your computer.\n\nThe following chapter stays in the tidyverse as we will learn how to use the popular tidyverse package {ggplot2} to visualise the pre-processed data from DƒÖbrowska (2019).\n\n\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences in linguistic attainment: A comparison of native and nonnative speakers. Language Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nHorst, Allison & Julie Lowndes. 2020. Openscapes - tidy data for efficiency, reproducibility, and collaboration. https://openscapes.org/blog/2020-10-12-tidy-data/.\n\n\nWickham, Hadley, Davis Vaughan & Maximilian Girlich. Tidy messy data. https://tidyr.tidyverse.org/.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "9_DataWrangling.html#footnotes",
    "href": "9_DataWrangling.html#footnotes",
    "title": "9¬† Data wRangling",
    "section": "",
    "text": "Note that, throughout this chapter, long code output is shortened to save space. When you run this command on your own computer, however, you will see that the output is much longer that what is reprinted in this chapter. You will likely need to scroll up in your Console window to view it all.‚Ü©Ô∏é\nThe equivalent base R function is toupper().‚Ü©Ô∏é\nNote that this a {tidyverse} approach to working out percentages, see Section 8.1.3 for a base R approach.‚Ü©Ô∏é\nWe could also use the full_join() function since we want to retain all rows and all columns from both datasets.‚Ü©Ô∏é\nAlternatively, you may have gathered from the cheatsheet (Figure¬†9.10) that the bind_rows() function has an optional ‚Äú.id‚Äù argument that can be used to create an additional column to disambiguate between the two combined datasets. In this case, we do not need to add a Group column to both datasets prior to combining them.\n\ncombined.data &lt;- bind_rows(L1 = L1.data, \n                           L2 = L2.data, \n                           .id = \"Group\")\n\n‚Ü©Ô∏é\nAs usual, there is a base R alternative: write.csv() will work just as well but, for larger datasets, it is considerably slower than write_csv(). For finer differences, check out the functions‚Äô respective help files.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Data w`R`angling</span>"
    ]
  },
  {
    "objectID": "99_references.html",
    "href": "99_references.html",
    "title": "References",
    "section": "",
    "text": "2011. IRIS. https://iris-database.org/.\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu & Mark Ziemann.\n2021. Gene name errors: Lessons not learned. PLOS Computational\nBiology. Public 17(7). e1008984. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nAlhazmi, Fahd. 2020. A visual interpretation of the standard deviation.\nMedium. https://towardsdatascience.com/a-visual-interpretation-of-the-standard-deviation-30f4676c291c.\n\n\nalvinashcraft, alexbuckgit, ArcticLampyrid & bearmannl. 2022.\nMaximum path length limitation. Learn Microsoft. https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation.\n\n\nBarrett, Malcolm. 2018. Why should i use the here package when i‚Äôm\nalready using projects? - malcolm barrett. https://malco.io/articles/2018-11-05-why-should-i-use-the-here-package-when-i-m-already-using-projects.\n\n\nBerez-Kroeker, Andrea L., Bradley McDonnell, Eve Koller & Lauren B.\nCollister. 2022. The open handbook of linguistic data\nmanagement. MIT Press. https://doi.org/10.7551/mitpress/12200.001.0001.\n\n\nBryan, Jennifer. 2018. Let‚Äôs git started | happy git and GitHub for\nthe useR. Open Education Resource. https://happygitwithr.com/.\n\n\nBryan, Jenny. 2017. Project-oriented workflow. Tidyverse.org.\nhttps://www.tidyverse.org/blog/2017/12/workflow-vs-script/.\n\n\nBusterud, Guro, Anne Dahl, Dave Kush & Kjersti Faldet Listhaug.\n2023. Verb placement in L3 french and L3 german: The role of\nlanguage-internal factors in determining cross-linguistic influence from\nprior languages. Linguistic Approaches to Bilingualism. John\n13(5). 693‚Äì716. https://doi.org/10.1075/lab.22058.bus.\n\n\nDƒÖbrowska, Ewa. 2019. Experience, aptitude, and individual differences\nin linguistic attainment: A comparison of native and nonnative speakers.\nLanguage Learning 69(S1). 72‚Äì100. https://doi.org/10.1111/lang.12323.\n\n\nDauber, Daniel. 2024. R for non-programmers: A guide for social\nscientists. Open Education Resource. https://bookdown.org/daniel_dauber_io/r4np_book/.\n\n\nDouglas, Alex, Deon Roos, Francesca Mancini & David Lusseau. 2024.\nAn introduction to R. https://intro2r.com/.\n\n\nEkman, Paul & Wallace V Friesen. 1978. Facial action coding system.\nEnvironmental Psychology & Nonverbal Behavior.\n\n\nFricke, Lea, Patrick G Grosz & Tatjana Scheffler. 2024. Semantic\ndifferences in visually similar face emojis. Language and\nCognition. Cambridge University Press 1‚Äì15. https://doi.org/10.1017/langcog.2024.12.\n\n\nFugate, Jennifer MB & Courtny L Franco. 2021. Implications for\nemotion: Using anatomically based facial coding to compare emoji faces\nacross platforms. Frontiers in Psychology. Frontiers Media SA\n12. 605928. https://doi.org/10.3389/fpsyg.2021.605928.\n\n\nGries, Stefan Th. & Nick C. Ellis. 2015. Statistical measures for\nusage-based linguistics. Language Learning 65(S1). 228‚Äì255. https://doi.org/10.1111/lang.12119.\n\n\nHorst, Allison & Julie Lowndes. 2020. Openscapes - tidy data for\nefficiency, reproducibility, and collaboration. https://openscapes.org/blog/2020-10-12-tidy-data/.\n\n\nLausberg, Hedda & Han Sloetjes. 2009. Coding gestural behavior with\nthe NEUROGES-ELAN system. Behavior Research Methods 41(3).\n841‚Äì849. https://doi.org/10.3758/BRM.41.3.841.\n\n\nLe Foll, Elen. 2022. Textbook English: A\ncorpus-based analysis of the language of EFL textbooks used in secondary\nschools in France, Germany and\nSpain. Osnabr√ºck University PhD thesis. https://doi.org/10.48693/278.\n\n\nMaier, Emar. 2023. Emojis as pictures. Ergo 10. https://doi.org/10.3998/ergo.4641.\n\n\nNeuwirth, Erich. 2022. Package ‚ÄúRColorBrewer.‚Äù\nColorBrewer palettes 991. https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf.\n\n\nParsons, Sam, Fl√°vio Azevedo, Mahmoud M. Elsherif, Samuel Guay, Owen N.\nShahim, Gisela H. Govaart, Emma Norris, et al. 2022. A community-sourced\nglossary of open scholarship terms. Nature Human\nBehaviour. Nature 6(3). 312‚Äì318. https://doi.org/10.1038/s41562-021-01269-4.\n\n\nPedersen, Thomas Lin. 2024. Patchwork: The composer of plots.\nhttps://patchwork.data-imaginist.com.\n\n\nPedersen, Thomas Lin & Maxim Shemanarev. 2024. Ragg: Graphic\ndevices based on AGG. https://ragg.r-lib.org.\n\n\nPfadenhauer, Katrin & Evelyn Wiesinger (eds.). 2024. Romance\nmotion verbs in language change: Grammar, lexicon, discourse. De\nGruyter. https://doi.org/10.1515/9783111248141.\n\n\nPfeifer, Valeria A, Emma L Armstrong & Vicky Tzuyin Lai. 2022. Do\nall facial emojis communicate emotion? The impact of facial emojis on\nperceived sender emotion and text processing. Computers in Human\nBehavior. Elsevier 126. 107016. https://doi.org/10.1016/j.chb.2021.107016.\n\n\nPrat, Chantel S., Tara M. Madhyastha, Malayka J. Mottarella &\nChu-Hsuan Kuo. 2020. Relating natural language aptitude to individual\ndifferences in learning programming languages. Scientific\nReports. Nature 10(1). 3817. https://doi.org/10.1038/s41598-020-60661-8.\n\n\nR Core Team. 2024. R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nScheffler, Tatjana & Ivan Nenchev. 2024. Affective, semantic,\nfrequency, and descriptive norms for 107 face emojis. Behavior\nResearch Methods. Springer 1‚Äì22. https://doi.org/10.3758/s13428-024-02444-x.\n\n\nSchimke, Sarah, Israel de la Fuente, Barbara Hemforth & Saveria\nColonna. 2018. First language influence on second language offline and\nonline ambiguous pronoun resolution. Language Learning 68(3).\n744‚Äì779. https://doi.org/10.1111/lang.12293.\n\n\nSchweinberger, Martin. 2022. Data management, version control, and\nreproducibility. https://ladal.edu.au/repro.html.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen‚Äôs complete\nnovels. https://CRAN.R-project.org/package=janeaustenr.\n\n\nStefanowitsch, Anatol & Susanne Flach. 2017. The corpus-based\nperspective on entrenchment. In Hans-J√∂rg Schmid (ed.), Entrenchment\nand the psychology of language learning: How we reorganize and adapt\nlinguistic knowledge, 101‚Äì127. De Gruyter. https://doi.org/10.1037/15969-006.\n\n\nThe Turing Way Community. 2022. The turing way: A handbook for\nreproducible, ethical and collaborative research (1.0.2). Zenodo. https://doi.org/10.5281/zenodo.3233853.\n\n\nVan Hulle, Sven & Renata Enghels. 2024a. The category of throw verbs\nas productive source of the spanish inchoative construction. In Katrin\nPfadenhauer & Evelyn Wiesinger (eds.), Romance motion verbs in\nlanguage change, 213‚Äì240. De Gruyter. https://doi.org/10.1515/9783111248141-009.\n\n\nVan Hulle, Sven & Renata Enghels. 2024b. TROLLing replication data\nfor: ‚ÄúThe category of throw verbs as productive source of the\nspanish inchoative construction. DataverseNO, V1.‚Äù https://doi.org/10.18710/TR2PWJ.\n\n\nWickham, Hadley, Romain Fran√ßois & Lucy D‚ÄôAgostino McGowan. 2024.\nEmo: Easily insert ‚Äôemoji‚Äô. https://github.com/hadley/emo.\n\n\nWickham, Hadley, Davis Vaughan & Maximilian Girlich. Tidy messy\ndata. https://tidyr.tidyverse.org/.\n\n\nWinter, Bodo. 2019. Statistics for linguists: An introduction using\nR. Routledge. https://doi.org/10.4324/9781315165547.\n\n\nZiemann, Mark, Yotam Eren & Assam El-Osta. 2016. Gene name errors\nare widespread in the scientific literature. Genome Biology\n17(1). 177. https://doi.org/10.1186/s13059-016-1044-7.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "CS_RoseGina.html",
    "href": "CS_RoseGina.html",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "",
    "text": "Chapter overview\nThis case-study chapter will guide you through the steps to reproduce selected results from a published experimental linguistics study (Fricke, Grosz & Scheffler 2024) using R.\nThe chapter will walk you through how to:\nWe will work with the original raw data from:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_RoseGina.html#sec-introducing-the-study",
    "href": "CS_RoseGina.html#sec-introducing-the-study",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "10.1 Introducing the study üôÇ",
    "text": "10.1 Introducing the study üôÇ\nFace emojis are frequently used in text messages. They represent facial expressions and often make fundamental contributions to the subtext of a text message. A few studies have investigated the relationship between emojis and the emotions that they depict (Fugate & Franco 2021; Maier 2023; Pfeifer, Armstrong & Lai 2022). However, as emojis are a relatively recent phenomenon, there is still a lot to be discovered. In this chapter, we will look into a study by Fricke, Grosz & Scheffler (2024).\n\n10.1.1 Deconstructing emojis into Action Units\nFricke, Grosz & Scheffler (2024) compared ‚Äúvisually similar face emojis‚Äù using an annotation system developed by Fugate & Franco (2021). This system is based on the Facial Action Coding System (FACS) for human faces (Ekman & Friesen 1978). Fricke, Grosz & Scheffler (2024) assigned numbers to human-like facial features such as eyebrows arched and eyes wide. These numbers are called Action Units, short AUs. As you can see in Figure¬†10.1, each emoji consists of several AUs.\n\n\n\n\n\n\nFigure¬†10.1: Emoji pairs and their AU codes (from Fricke, Grosz & Scheffler 2024: 5, CC-BY)\n\n\n\nFricke, Grosz & Scheffler (2024) defined two different types of emoji pairs: In the AU+ condition, the pairs of emojis are similar, but are assigned a different set of AUs. The emoji pairs in the AU- condition are also similar, but their AUs are identical. AUs capture facial expressions by numbers and, as such, can assist linguists to accurately describe emojis. However, only expressions that can be consciously changed by humans receive number-labels: For example, the AU difference between üòÉ and üòÜ captures the fact that the former emoji has open eyes while the latter has closed eyes. Since humans can choose whether to open or close their eyes, this is an AU+ pair. If the subtle difference between emojis is not manipulable, as in üòÑ and üòÅ, the emojis are described by identical numbers (AU-).\n\n\n10.1.2 The experiment\n\n\n\n\n\n\nHow did the experiment work?\n\n\n\nThree AU+ and three AU- emoji pairs were created (see Figure¬†10.1). Each pair was assigned two contexts, with each context corresponding to the prominent usage of one emoji, but not the other. For example, the contexts of the first pair are happiness and (cheeky) laughter. The contexts were assigned based on https://emojipedia.org and a previous norming study (Scheffler & Nenchev 2024).\nFour single-sentence narratives were created for each of the contexts (see Figure¬†10.2, translated from German below (translation Fricke, Grosz & Scheffler 2024: 6)).\n\n\nAlex writes to his best friend Stefan:\nI just learned that my cousin‚Äôs dog has his own advent calendar.\nAlex is amused. Which of the emojis matches the message better? üòÑüòÅ\n\nAlex writes to his best friend Stefan:\nI just learned that I won 500 Euro in the lottery.\nAlex is overjoyed. Which of the emojis matches the message better? üòÑüòÅ\n\n\n\n\n\n\n\n\nFigure¬†10.2: Example of a test item in the experiment (from Fricke, Grosz & Scheffler 2024: 6, CC-BY)\n\n\n\nThese short narratives were divided up into into four experimental lists of 12 items. Each list also contained 12 filler items, so that each participant saw 24 items. The participants were then asked to help choose the emoji that best matched the context. Each participant saw each emoji pair twice. It was measured how often participants chose the context-matching emoji versus the non-matching emoji.\n\n\nFricke, Grosz & Scheffler (2024)‚Äôs central research question was: Do AU differences lead to differences in meaning between the two emojis of a pair? In line with the pictorial approach by Maier (2023) the authors predicted that visual differences between emojis which correspond to human facial features (AU+) would be more semantically relevant than those that do not (AU-).\n\n\n\n\n\n\nQuiz time!\n\n\n\nRead the abstract of the study:\n\nFricke, L., Grosz, P. G., & Scheffler, T. (2024). Semantic differences in visually similar face emojis. Language and Cognition, 1‚Äì15. https://doi.org/10.1017/langcog.2024.12\n\nQ1. According to the abstract, what were the results of Fricke, Grosz & Scheffler (2024)‚Äôs experiment?\n\n\nThere were no significant differences between the two conditions.\n\n\nFor both types of pairs, the context-matching emoji was preferred over the non-matching one.\n\n\nParticipants chose the context-matching emoji more often in the AU- condition.\n\n\nParticipants chose the context-matching emoji more often in the AU+ condition.\n\n\n\n\n\n¬†\nQ2. The actual results of the experiment were different from what Fricke, Grosz & Scheffler (2024) had expected. According to author‚Äôs research hypothesis (visual differences between emojis which correspond to human facial features are more semantically relevant than those that do not), which of these experimental results were expected?\n\n\nParticipants will choose the context-matching emoji more often in the AU- condition.\n\n\nFor both types of pairs, the context-matching emoji will be preferred over the non-matching one.\n\n\nParticipants will choose the context-matching emoji more often in the AU+ condition.\n\n\nFor the AU- pairs, the pattern will be more random.\n\n\nFor the AU+ pairs, the pattern will be more random.\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_RoseGina.html#sec-gender-understanding",
    "href": "CS_RoseGina.html#sec-gender-understanding",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "10.2 Exploring the relationship between gender and emoji understanding",
    "text": "10.2 Exploring the relationship between gender and emoji understanding\nFricke, Grosz & Scheffler (2024) asked participants about their gender, their attitude towards emojis, how often they use emojis on WhatsApp and how well they think they understand emojis. The authors visualised the distribution of men and women for emoji use and emoji attitude as barplots.\n\n\n\n\n\n\n\n\n\n\n\n(a) Emoji use by gender\n\n\n\n\n\n\n\n\n\n\n\n(b) Attitude towards emojis by gender\n\n\n\n\n\n\n\nFigure¬†10.3: Barplots from Fricke, Grosz & Scheffler (2024: 9-10, CC-BY)\n\n\n\nThe plots in Figure¬†10.3 show that women use emojis more often and have a more positive attitude towards emojis than men. We want to find out whether women also reported a higher level of emoji understanding than men. Our analysis will involve three steps:\n\nCalculating the frequencies of the genders in the data\n\nCalculating the frequencies of the different levels of emoji understanding for each gender\n\nVisualising the frequencies in a barplot similar to the plots above.\n\n\n10.2.1 ImpoRting the data\nFricke, Grosz & Scheffler (2024) have made their data and analysis code publicly available on the OSF repository (see Section 1.1). You can access these materials at https://osf.io/k2t9p/. There, the data is stored in the file raw_data.csv. To follow the steps of this chapter, you will need to download this file.\n\n\n\n\n\n\nSession set-up\n\n\n\nTo run the code of this chapter, you will need the following packages. Make sure that they are installed and loaded before starting.\n\nlibrary(here)\nlibrary(tidyverse)\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n#install.packages(\"ragg\")\nlibrary(ragg)\n\n\n\nWe import the authors‚Äô raw data using the read.csv() and here() functions. You will need to adjust the file path to match the folder structure of your computer (see Section 6.5).\n\nraw_data &lt;- read.csv(file = here(\"data\", \"raw_data.csv\"))\n\nAs specified by Fricke, Grosz & Scheffler (2024: 8), we filter out participants who exceed the maximum age of 35 years for all following analyses. We do this by using the filter() function and store the result in a new data frame called df.\n\ndf &lt;- raw_data |&gt; \n  filter(age &lt;= 35)\n\n\n\n10.2.2 Gender frequency analysis\nLet‚Äôs first get a general overview: How many men, women, and non-binary people participated in the study?\nThe relevant variable in the data set is called gender. However, you will see that the names of the gender groups are in German. To figure out what the labels of the different gender groups are, we use the count() function:\n\ndf |&gt; \n  count(gender)\n\n    gender    n\n1   divers   72\n2 m√§nnlich 2616\n3 weiblich 1128\n\n\nBefore we start analysing, we should translate the labels (levels) of the categories into English. Using a combination of mutate() and recode(), we translate m√§nnlich to men, weiblich to women, and divers to non-binary.1\n\ndf &lt;- df |&gt; \n  mutate(gender = recode(gender, \n                         \"m√§nnlich\" = \"men\", \n                         \"weiblich\" = \"women\", \n                         \"divers\" = \"non-binary\"))\n\ndf |&gt; \n  count(gender)\n\n      gender    n\n1        men 2616\n2 non-binary   72\n3      women 1128\n\n\nNow that gender variable have English labels, we want to determine how many male, female, and non-binary subjects participated. We have used the table() function, which determines the number of occurrences of the different genders in the data. But in this case counting the occurrences is not straightforward. The data frame contains 24 rows for each subject, as each participant saw 24 items (see Section 10.1.2). So, if we were to simply count the occurrences of men, women, and non-binary in the data with count(), we would end up with 24 times the values of the frequencies.\nTo determine the actual gender distribution, we need to count the occurrences according to the subjects‚Äô unique IDs. To do this, we apply the distinct() function to keep only unique occurrences (to be precise, the first unique occurrence) of each submission_id. The argument .keep_all is set to TRUE, which means that all other variables in the data frame are kept and not deleted.\n\ndf |&gt; \n  distinct(submission_id, .keep_all = TRUE) |&gt; \n  count(gender)\n\n      gender   n\n1        men 109\n2 non-binary   3\n3      women  47\n\n\nThe mode (see Section 8.1.3) of the gender variable in the dataset is men, as you can see from the output. The gender distribution is very uneven: 109 men, 47 women, and 3 non-binary people participated in the study. If we are not careful, this imbalance can lead to misleading data visualisations.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ3. Which of these problems are likely to occur if we plot emoji understanding by gender in a barplot with unequal gender groups?\n\n\nIt may appear as if men have better emoji understanding simply because of their group size.\n\n\nIt may appear as if non-binary people have lower emoji understanding simply because of their group size.\n\n\nWe will not be able to use ggplot to visualise the data.\n\n\nThe differences in emoji understanding between gender groups may look bigger or smaller than they actually are.\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nTo solve this problem, we will use the same strategy as Fricke, Grosz & Scheffler (2024). We will use relative rather than absolute frequencies to make sure that the numbers for the different genders are comparable. This means that we will calculate the percentages of emoji understanding within each gender group, treating the total number of male, female, and non-binary participants separately as 100%, rather than counting all subjects together as 100%. In this way, we can see, for example, what percentage of men, women, and non-binary participants reported a very good emoji understanding and compare the numbers across groups.\n\n\n10.2.3 How well do the different genders understand emojis?\nNext, we calculate the relative frequencies of the different levels of emoji understanding for each gender.\nThe variable we are interested in is called emoji_understanding. Just like with gender, we first have to do some data wrangling. We use the count() function to get the labels:\n\ndf |&gt; \n  count(emoji_understanding)\n\n  emoji_understanding    n\n1            eher gut  912\n2                 gut 1344\n3         mittelm√§√üig   48\n4            sehr gut 1512\n\n\nWe translate mittelm√§√üig to moderate, eher gut to rather good, gut to good, and sehr gut to very good:\n\ndf &lt;- df |&gt; \n    mutate(emoji_understanding = recode(emoji_understanding,\n                                        \"mittelm√§√üig\" = \"moderate\",\n                                        \"eher gut\" = \"rather good\",\n                                        \"gut\" = \"good\",\n                                        \"sehr gut\" = \"very good\"))\ndf |&gt; \n  count(emoji_understanding)\n\n  emoji_understanding    n\n1                good 1344\n2            moderate   48\n3         rather good  912\n4           very good 1512\n\n\nThe levels are still in the wrong order. We need to rearrange them in an ascending order from moderate to very good. To do this, we define a vector c(\"moderate\", \"rather good\", \"good\", \"very good\"). Using the factor() function, we encode this vector as a factor:\n\ndf &lt;- df |&gt; \n    mutate(emoji_understanding = factor(emoji_understanding,\n                                        levels = c(\"moderate\",\n                                                   \"rather good\",\n                                                   \"good\",\n                                                   \"very good\")))\n\ndf |&gt; \n  count(emoji_understanding)\n\n  emoji_understanding    n\n1            moderate   48\n2         rather good  912\n3                good 1344\n4           very good 1512\n\n\nThe levels now look good, so we can determine the frequencies for the different gender groups within emoji_understanding. We could do this by simply cross-tabulating gender with emoji understanding (see Section 8.1.3). But since we know that the sizes of the gender subsets are very unequal, we also want to calculate the relative frequencies to make the numbers comparable. There is an easy way to calculate relative frequencies using the proportions() function (see Section 8.2.1). However, we need to make two additional considerations:\n\nOur aim is to calculate proportions within groups and not across the whole data.\nWe want to create a comprehensive visualisation that includes both groups of men and women in a single barplot.\n\nTo achieve both, we have to first group our data, using the powerful combination of group_by() and count(). We create a new data frame gender_understanding_count and again keep only each participant‚Äôs unique submission_id as above. We group the data by gender and count the frequencies for the different genders within the emoji_understanding factor:\n\ndf |&gt;\n  distinct(submission_id, .keep_all = TRUE) |&gt;\n  group_by(gender) |&gt; \n  count(gender, emoji_understanding)\n\n# A tibble: 10 √ó 3\n# Groups:   gender [3]\n   gender     emoji_understanding     n\n   &lt;chr&gt;      &lt;fct&gt;               &lt;int&gt;\n 1 men        moderate                1\n 2 men        rather good            25\n 3 men        good                   41\n 4 men        very good              42\n 5 non-binary rather good             1\n 6 non-binary good                    2\n 7 women      moderate                1\n 8 women      rather good            12\n 9 women      good                   13\n10 women      very good              21\n\n\nIn this table, n was calculated by the count() function and represents the number of occurrences for each combination of gender and emoji_understanding. Next, we use mutate() to add a column with the relative frequencies, which we calculate with the formula proportions(n) * 100 to obtain percentages.\n\ngender_understanding_count &lt;- df |&gt;\n  distinct(submission_id, .keep_all = TRUE) |&gt;\n  group_by(gender) |&gt; \n  count(gender, emoji_understanding) |&gt; \n  mutate(percentage = proportions(n) * 100)\n\ngender_understanding_count\n\n# A tibble: 10 √ó 4\n# Groups:   gender [3]\n   gender     emoji_understanding     n percentage\n   &lt;chr&gt;      &lt;fct&gt;               &lt;int&gt;      &lt;dbl&gt;\n 1 men        moderate                1      0.917\n 2 men        rather good            25     22.9  \n 3 men        good                   41     37.6  \n 4 men        very good              42     38.5  \n 5 non-binary rather good             1     33.3  \n 6 non-binary good                    2     66.7  \n 7 women      moderate                1      2.13 \n 8 women      rather good            12     25.5  \n 9 women      good                   13     27.7  \n10 women      very good              21     44.7  \n\n\nThis tabular presentation of the data already shows us that non-binary participants reported either a rather good or good understanding of emojis. A higher percentage of women (44.7%) reported a very good emoji understanding compared to men (38.5%). But let‚Äôs create our barplot to see the distribution more clearly.\n\n\n10.2.4 Data visualisation üìä\nAs mentioned above, we will visualise the relative rather than the absolute frequencies to make sure that the numbers for the different genders are comparable. In line with Fricke, Grosz & Scheffler (2024: 9), we also exclude the three non-binary participants. To this end, we use the filter() function combined with the != operator (see Section 5.5).\n\ngender_understanding_count &lt;- gender_understanding_count |&gt; \n  filter(gender != \"non-binary\")\n\nWe use ggplot() to create a barplot with the emoji_understanding categories on the x-axis and the relative frequencies that we calculated on the y-axis. The bars are coloured according to gender. We also add a title and axis labels. Finally, we remove the white space between the bottom of the bars with an additional scale_y_continuous(expand = c(0,0)) layer and change the colours to make our plot look nicer. The hexadecimal color values chosen here are from the colour-blind friendly palette ‚ÄúSet2‚Äù from the package {RColorBrewer}(Neuwirth 2022). Since we only need two colours, we chose to insert them manually to avoid having to install an additional package.\n\nggplot(gender_understanding_count, \n       aes(x = emoji_understanding, \n           y = percentage, \n           fill = gender)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_y_continuous(expand = c(0,0)) +\n  labs(title = \"Self-reported Emoji Understanding by Gender\",\n       x = \"Emoji understanding\",\n       y = \"Percent\") +\n  scale_fill_manual(values = c(\"#8DA0CB\", \"#FC8D62\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\nAs you can see from the barplot, the gender distribution for emoji understanding is much more even than for emoji use and emoji attitude (see Figure¬†10.3).\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ4. How do you interpret this plot?\n\n\nProportionally more women than men reported a very good emoji understanding.\n\n\nProportionally more women than men reported a moderate emoji understanding.\n\n\nAround half of all participants reported a rather good understanding of emojis.\n\n\nWomen reported a lower level of emoji understanding than men.\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nWhen comparing our barplot to Figure¬†10.3, it is interesting to note that, whilst women reported more frequent use of emojis and a more positive attitude towards emojis, they did not report a higher understanding of emojis. It is possible that some women were more modest in rating their understanding of emojis than men, which could indicate a gender confidence gap. Reporting a good understanding likely requires more confidence compared to emoji use or attitude towards emoji.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_RoseGina.html#sec-au-plot",
    "href": "CS_RoseGina.html#sec-au-plot",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "10.3 Comparing matching rates between AU conditions",
    "text": "10.3 Comparing matching rates between AU conditions\nWe will now turn to exploring the central research question of Fricke, Grosz & Scheffler (2024): Do AU differences lead to differences in meaning between the two emojis of a pair? As explained in Section 10.1.1, AUs are numbers which correspond to human-like facial features. In emoji pairs of the AU+ condition, the visual difference between the emojis is reflected in a number difference, e.g.¬†grinning face with big eyes üòÉ (AU: 5 + 12 + 25 + 26) and grinning squinting face üòÜ (AU: 12 + 25 + 26 + 43). In the AU- condition, the visual difference does not correspond to an AU difference, e.g.¬†grinning face with smiling eyes üòÑ and beaming face with smiling eyes üòÅ both have the same AU (12 + 25 + 26 + 63).\nStep by step, we will build an informative plot which will include all the information needed to answer this question. This plot will display how many times each emoji was chosen in its presumed corresponding context.\nTo achieve this, we need to create a variable that tells us when each participant responded with the matching emoji. In the following, we will create this variable based on the raw data from Fricke, Grosz & Scheffler (2024).\n\n10.3.1 Preprocessing the data\nFirst, we need a variable that includes the experimental conditions of each trial. The variable name tells us whether a trial consisted of an emoji pair with an AU difference (AU+) or not (AU-), or a filler. Trials with AU+ differences include ‚ÄúAU‚Äù in the trial name, those with no AU difference begin with ‚ÄúN‚Äù, and the fillers with ‚Äúfiller‚Äù.\n\ndf |&gt; \n  distinct(name)\n\n\n\n              name\n1     filler-party\n2         AU-23-L1\n3   filler-heizung\n4         AU-40-L1\n5  filler-schlafen\n6     filler-krank\n7          N-04-L1\n8   filler-friseur\n9          N-37-L1\n10   filler-zombie\n\n\nWe use a combination of mutate(), case_when() and str_detect() to construct a new variable (AU_difference) that captures the type of trial that we are dealing with. The command essentially says: look for the string ‚ÄúAU‚Äù in the column name, and in all cases where you find it (case_when()), insert the value ‚ÄúAU+‚Äù in a new column called AU_difference. We follow this procedure for the other trial conditions, too. If neither ‚ÄúAU‚Äù, ‚ÄúN‚Äù or ‚Äúfiller‚Äù is detected, nothing (NULL) is inserted in AU_difference.\n\ndf &lt;- df |&gt; \n  mutate(AU_difference = case_when(str_detect(name, \"AU\") ~ \"AU+\",\n                                   str_detect(name, \"N\") ~ \"AU-\",\n                                   str_detect(name, \"filler\") ~ \"filler\",\n                                   .default = NULL))\n\nWe use select() to compare the two columns and check that everything worked.\n\ndf |&gt; \n  slice(1:10) |&gt; \n  select(name, AU_difference)\n\n              name AU_difference\n1     filler-party        filler\n2         AU-23-L1           AU+\n3   filler-heizung        filler\n4         AU-40-L1           AU+\n5  filler-schlafen        filler\n6     filler-krank        filler\n7          N-04-L1           AU-\n8   filler-friseur        filler\n9          N-37-L1           AU-\n10   filler-zombie        filler\n\n\nThis looks promising. Since we are only interested in the experimental items, we now filter out all filler trials.\n\ndf &lt;- df |&gt;\n  filter(AU_difference != \"filler\")\n\nWe will now create another variable called context. The column of this variable will contain the context descriptions used by Fricke, Grosz & Scheffler (2024: 5) in Figure¬†10.1. Again, we combine mutate(), case_when() and str_detect(): In the question column, we look for context-characteristic strings, and add the context descriptions whenever we have a match. Again, we check the output with table().\n\ndf &lt;- df |&gt; \n  mutate(context = case_when(str_detect(question, \"freut sich\") ~ \"happiness\",\n                             str_detect(question, \"lacht\") ~ \"(cheeky) laughter\",\n\n                             str_detect(question, \"macht sich Sorgen\") ~ \"concern\",\n                             str_detect(question, \"ist √ºberrascht\") ~ \"surprise\",\n                             str_detect(question, \"ist etwas genervt\") ~ \"mild irritation\",\n                             str_detect(question, \"√§rgert sich\") ~ \"annoyance\",\n                             str_detect(question, \"am√ºsiert sich\") ~ \"amusement\",\n                             str_detect(question, \"ist √ºbergl√ºcklich\") ~ \"(intense) happiness\",\n                             str_detect(question, \"ist entt√§uscht\") ~ \"mild disappointment\",\n                             str_detect(question, \"ist entt√§uscht\") ~ \"moderate disappointment\",\n                             str_detect(question, \"ist gut gelaunt\") ~ \"happiness2\",\n                             str_detect(question, \"ist verlegen\") ~ \"bashfulness\",\n                                   .default = NULL))\n\n\ntable(df$context)\n\n\n  (cheeky) laughter (intense) happiness           amusement           annoyance \n                159                 159                 159                 159 \n        bashfulness             concern           happiness          happiness2 \n                159                 159                 159                 159 \nmild disappointment     mild irritation            surprise \n                318                 159                 159 \n\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ5. Which problems become apparent when checking the content of our new context variable using the table() function?\n\n\nThere are more contexts in the output than we coded for.\n\n\nThe context descriptions were not correctly assigned in the case of matching strings.\n\n\nThere are too many occurrences of matches per context than can reasonably be assumed.\n\n\nAll contexts have 159 occurrences, except for mild disappointment which occurs 318 times.\n\n\nThere are fewer contexts in the output than we coded for\n\n\n\n\n\n\n\nThe contexts mild disappointment and moderate disappointment have created some issues: In the question variable, both are described as ist entt√§uscht (‚Äòis disappointed‚Äô). Except for their encoding in the name variable, these contexts appear to be identical. At this point, we have no choice but to look for additional disambiguating information in Fricke, Grosz & Scheffler (2024)‚Äôs analysis script, which you can access at https://osf.io/k8dtp. The relevant information can be found in lines 522 and 523 (see Figure¬†10.4).\n\n\n\n\n\n\nFigure¬†10.4: Screenshot from the authors‚Äô analysis script available at https://osf.io/k8dtp\n\n\n\nThe emoji üôÅ (mild disappointment) is coded as N-36 and ‚òπÔ∏è (moderate disappointment) as N-37. We use this information to assign these two contexts to our context variable.\n\ndf &lt;- df |&gt; \n  mutate(context = case_when(\n                             str_detect(name, \"N-36\") ~ \"mild disappointment\",\n                             str_detect(name, \"N-37\") ~ \"moderate disappointment\",\n                                   .default = context))\n\ntable(df$context)\n\n\n      (cheeky) laughter     (intense) happiness               amusement \n                    159                     159                     159 \n              annoyance             bashfulness                 concern \n                    159                     159                     159 \n              happiness              happiness2     mild disappointment \n                    159                     159                     159 \n        mild irritation moderate disappointment                surprise \n                    159                     159                     159 \n\n\nFinally, we add the critical variable that describes whether there is a match between the chosen emojis and the contexts: if the emoji and the context agree, the variable will have the value match. Otherwise, the value will be no match.\n\ndf &lt;- df |&gt; \n  mutate(\n  match = case_when(\n    context == \"happiness\" & response == \"grinning_face_with_big_eyes\" ~ \"match\",\n    context == \"(cheeky) laughter\" & response == \"grinning_squinting_face\" ~ \"match\",\n    context == \"concern\" & response == \"hushed_face\" ~ \"match\",\n    context == \"surprise\" & response == \"astonished_face\" ~ \"match\",\n    context == \"mild irritation\" & response == \"neutral_face\" ~ \"match\",\n    context == \"annoyance\" & response == \"expressionless_face\" ~ \"match\",\n    context == \"amusement\" & response == \"grinning_face_with_smiling_eyes\" ~ \"match\",\n    context == \"(intense) happiness\" & response == \"beaming_face_with_smiling_eyes\" ~ \"match\",\n    context == \"mild disappointment\" & response == \"slightly_frowning_face\" ~ \"match\",\n    context == \"moderate disappointment\" & response == \"frowning_face\" ~ \"match\",\n    context == \"happiness2\" & response == \"smiling_face_with_smiling_eyes\" ~ \"match\",\n    context == \"bashfulness\" & response == \"smiling_face\" ~ \"match\",\n    .default = \"no match\"))\n\n\n\n10.3.2 Building the plots\nWe will now build our plots to visualise the matching rates per emoji pair. In a new table called data_AU, we group the data by contexts. The command count(match) counts matches and non-matches for each context and stores them in a new column called n. We add the column percent to store the rounded percentage of matches and non-matches for each context-pair.\n\ndata_AU &lt;- df |&gt; \n  group_by(context) |&gt; \n  count(match) |&gt;\n  mutate(percent = round(proportions(n)*100, 2))\n\nUsing the View() function, we take a look at our data.\n\n\n\n\n\n\nFigure¬†10.5: The first 14 columns of the data frame data_AU as visualised using the View() function in RStudio\n\n\n\nWe plot the first emoji pair of the AU+ condition üòØ üò≤ with their respective contexts concern and surprise.\n\nplot_concern_surprise &lt;- data_AU |&gt; \n  filter(context == \"concern\" | context == \"surprise\") |&gt;\n  ggplot(aes(x = context, y = percent, fill = match)) +\n  geom_col() +\n  scale_x_discrete(limits = c(\"concern\", \"surprise\")) +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_fill_manual(values = c(\"#66C2A5\", \"#E78AC3\")) +\n  geom_text(aes(label = percent), position = position_stack(vjust = 0.5)) +\n  labs(title = \"üòØ üò≤\", x = \"\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 20), \n        legend.title=element_blank())\n\nThe code above creates a barplot and stores it in plot_concern_surprise. Here‚Äôs what each line of code does:\n\nBegin by assigning a clear name for the plot and call the data to be plotted.\nFilter the contexts, such that only rows of the contexts concern or (|) surprise are plotted.\nCreate a ggplot object with the context values on the x-axis and percentages on the y-axis. Colours are filled corresponding to the match values.\nDisplay the data as a barplot. By default, geom_bar() counts how many times match and no match occur. However, as we have already calculated and stored the values in the column percent, we use geom_col() to be able to use the data as is.\nThe context values, which are displayed on the x-axis, are discrete. With this command, we set and order the contexts.\nUse the ‚Äúexpand‚Äù argument of the scale_y_continuous() function to remove the white space between the bars and the x-axis.\nAdjust colours with values from ‚ÄúSet2‚Äù from the {RColorBrewer} package (see Section 10.2.4).\nAnnotate the percentages of matching rates by adding them as text and placing them inside the plot, in the middle of the corresponding bars.\nAdd the corresponding emojis at the top of the plot and remove the superfluous x-axis label ‚Äúcontext‚Äù.\nAdd a theme, in this case theme_classic().\nPlots are left-aligned by default. Since we want the emojis to be displayed on top of their corresponding context bars, we move the title to the centre of the plot. To ensure that the emojis are easily interpretable, we also increase the font size to 20 points.\nFinally, we remove the title of the legend because the match and no match values are self-explanatory.\n\nLet‚Äôs take a look at our plot. It‚Äôs looking great, but we don‚Äôt need just one plot, we need six: one for each emoji pair. We could write it all out for each emoji pair, but since the code is identical (except for the contexts and the emojis), it is much more efficient to define a function to do this.\n\nplot_concern_surprise\n\n\n\n\n\n\n\n\nDefining our own functions\n\n\n\nFunctions are reusable code snippets that perform specific tasks. So far, we have only used built-in R functions (see Section 7.4) and functions from add-on packages such {dplyr} from the tidyverse (see Section 9.1), but we have not defined our own functions.\nDefining our own functions can help us make our code more efficient and organised. As a rule of thumb, whenever writing new code seems redundant (i.e., when you find yourself copying and pasting entire sections of code), it is best to define a function for that task. This is will ensure that the task is always performed in the same way and, if you find that you need to amend the code to perform the task, you will only need to make the change once, within the function assigned to this task.\nThe basic structure of a function is function(argument). Looks familiar? Accordingly, we define a function the following way: function(parameters){function body}\nHere are the steps:\n\nWe define a function using the keyword function. After this keyword, we write a list of parameters in brackets. Parameters act as placeholders for the function‚Äôs arguments.\nWe then write code in the function body and enclose it in curly brackets. The function body tells the function what it is meant to do when called upon.\nWe assign our function a name using the assignment operator (&lt;-). This name will be used to call up the function. To avoid conflicts (see Section 9.2), we choose a name that is not already assigned to a built-in function.\n\n\n\nIn our case, the process of defining a plotting function is straightforward:\n\nWe start with the keyword function() and state that our function should take contexts as its first argument and emojis as its second argument, as only these change with each plot.\nWe then simply paste the code that we just wrote for the above barplot inside the curly braces, replacing the specific contexts and emojis with the parameters of our function.\nWe name our function plot_AU_matches to make clear what it does: it plots AU matches.\n\n\nplot_AU_matches &lt;- function(contexts, emojis) {\n  data_AU |&gt; \n    filter(context %in% contexts) |&gt; \n    ggplot(aes(x = context, y = percent, fill = match)) +\n    geom_col() +\n    scale_x_discrete(limits = contexts) +\n    scale_y_continuous(expand = c(0,0)) +\n    scale_fill_manual(values = c(\"#66C2A5\", \"#E78AC3\")) +\n    geom_text(aes(label = percent), position = position_stack(vjust = 0.5)) +\n    labs (x=\"\", y = \"percent\", title = emojis) +\n    theme_classic() +\n    theme(plot.title = element_text(hjust = 0.5, size = 20), \n          legend.title=element_blank())\n}\n\nWe the apply this function to all contexts and emoji pairs by filling them in as the arguments.\n\nplot_concern_surprise &lt;- plot_AU_matches(c(\"concern\", \"surprise\"), \"üòØ üò≤\")\nplot_happiness_cheeky &lt;- plot_AU_matches(c(\"happiness\", \"(cheeky) laughter\"), \"üòÉ üòÜ\")\nplot_mild_irr_annoyance &lt;- plot_AU_matches(c(\"mild irritation\", \"annoyance\"), \"üòê üòë\")\nplot_mild_disapp_mod_dissap &lt;- plot_AU_matches(c(\"mild disappointment\", \"moderate disappointment\"), \"üôÅÔ∏è ‚òπÔ∏è\")\nplot_amusement_int_happiness &lt;- plot_AU_matches(c(\"amusement\", \"(intense) happiness\"), \"üòÑ üòÅ\")\nplot_happiness2_bashfulness &lt;- plot_AU_matches(c(\"happiness2\", \"bashfulness\"), \"üòä ‚ò∫Ô∏è\")\n\n\n\n\n\n\n\nHow to insert emojis in R and render plots with emojis üò∞\n\n\n\n\n\nThere are various ways to insert emojis in R. The easiest is to use the emoji keyboard (see Figure¬†10.6 (a)). To open it on MacOS, use the keyboard shortcut Crtl + ‚åò Cmd + Space or üåê fn + e and on Windows ‚äû Win + . (period). The emoji keyboard is also available in RStudio, if you go to the ‚ÄúEdit‚Äù drop-down menu and click on ‚ÄúEmojis & Symbols‚Äù. Alternatively, there are emoji libraries for R, for example {emo(ji)} (Wickham, Fran√ßois & D‚ÄôAgostino McGowan 2024).\nAs we want to display emojis within plots, we need to pay even more attention to graphics. Emojis as part of plots created by ggplot cannot be displayed by default. Additional problems can occur when rendering a Quarto or RMarkdown document to HTML.\nIf displaying emojis as part of plots in RStudio does not work for you, you will need to use the high-quality graphics library ‚ÄúAGG‚Äù (‚ÄúAnti-Grain Geometry‚Äù) or ‚ÄúCairo‚Äù as your graphics backend in RStudio. To do this, head to the ‚ÄúTools‚Äù drop-down menu and click on ‚ÄúGlobal Options‚Äù. Then, go to the ‚ÄúGraphics‚Äù tab and select the ‚ÄúAGG‚Äù or ‚ÄúCairo‚Äù option (see Figure¬†10.6 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) The emoji keyboard\n\n\n\n\n\n\n\n\n\n\n\n(b) Recommended graphics backend in RStudio\n\n\n\n\n\n\n\nFigure¬†10.6: Tools for inserting and displaying emojis in RStudio\n\n\n\nIf you are using AGG as your graphics backend, you can use the {ragg} package (Pedersen & Shemanarev 2024) to correctly render your Quarto/RMarkdown document to HTML with all the emojis in the plots. This package provides graphic devices based on AGG and includes advanced text rendering, with support for emojis. To use {ragg} in combinination with the {knitr} engine, first install the package and then add the following setup command at the beginning of your document:\n\ninstall.packages(\"ragg\")\n\nknitr::opts_chunk$set(dev = \"ragg_png\")\n\nIf AGG does not work for you, you can use Cairo. Cairo comes preinstalled with R so you don‚Äôt need to install it yourself. The set-up command for your Quarto/RMarkdown document is:\n\nknitr::opts_chunk$set(dev.args = list(png = list(type = \"cairo\")))\n\n\n\n\n\n\n10.3.3 Assembling plots with {patchwork}\nBy applying our newly created function plot_AU_matches() to all emoji pairs and contexts, we have created one barplot for each emoji pair. We will now use the {patchwork} package (Pedersen 2024) to assemble the plots into one figure. As the name suggests, {patchwork} enables us to patch several plots together and arrange them as we wish. The basic operator to combine plots in {patchwork} is the + operator. Additionally, plots can be combined:\n\nHorizontally using | and\nVertically using /.\n\nBrackets can be used to combine horizontal and vertical arrangements.\n\n\n\n\n\n\nFigure¬†10.7: Patchwork artwork by Allison Horst (CC-BY)\n\n\n\nIn line with the research question of Fricke, Grosz & Scheffler (2024), we want to compare the matching rates of emojis and contexts in the AU+ condition with the matching rates in the AU- condition. Our goal is therefore to create a plot that looks similar to Figure¬†10.8 created by Fricke, Grosz & Scheffler (2024).\n\n\n\n\n\n\nFigure¬†10.8: Plot of individual emoji pairs that compares AU conditions (Fricke, Grosz & Scheffler 2024: 11, CC-BY)\n\n\n\nFirst, let us plan the layout of our combined plot with some placeholder names. Our combined plot will have two columns and three rows: In both column1 and column2, three plots are stacked vertically on top of each other. These are the plots of the AU+ and the AU- condition, respectively. We then place these patchworks next to each other (horizontally) for comparison.\ncolumn1 &lt;- p1 / p2 / p3\n\ncolumn2 &lt;- p4 / p5 / p6\n\ncolumns_combined &lt;- column1 | column2\nWe follow the logic above to create our combined plot, choosing informative names for our subplots. To run this code, you will need to have the {patchwork} package installed and the library loaded.\n\n#install.packages(\"patchwork\")\n#library(patchwork)\n\n#AU+ condition:\nAU_plus_patch &lt;-\n  plot_concern_surprise / plot_happiness_cheeky / plot_mild_irr_annoyance\n\n#AU- condition:\nAU_minus_patch &lt;-\n  plot_mild_disapp_mod_dissap / plot_amusement_int_happiness / plot_happiness2_bashfulness\n\nTo further specify the layout, we use the plot_layout() function from {patchwork}. By setting the ‚Äúguide‚Äù argument to ‚Äúcollect‚Äù, legends that are identical within each patchwork are merged into one. We place the legends at the bottom of each subplot.\n\n#AU+ condition:\nAU_plus_patch &lt;- AU_plus_patch +\n  plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n#AU- condition:\nAU_minus_patch &lt;- AU_minus_patch +\n  plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\nSince AU_plus_patch and AU_minus_patch are to be combined in one plot, we need to add titles to keep them apart. Technically, it is possible (and recommended!) to use the plot_annotation() function of the {patchwork} package for this. However, annotations made with this function are only shown at the highest nesting level. As we will be building a double-nested plot, any annotations we do on the ‚Äúblocks-of-three‚Äù-level will not be displayed. We can work around this by using the function wrap_elements(). This fixates the blocks in their current position and allows us to add titles using ggtitles() instead.\n\nAU_plus_patch &lt;- wrap_elements(plot = AU_plus_patch) +\n  ggtitle(\"[AU+] condition\")\n\nAU_minus_patch &lt;- wrap_elements(plot = AU_minus_patch) +\n  ggtitle(\"[AU-] condition\")\n\nFinally, we put both columns together to get our final plot.\n\nAU_plus_patch | AU_minus_patch\n\n\n\n\n\n\n\nFigure¬†10.9: Plot combining all six plots into one figure\n\n\n\nFigure¬†10.9 contains information on the matching rates of all emoji pairs with their contexts. It contains the same information as Figure¬†10.8 from Fricke, Grosz & Scheffler (2024), but does not look exactly the same. Which look do you think is easiest to interpret?\n\n\n\n\n\n\nAlternative ways of dealing with the legend\n\n\n\n\n\nYou probably will have noticed that Figure¬†10.9 contains two identical legends. This is the trade-off we take by using the wrap_elements() function: We have fixated the patchworks in their state with their legends, which means that the legends cannot be merged later. There are a couple of other options that will produce different outcomes, however, none is going to be perfect: Since we do not want to delete the legend completely, one option would be to keep the legends of all six plots, as in Figure¬†10.10 (a). Another option would be to keep the legend of one block and delete the other. However, as you can see in Figure¬†10.10 (b), this makes the bars take up the space of the legend, and the bars in one block become wider than in the other one.\n\n\n\n\n\n\n\n\n\n\n\n(a) The combined plot with six legends\n\n\n\n\n\n\n\n\n\n\n\n(b) The combined plot with one legend\n\n\n\n\n\n\n\nFigure¬†10.10: Options for the legend placement\n\n\n\n\n\n\n\n\n10.3.4 Interpreting the plot\nBy looking and interpreting Figure¬†10.9, we can now finally answer the research question: Do AU differences lead to differences in meaning between the two emojis of a pair?\nBased on the descriptive statistics visualised in Figure¬†10.9, the answer is no, seemingly not. The AU difference does not seem to be critical when deciding which emoji to use in a specific context. The original study also concluded that the matching emoji was ‚Äúgenerally preferred with matching rates above chance level‚Äù (Fricke, Grosz & Scheffler 2024: 11), both in the AU+ and in the AU- condition. Now, was all this work for nothing?\nNo, not at all! We can still draw some interesting inferences from the plot we created. For example, we see that minor visual differences between emojis do appear to affect the understanding and selection of emojis in different contexts: By slightly varying the contexts, participants were made to choose emojis with different facial features. Notably, matching rates were quite similar within emoji pairs. In particular, this was the case for the contexts happiness üòÉ - (cheeky) laughter üòÜ and happiness2 üòä - bashfulness ‚ò∫Ô∏è : there was less than a 2% difference between the matching rates. Hence, there is much more to explore in future linguistics studies on the semantics of emoji in text messages!\nThe following quiz questions are about the interpretation of Figure¬†10.9. The questions should help you make sense of the information displayed.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ6. In which context pair did participants choose the matching emojis most often?\n\n\nmild disappointment and moderate disappointment\n\n\nhappiness2 and bashfulness \n\n\nconcern and surprise\n\n\nmild irritation and annoyance\n\n\nhappiness and (cheeky) laughter\n\n\namusement and (intense) happiness\n\n\n\n\n\n Q7. How can a small difference (such as &lt; 2%) between matching rates within pairs be interpreted?\n\n\nWithin these pairs, a very similar number of participants chose the matching emoji.\n\n\nWithin these pairs, half the participants preferred one emoji and half preferred the other.\n\n\nWithin these pairs, participants had the most difficulty choosing one emoji over the other.\n\n\nWithin these pairs, the least number of participants chose the matching emoji.\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n Q8. Looking at the final patchwork (Figure¬†10.9), which subplot stands out the most?\n\n\nlower right\n\n\nlower left\n\n\nupper left\n\n\nmiddle right\n\n\nmiddle left\n\n\nupper right\n\n\n\n\n\n Q9. Which interpretations of the lower left plot are correct? Select all that apply.\n\n\nThe disparate matching rates of the üòêüòë pair prove that, in general, AU differences affect participants' preferences.\n\n\nThere is a major difference between the matching rates of the annoyance and happiness contexts.\n\n\nThe emoji üòê was selected for its matching context considerably less often than all the other emojis.\n\n\nOnly in the üòêüòë pair does the matching rate for one emoji exceed chance level, while the matching rate for the other falls below chance.\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n Q10. What are plausible reasons for the striking results presented in the lower left barplot? Select all that apply.\n\n\nThe participants did not realize there was a difference between üòê and üòë.\n\n\nThe stories created for mild irritation and annoyance triggered a similar reaction.\n\n\nThe stories of the mild irritation context were perceived as more annoying than the authors had anticipated.\n\n\nUnlike what was anticipated, most participants used the emojis üòê and üòë in very different contexts.\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nOverall, Figure¬†10.9 shows that there was indeed a preference for context-matching emojis. However, the findings do not support the pictorial approach adopted by Fricke, Grosz & Scheffler (2024): Whether or not emoji features coincided with human facial features, did not (significantly) affect the participants‚Äô decision for one emoji or the other.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_RoseGina.html#sec-conclusion",
    "href": "CS_RoseGina.html#sec-conclusion",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "10.4 Conclusion",
    "text": "10.4 Conclusion\nYou have successfully completed 0 out of 10 quiz questions in this chapter.\nYou are now a pro in handling (stacked) barplots! You can build, customise, arrange, and interpret them. Barplots are powerful for visualising categorical data, offering a straightforward way to compare frequencies and make patterns apparent. However, they do have their limitations. For instance, they are not ideal for displaying continuous data. Building and assembling plots can be quite fiddly and it can take some trial-and-error to make the plot look like what you had imagined. But there is a solution for (almost) everything and hopefully, the beautiful plot you create in the process will be worth the effort.\nThis chapter‚Äôs analysis revealed gender-specific differences in emoji understanding, potentially indicating a gender confidence gap between men and women. On average, however, both genders, reported at least a good understanding of emojis. The visualisations have been adjusted for the gender imbalance in the data, demonstrating the importance of accounting for differences in group sizes.\nIn this chapter, we have created an informative figure that answers the experiment‚Äôs research question by combining multiple plots. The question whether Action Unit (AU) differences are critical for emoji preference was answered in the negative. However, we have made several other discoveries along the way: As noted by Fricke, Grosz & Scheffler (2024), we have found that small changes of emojis‚Äô facial features do affect choice patterns.\nEmojis, it turns out, contain lots of information, and there is a science behind them ü§ì. While experimentally measuring why we prefer certain emojis over other ones represents a real challenge, Fricke, Grosz & Scheffler (2024) provides valuable insights into this fascinating area of study. As the authors shared their data and code, we were able to successfully reproduce their results, as well as create new informative figures on the basis of their data.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_RoseGina.html#references",
    "href": "CS_RoseGina.html#references",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "References",
    "text": "References\nFricke, Lea, Patrick G Grosz, and Tatjana Scheffler. 2024. Semantic Differences in Visually Similar Face Emojis. Language and Cognition. Cambridge University Press 1‚Äì15. https://doi.org/10.1017/langcog.2024.12.\nFugate, Jennifer MB & Courtny L Franco. 2021. Implications for Emotion: Using Anatomically Based Facial Coding to Compare Emoji Faces Across Platforms. Frontiers in Psychology. Frontiers Media SA 12. 605928. https://doi.org/10.3389/fpsyg.2021.605928.\nMaier, Emar. 2023. Emojis as Pictures. Ergo 10. https://doi.org/10.3998/ergo.4641.\nNeuwirth, Erich. 2022. Package ‚ÄúRColorBrewer‚Äù. ColorBrewer Palettes 991. https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf.\nPedersen, Thomas Lin. 2024. Patchwork: The Composer of Plots. https://patchwork.data-imaginist.com.\nPedersen, Thomas Lin & Maxim Shemanarev. 2024. Ragg: Graphic Devices Based on AGG. https://ragg.r-lib.org.\nPfeifer, Valeria A, Emma L Armstrong & Vicky Tzuyin Lai. 2022. Do all facial emojis communicate emotion? The impact of facial emojis on perceived sender emotion and text processing. Computers in Human Behavior. Elsevier 126. 107016. https://doi.org/10.1016/j.chb.2021.107016.\nScheffler, Tatjana & Ivan Nenchev. 2024. Affective, semantic, frequency, and descriptive norms for 107 face emojis. Behavior Research Methods. Springer 1‚Äì22. https://doi.org/10.3758/s13428-024-02444-x.\nWickham, Hadley, Romain Fran√ßois & Lucy D‚ÄôAgostino McGowan. 2024. Emo: Easily Insert ‚Äôemoji‚Äô. https://github.com/hadley/emo.\n\nPackages used in this chapter\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Brussels\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] knitcitations_1.0.12 ragg_1.3.3           patchwork_1.2.0     \n [4] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n [7] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[10] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[13] tidyverse_2.0.0      here_1.0.1           checkdown_0.0.12    \n[16] webexercises_1.1.0  \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4        generics_0.1.3    xml2_1.3.6        stringi_1.8.4    \n [5] hms_1.1.3         digest_0.6.36     magrittr_2.0.3    evaluate_0.24.0  \n [9] grid_4.4.1        timechange_0.3.0  fastmap_1.2.0     plyr_1.8.9       \n[13] rprojroot_2.0.4   jsonlite_1.8.8    backports_1.5.0   httr_1.4.7       \n[17] fansi_1.0.6       scales_1.3.0      codetools_0.2-20  bibtex_0.5.1     \n[21] textshaping_0.4.0 cli_3.6.3         rlang_1.1.4       commonmark_1.9.1 \n[25] munsell_0.5.1     withr_3.0.1       yaml_2.3.8        tools_4.4.1      \n[29] tzdb_0.4.0        colorspace_2.1-0  vctrs_0.6.5       R6_2.5.1         \n[33] lifecycle_1.0.4   RefManageR_1.4.0  htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[37] pillar_1.9.0      gtable_0.3.5      Rcpp_1.0.12       glue_1.7.0       \n[41] systemfonts_1.1.0 xfun_0.45         tidyselect_1.2.1  rstudioapi_0.16.0\n[45] knitr_1.47        farver_2.1.2      htmltools_0.5.8.1 labeling_0.4.3   \n[49] rmarkdown_2.27    compiler_4.4.1    markdown_1.13    \n\n\n\n\nPackage references\n[1] D. Barr and L. DeBruine. webexercises: Create Interactive Web Exercises in R Markdown (Formerly webex). R package version 1.1.0. 2023. https://github.com/psyteachr/webexercises.\n[2] C. Boettiger. knitcitations: Citations for Knitr Markdown Files. R package version 1.0.12. 2021. https://github.com/cboettig/knitcitations.\n[3] G. Grolemund and H. Wickham. ‚ÄúDates and Times Made Easy with lubridate‚Äù. In: Journal of Statistical Software 40.3 (2011), pp. 1-25. https://www.jstatsoft.org/v40/i03/.\n[4] G. Moroz. checkdown: Check-Fields and Check-Boxes for rmarkdown. R package version 0.0.12. 2023. https://agricolamz.github.io/checkdown/.\n[5] G. Moroz. Create check-fields and check-boxes with checkdown. 2020. https://CRAN.R-project.org/package=checkdown.\n[6] K. M√ºller. here: A Simpler Way to Find Your Files. R package version 1.0.1. 2020. https://here.r-lib.org/.\n[7] K. M√ºller and H. Wickham. tibble: Simple Data Frames. R package version 3.2.1. 2023. https://tibble.tidyverse.org/.\n[8] T. L. Pedersen. patchwork: The Composer of Plots. R package version 1.2.0. 2024. https://patchwork.data-imaginist.com.\n[9] T. L. Pedersen and M. Shemanarev. ragg: Graphic Devices Based on AGG. R package version 1.3.3. 2024. https://ragg.r-lib.org.\n[10] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria, 2024. https://www.R-project.org/.\n[11] V. Spinu, G. Grolemund, and H. Wickham. lubridate: Make Dealing with Dates a Little Easier. R package version 1.9.3. 2023. https://lubridate.tidyverse.org.\n[12] H. Wickham. forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0. 2023. https://forcats.tidyverse.org/.\n[13] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. ISBN: 978-3-319-24277-4. https://ggplot2.tidyverse.org.\n[14] H. Wickham. stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.1. 2023. https://stringr.tidyverse.org.\n[15] H. Wickham. tidyverse: Easily Install and Load the Tidyverse. R package version 2.0.0. 2023. https://tidyverse.tidyverse.org.\n[16] H. Wickham, M. Averick, J. Bryan, et al.¬†‚ÄúWelcome to the tidyverse‚Äù. In: Journal of Open Source Software 4.43 (2019), p.¬†1686. DOI: 10.21105/joss.01686.\n[17] H. Wickham, W. Chang, L. Henry, et al.¬†ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.5.1. 2024. https://ggplot2.tidyverse.org.\n[18] H. Wickham, R. Fran√ßois, L. Henry, et al.¬†dplyr: A Grammar of Data Manipulation. R package version 1.1.4. 2023. https://dplyr.tidyverse.org.\n[19] H. Wickham and L. Henry. purrr: Functional Programming Tools. R package version 1.0.2. 2023. https://purrr.tidyverse.org/.\n[20] H. Wickham, J. Hester, and J. Bryan. readr: Read Rectangular Text Data. R package version 2.1.5. 2024. https://readr.tidyverse.org.\n[21] H. Wickham, D. Vaughan, and M. Girlich. tidyr: Tidy Messy Data. R package version 1.3.1. 2024. https://tidyr.tidyverse.org.\n[22] Y. Xie. Dynamic Documents with R and knitr. 2nd. ISBN 978-1498716963. Boca Raton, Florida: Chapman and Hall/CRC, 2015. https://yihui.org/knitr/.\n[23] Y. Xie. ‚Äúknitr: A Comprehensive Tool for Reproducible Research in R‚Äù. In: Implementing Reproducible Computational Research. Ed. by V. Stodden, F. Leisch and R. D. Peng. ISBN 978-1466561595. Chapman and Hall/CRC, 2014.\n[24] Y. Xie. knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.47. 2024. https://yihui.org/knitr/.\n\n\n\n\nEkman, Paul & Wallace V Friesen. 1978. Facial action coding system. Environmental Psychology & Nonverbal Behavior.\n\n\nFricke, Lea, Patrick G Grosz & Tatjana Scheffler. 2024. Semantic differences in visually similar face emojis. Language and Cognition. Cambridge University Press 1‚Äì15. https://doi.org/10.1017/langcog.2024.12.\n\n\nFugate, Jennifer MB & Courtny L Franco. 2021. Implications for emotion: Using anatomically based facial coding to compare emoji faces across platforms. Frontiers in Psychology. Frontiers Media SA 12. 605928. https://doi.org/10.3389/fpsyg.2021.605928.\n\n\nMaier, Emar. 2023. Emojis as pictures. Ergo 10. https://doi.org/10.3998/ergo.4641.\n\n\nNeuwirth, Erich. 2022. Package ‚ÄúRColorBrewer.‚Äù ColorBrewer palettes 991. https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf.\n\n\nPedersen, Thomas Lin. 2024. Patchwork: The composer of plots. https://patchwork.data-imaginist.com.\n\n\nPedersen, Thomas Lin & Maxim Shemanarev. 2024. Ragg: Graphic devices based on AGG. https://ragg.r-lib.org.\n\n\nPfeifer, Valeria A, Emma L Armstrong & Vicky Tzuyin Lai. 2022. Do all facial emojis communicate emotion? The impact of facial emojis on perceived sender emotion and text processing. Computers in Human Behavior. Elsevier 126. 107016. https://doi.org/10.1016/j.chb.2021.107016.\n\n\nScheffler, Tatjana & Ivan Nenchev. 2024. Affective, semantic, frequency, and descriptive norms for 107 face emojis. Behavior Research Methods. Springer 1‚Äì22. https://doi.org/10.3758/s13428-024-02444-x.\n\n\nWickham, Hadley, Romain Fran√ßois & Lucy D‚ÄôAgostino McGowan. 2024. Emo: Easily insert ‚Äôemoji‚Äô. https://github.com/hadley/emo.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_RoseGina.html#footnotes",
    "href": "CS_RoseGina.html#footnotes",
    "title": "10¬† The semantics of emojis: ExploRing the results of an experimental study",
    "section": "",
    "text": "We decided to translate ‚Äòdivers‚Äô as ‚Äònon-binary‚Äô, as this is the English term that Fricke, Grosz & Scheffler (2024) used in their paper.‚Ü©Ô∏é",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>The semantics of emojis: Explo`R`ing the results of an experimental study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html",
    "href": "CS_Poppy.html",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "",
    "text": "Chapter overview\nThis chapter will guide you through the steps to reproduce the results of a published corpus linguistics study (Van Hulle & Enghels 2024a) using R.\nThe chapter will walk you through how to:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#introducing-the-study",
    "href": "CS_Poppy.html#introducing-the-study",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.1 Introducing the study",
    "text": "11.1 Introducing the study\nIn this chapter, we attempt to reproduce the results of a corpus linguistics study by Van Hulle & Enghels (2024a), published as a book chapter in a volume edited by Pfadenhauer & Wiesinger (2024). The study focuses on the development of five throw verbs in Peninsular Spanish: echar, lanzar, disparar, tirar, and arrojar (Van Hulle & Enghels 2024a). These verbs have evolved into aspectual auxiliaries in inchoative constructions that convey the beginning of an event. Van Hulle & Enghels (2024a) use historical data to trace the evolution of these verbs, and contemporary data to analyse their usage in micro-constructions. Below are examples of the five throw verbs in inchoative constructions (all taken from Van Hulle & Enghels 2024a).\n\nLos nuevos rebeldes se arrojaron a atacar al sistema de control social. (‚ÄòThe new rebels started (lit. ‚Äòthrew/launched themselves‚Äô) to attack the system of social control.‚Äô)\nEl ni√±o abri√≥ los ojos y ech√≥ a correr de regreso a su casa. (‚ÄòThe child opened his eyes and started (lit. ‚Äòthrew‚Äô) to run back to his house.‚Äô)\nEl grupo de investigaci√≥n se lanz√≥ a analizar otros par√°metros. (‚ÄòThe investigation group started (lit. ‚Äòlaunched itself‚Äô) to analyse other parameters.‚Äô)\nDecid√≠ no tirarme a llorar y empec√© a buscar algo que me ayudara. (‚ÄòI decided not to start (lit. ‚Äòthrow myself‚Äô) to cry and I started to look for something that would help me.‚Äô)\nY todos dispararon a correr, sin volver la cabeza atr√°s. (‚ÄòAnd everybody started (lit. ‚Äòshot‚Äô) to run, without looking back.‚Äô)\n\n\n\n\n\n\n\nQuiz time!\n\n\n\nFollow the study‚Äôs DOI link and read the abstract to learn about the study‚Äôs research focus.\n\nVan Hulle, Sven & Renata Enghels. 2024. The category of throw verbs as productive source of the Spanish inchoative construction. In Katrin Pfadenhauer & Evelyn Wiesinger (eds.), Romance motion verbs in language change, 213‚Äì240. De Gruyter. https://doi.org/10.1515/9783111248141-009.\n\nQ1. What is the main focus of this study?\n\n\n\n\nThe relationship between inchoative constructions and nouns of motion.\n\n\nThe role of spatial expressions in the process of grammaticalization.\n\n\nThe development of 'throw' verbs as aspectual auxiliaries.\n\n\nSemantic differences between the five Spanish 'throw' verbs.\n\n\n\n\n\n\n\nQ2. According to the study, what semantic features help explain the connection between ‚Äòthrow‚Äô verbs and inchoative constructions?\n\n\n\n\nThe ability of 'throw' verbs to convey accuracy and control.\n\n\nThe meaning of abruptness and the interruption of inertia.\n\n\nThe shift in meaning from the concrete to the abstract.\n\n\nThe aspect of moving towards a specified destination.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\n\nIn this chapter, we will use the authors‚Äô original data to reproduce Tables 5 and 8 (Van Hulle & Enghels 2024a: 227, 232), as well as visualising the data with a series of informative line plots to facilitate interpretation.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#sec-ThrowData",
    "href": "CS_Poppy.html#sec-ThrowData",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.2 Retrieving the authors‚Äô original data",
    "text": "11.2 Retrieving the authors‚Äô original data\nIn the spirit of Open Science (see Section 1.1), Van Hulle & Enghels (2024a) have made their research data openly accessible on the Troms√∏ Repository of Language and Linguistics (TROLLing):\n\nVan Hulle, Sven & Renata Enghels. 2024. Replication Data for: ‚ÄúThe category of throw verbs as productive source of the Spanish inchoative construction.‚Äù DataverseNO. Version 1. https://doi.org/10.18710/TR2PWJ.\n\nFollow the link and read the description of the dataset. Next, scroll down the page where three different downloadable files are listed.\n\n0_ReadME_Spanish_ThrowVerbs_Inchoatives_20230413.txt This is a text ‚Äúfile which provides general information about the nature of the dataset and how the data was collected and annotated, and brief data-specific information for each file belonging to this dataset‚Äù (Van Hulle & Enghels 2024b).\nSpanish_ThrowVerbs_Inchoatives_20230413.csv This is a comma-separated file (see Section 2.5.1) which ‚Äúcontains the input data for the analysis, including the variables ‚ÄòAUX‚Äô, ‚ÄòCentury‚Äô, ‚ÄòINF‚Äô and ‚ÄòClass‚Äô, for the throw verbs arrojar, disparar, echar, lanzar and tirar‚Äù (Van Hulle & Enghels 2024b).\nSpanish_ThrowVerbs_Inchoatives_queries_20230413.txt ‚ÄúThis file specifies all corpus queries‚Äùthat were used to download the samples per auxiliary from the Spanish Web corpus (esTenTen18), that was accessed via Sketch Engine, and from the Corpus Diacr√≥nico del Espa√±ol (CORDE)‚Äù (Van Hulle & Enghels 2024b).\n\nIn corpus linguistics, it is often the case that corpora cannot be openly shared for copyright and/or data protection reasons. Instead, authors who strive to make their work transparent and reproducible can share details of the corpora that they analysed and of the specific corpus queries they used, so that the data that they share are only the results of the queries.\nAs we are interested in the frequencies retrieved from the corpora, we download the CSV file Spanish_ThrowVerbs_Inchoatives_20230413.csv.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ3. Where did the data for Van Hulle & Enghels (2024b)‚Äòs study on the five Spanish ‚Äôthrow‚Äô verbs come from?\n\n\n\n\nFor the contemporary data, from the Spanish corpus of the Troms√∏ Repository of Language and Linguistics.\n\n\nFor all occurrences of 'throw' verbs, from the European Spanish Web Corpus (esTenTen18).\n\n\nFor the contemporary data, from the European Spanish subcorpus of the Spanish Web Corpus (esTenTen18).\n\n\nFor the historical data, from the Corpus Diacr√≥nico del Espa√±ol (CORDE).\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n\nQ4. What does the term ‚Äúfalse positive‚Äù refer to in the context of this study?\n\n\n\n\nTokens that represent infinitives following 'a' with an incorrect auxiliary.\n\n\nTokens removed due to being irrelevant to the study's focus on 'throw' verbs.\n\n\nTags that incorrectly label nouns as infinitives or misidentify inchoative constructions.\n\n\nTokens that were correctly identified as inchoative constructions.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#sec-ImportingVanHulle",
    "href": "CS_Poppy.html#sec-ImportingVanHulle",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.3 ImpoRting the authors‚Äô original data",
    "text": "11.3 ImpoRting the authors‚Äô original data\nBefore we can import the dataset, we need to load all the packages that we will need for this project. Note that you may need to install some of these packages first (see Section 4.4.2 for instructions).\n\n# Loading required packages for this project\nlibrary(here)\nlibrary(tidyverse)\nlibrary(xfun)\n\nNext, we import the dataset containing the number of occurrences of ‚Äòthrow‚Äô verbs in the corpora analysed in Van Hulle & Enghels (2024a) (Spanish_ThrowVerbs_Inchoatives_20230413.csv) as a new object called spanish.data. You will need to adjust the file path to match the folder structure of your computer (see Section 6.5).\n\n# Importing the Spanish verbs dataset\nspanish.data &lt;- read.csv(file = here(\"data\", \"Spanish_ThrowVerbs_Inchoatives_20230413.csv\"),\n                    header = TRUE,\n                    sep = \"\\t\",\n                    quote = \"\\\"\",\n                    dec = \".\")\n\nWe check the sanity of the imported data by visually examining the output of View(spanish.data) (Figure¬†11.1).\n\n\n\n\n\n\nFigure¬†11.1: Screenshot showing part of the dataset using the View() function.\n\n\n\nAs you can see in Figure¬†11.1, the dataset contains 2882 rows (i.e., the number of occurrences of ‚Äòthrow‚Äô verbs observed in the corpora) and 4 columns (i.e., variables describing these observations).\nThe readme file delivered with the data (0_ReadME_Spanish_ThrowVerbs_Inchoatives_20230413.txt) describes the variables as follows:\n-----------------------------------------\nDATA-SPECIFIC INFORMATION FOR: Spanish_ThrowVerbs_Inchoatives_20230413.csv\n-----------------------------------------\n\n#   Variable    Explanation\n\n1   AUX         This column contains the inchoative auxiliary. [...]\n\n2   Century     This column contains the century to which the concrete example belongs. \n\n3   INF         This column contains the infinitive observed in the filler slot of the inchoative construction. \n\n4   Class       This column contains the semantical class to which the infinitive belongs, based on the classification of ADESSE. This lexical classification classifies Spanish verbs in semantic groups, which we adopted for the annotation (http://adesse.uvigo.es/data) (@ref Garc√≠a-Miguel & Albertuz 2005). [...]\n\n¬†\nTo obtain a list of all the ‚Äòthrow‚Äô verbs included in the dataset and their total frequencies, we use the familiar count() function from {dyplr} (see Chapter 9).\n\nspanish.data |&gt; \n  count(AUX)\n\n       AUX    n\n1  arrojar  160\n2 disparar    8\n3    echar 1936\n4   lanzar  680\n5    tirar   98\n\n\nAs you can see, there are five ‚Äòthrow‚Äô verbs in the dataset: echar, lanzar, tirar, arrojar, and disparar. The most frequent one is echar.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ5. Which column in the dataset contains the general meaning of the verbs in the filler slot of the inchoative construction?\n\n\n\n\nAUX\n\n\nINF\n\n\nCentury\n\n\nClass\n\n\n\n\n\n\n\nQ6. Which of the following verbs is classified under the semantic category ‚ÄòDesplazamiento‚Äô (‚Äòmovement‚Äô)?\n\n\n\n\ndormir\n\n\natacar\n\n\nllevar\n\n\nhacer\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#token-absolute-frequency",
    "href": "CS_Poppy.html#token-absolute-frequency",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.4 Token (absolute) frequency",
    "text": "11.4 Token (absolute) frequency\nAccording to Gries & Ellis (2015: 232):\n\n‚ÄúToken frequency counts how often a particular form appears in the input.‚Äù\n\nIn Van Hulle & Enghels (2024a), token frequency refers to the number of occurrences of combinations of ‚Äòthrow‚Äô verbs and infinitives in inchoative constructions, as identified in the corpora queried for this study (see Section 11.2).\n\n11.4.1 Creating a table of token frequencies\nFirst of all, we want to find out how often each ‚Äòthrow‚Äô verb was observed in each century. To do so, we use the count() function to output the number of corpus occurrences for all possible combinations of the AUX and Century variables. Then, we pipe this output into an arrange() command to order the rows of the table by the values of the Century and AUX columns (as shown in Table¬†11.1 below), prioritising the order of the Century over the alphabetical order of the AUX. This ensures that the centuries are ordered correctly from the 13th to the 21st century, rather than being jumbled. We store this summary table (see Table¬†11.1) as a new R object called verbs.investigated.\n\nverbs.investigated &lt;- spanish.data |&gt;\n  count(AUX, Century, sort = TRUE) |&gt; \n  arrange(Century, AUX)\n\nTable¬†11.1 contains 26 rows and three columns, AUX, Century, both from the original dataset, and n which contains the number of occurrences for each combination of the Centuryand AUX variables. For example, the verb echar occurs 32, 15, and 101 times in the corpus data from the 13th, 14th, and 15th centuries respectively and so on.\n\n\n\n\nTable¬†11.1: Frequency of each Spanish ‚Äòthrow‚Äô verb in each century\n\n\n\n\n\n\nAUX\nCentury\nn\n\n\n\n\nechar\n13\n32\n\n\nechar\n14\n15\n\n\nechar\n15\n101\n\n\ntirar\n15\n2\n\n\narrojar\n16\n20\n\n\nechar\n16\n153\n\n\narrojar\n17\n47\n\n\ndisparar\n17\n3\n\n\nechar\n17\n95\n\n\narrojar\n18\n16\n\n\nechar\n18\n40\n\n\ntirar\n18\n8\n\n\narrojar\n19\n38\n\n\ndisparar\n19\n1\n\n\nechar\n19\n500\n\n\nlanzar\n19\n55\n\n\narrojar\n20\n11\n\n\ndisparar\n20\n1\n\n\nechar\n20\n500\n\n\nlanzar\n20\n125\n\n\ntirar\n20\n7\n\n\narrojar\n21\n28\n\n\ndisparar\n21\n3\n\n\nechar\n21\n500\n\n\nlanzar\n21\n500\n\n\ntirar\n21\n81\n\n\n\n\n\n\n\n\nWe will now attempt to reproduce ‚ÄúTable 4: General overview of the dataset‚Äù (Van Hulle & Enghels 2024a: 225), reprinted below as Table¬†11.2.\n\n\n\n\nTable¬†11.2: Absolute token frequency of Spanish ‚Äòthrow‚Äô verbs as reported in van Hulle & Enghels (2024a: 225)\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\nTotal\n\n\n\n\narrojar\n0\n0\n0\n20\n47\n16\n38\n11\n28\n160\n\n\ndisparar\n0\n0\n0\n0\n3\n0\n1\n1\n3\n8\n\n\nechar\n32\n15\n101\n153\n95\n40\n500\n500\n500\n1936\n\n\nlanzar\n0\n0\n0\n0\n0\n0\n55\n125\n500\n680\n\n\ntirar\n0\n0\n0\n0\n0\n0\n0\n7\n81\n88\n\n\nTotal\n32\n15\n101\n173\n145\n56\n594\n644\n1112\n2872\n\n\n\n\n\n\n\n\nTo reproduce this table on the basis of the data provided by the authors, we begin by reshaping the data frame spanish.data from long format to wide format using the pivot_wider() function (see Section 9.7). This function takes the arguments ‚Äúnames_from‚Äù to specify which column is to provide the names for the output columns, and ‚Äúvalues_from‚Äù to determine which column is to supply the cell values.\n\nverbs.investigated |&gt;\n  pivot_wider(names_from = Century, values_from = n) \n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 echar       32    15   101   153    95    40   500   500   500\n2 tirar       NA    NA     2    NA    NA     8    NA     7    81\n3 arrojar     NA    NA    NA    20    47    16    38    11    28\n4 disparar    NA    NA    NA    NA     3    NA     1     1     3\n5 lanzar      NA    NA    NA    NA    NA    NA    55   125   500\n\n\nAs you can see, this table includes a lot of NA values for the verbs for which zero occurrences were found in certain centuries. To replace missing values (NA) with a different value (here 0 to match Table¬†11.2), we can use the tidyverse function replace_na() in combination with mutate(). By applying this operation across(everything()), we ensure that the modifications are performed on all columns. We pipe this output into the arrange() function to order the rows of the table by the alphabetical order of the AUX. This is important as we will later use token.data to calculate the type/token frequency (see Table¬†11.11) for which we will merge token.data with the types.wide, which is also arranged by the alphabetical order of the AUX.\n\ntoken.data &lt;- verbs.investigated |&gt;\n  pivot_wider(names_from = Century, values_from = n) |&gt; \n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX) \n\ntoken.data\n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 arrojar      0     0     0    20    47    16    38    11    28\n2 disparar     0     0     0     0     3     0     1     1     3\n3 echar       32    15   101   153    95    40   500   500   500\n4 lanzar       0     0     0     0     0     0    55   125   500\n5 tirar        0     0     2     0     0     8     0     7    81\n\n\nWe now want to add the total number of verb occurrences in each row and column of our table, as in (Van Hulle & Enghels 2024a, Table 4) (see also Table¬†11.2). We begin by calculating the total number of occurrences of each verb in token.data. We therefore first select just the columns containing numeric values.\n\nnumeric_columns &lt;- token.data |&gt; \n  select(where(is.numeric))\n\nnumeric_columns\n\n# A tibble: 5 √ó 9\n   `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0    20    47    16    38    11    28\n2     0     0     0     0     3     0     1     1     3\n3    32    15   101   153    95    40   500   500   500\n4     0     0     0     0     0     0    55   125   500\n5     0     0     2     0     0     8     0     7    81\n\n\nIt is important that we specify that we only add the values in columns representing numeric variables because if we ask R to do any mathematical operations with values of the AUX variable, we will get an error message indicating that it is impossible to add up character string values!\n\nsum(token.data$AUX)\n\nError in sum(token.data$AUX) : invalid 'type' (character) of argument\nThis is why we first created an R object that contains only the numeric variables of token.data: these are the columns that we will need to compute our sums. Next, we use the base R function rowSums() to calculate the total number of occurrences of each ‚Äòthrow‚Äô verb across all corpus texts queried, from the 13th to the 21th century.\n\nrow_sums &lt;- rowSums(numeric_columns)\n\nWe have saved the output of the rowSums() function to a new object called row_sums. This object is a numeric vector containing just the row totals.\n\nrow_sums\n\n[1]  160    8 1936  680   98\n\n\nTo check that these are in fact the correct totals, we can compare these row sums to the output of table(spanish.data$AUX) (see also Section 11.3). As the numbers match, we can now use mutate() to add row_sums as a new column to token.data.\n\ntoken.data.rowSums &lt;- token.data |&gt; \n  mutate(Total = row_sums)\n\ntoken.data.rowSums\n\n# A tibble: 5 √ó 11\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21` Total\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 arrojar      0     0     0    20    47    16    38    11    28   160\n2 disparar     0     0     0     0     3     0     1     1     3     8\n3 echar       32    15   101   153    95    40   500   500   500  1936\n4 lanzar       0     0     0     0     0     0    55   125   500   680\n5 tirar        0     0     2     0     0     8     0     7    81    98\n\n\nNow, let‚Äôs turn to the column totals. We can use colSums() to calculate the total number of ‚Äòthrow‚Äô verb occurrences in each century.\n\ncolumn_sums &lt;- colSums(numeric_columns)\n\ncolumn_sums\n\n  13   14   15   16   17   18   19   20   21 \n  32   15  103  173  145   64  594  644 1112 \n\n\nIn the original paper, the row of totals is labelled ‚ÄúTotal‚Äù. Furthermore, we also have a value representing the total number of verbs included in the dataset. Hence, the last row will be constructed as follows using the combine function c().\n\ntotal_row &lt;- c(\"Total\", column_sums, sum(row_sums))\n\ntotal_row\n\n             13      14      15      16      17      18      19      20      21 \n\"Total\"    \"32\"    \"15\"   \"103\"   \"173\"   \"145\"    \"64\"   \"594\"   \"644\"  \"1112\" \n        \n \"2882\" \n\n\nAgain, we can check that we have not ‚Äúlost‚Äù any verbs along the way by comparing the last value of total_row with the number of observations in our original long-format dataset.\n\nnrow(spanish.data)\n\n[1] 2882\n\n\nFinally, we use rbind() to append the total_row vector to token.data, creating a complete table with both row and column totals (Table¬†11.3).\n\n\n\nTable¬†11.3: Absolute token frequency of Spanish ‚Äòthrow‚Äô verbs based on dataset\n\n\ntoken.table.totals &lt;- rbind(token.data.rowSums, total_row)\n\ntoken.table.totals\n\n# A tibble: 6 √ó 11\n  AUX      `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`  Total\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 arrojar  0     0     0     20    47    16    38    11    28    160  \n2 disparar 0     0     0     0     3     0     1     1     3     8    \n3 echar    32    15    101   153   95    40    500   500   500   1936 \n4 lanzar   0     0     0     0     0     0     55    125   500   680  \n5 tirar    0     0     2     0     0     8     0     7     81    98   \n6 Total    32    15    103   173   145   64    594   644   1112  2882 \n\n\n\n\nIf we now compare the values of the table in the published study (reproduced as Table¬†11.2) with Table¬†11.3 based on the authors‚Äô archived data, we can see that the total number of ‚Äòthrow‚Äô verbs is only 2872, suggesting that ten verb occurrences are somehow missing in the summary table printed in the published paper. In the data frame spanish.data, these missing data points correspond to occurrences of the verb tirar, specifically two tokens from the 15th century and eight tokens from the 18th century (Table¬†11.3).\nSince Van Hulle & Enghels (2024a) focus their analyses on the other two verbs, echar and lanzar, this discrepancy is not particularly conspicuous. However, it suggests that the version of the dataset archived on TROLLing is not exactly the same as the one that the authors presumably used for the analyses presented in the 2024 paper.1\n\n\n11.4.2 Visualising the absolute frequencies in a tabular format\nAs Van Hulle & Enghels (Van Hulle & Enghels 2024a: 224) state,\n\n‚ÄúThe searches in the databases of CORDE and esTenTen18 were exhaustive, but, for reasons of feasibility, only the first 500 relevant cases were included in the final dataset.‚Äù\n\nThat is, the corpus contains much more data than what the authors could feasibly investigate. For example, the verb echar appears 799 times in the 19th century texts, 1,641 times in the 20th texts, and 10,347 times in those from the 21st century. However, in their final dataset Van Hulle & Enghels (2024a) included only 500 instances of echar in these centuries, as shown in Table¬†11.2 above.\nTo generate a table that includes the absolute token frequency in the corpus, similar to the ‚Äúabsolute token frequency‚Äù subsection of Table 5 in Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227), we need to modify the values of echar in the 19th, 20th, and 21st centuries, and lanzar in the 21st century in verbs.investigated that we previously created.\nWe use the mutate() function to update specific columns and case_when() to define the conditions of the changes. For example, if the verb echar appears in the AUX variable and at the same time the value 19 is found in the Century variable, then the cell value should be changed to 799, and so on. The formula TRUE ~ n ensures that the original value is retained if no condition is met. The modified table is assigned to a new data frame object, which we name verbs.corpus.\nNext, we generate a contingency table with the altered values for those verbs by applying the pivot_wider() function as in Table¬†11.3 above. The result is displayed in Table¬†11.4 below.\n\nShow the R code to generate the table below.\nverbs.corpus &lt;- verbs.investigated |&gt;\n  # Modifying specific columns with mutate()\n  mutate(n = case_when(\n    AUX == \"echar\" & Century == 19 ~ 799,\n    AUX == \"echar\" & Century == 20 ~ 1641,\n    AUX == \"echar\" & Century == 21 ~ 10347,\n    AUX == \"lanzar\" & Century == 21 ~ 7625,\n    # Keep the original value if no condition is met\n    TRUE ~ n))\n\n# Generating a contingency table with the altered verb values using pivot_wider()\nverbs.corpus.wide &lt;- verbs.corpus |&gt;\n  pivot_wider(names_from = Century, values_from = n) |&gt;\n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX)\n\nverbs.corpus.wide\n\n\n\n\nTable¬†11.4: Absolute token frequency of Spanish ‚Äòthrow‚Äô verbs as observed in the corpus\n\n\n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arrojar      0     0     0    20    47    16    38    11    28\n2 disparar     0     0     0     0     3     0     1     1     3\n3 echar       32    15   101   153    95    40   799  1641 10347\n4 lanzar       0     0     0     0     0     0    55   125  7625\n5 tirar        0     0     2     0     0     8     0     7    81\n\n\n\n\nThe difference between the corpus data (Table¬†11.4) and the final dataset (Table¬†11.3) is found only in echar in 19th, 20th, 21st, and in lanzar in 21st centuries. Following Van Hulle & Enghels (2024a), we will use the frequency of Spanish ‚Äòthrow‚Äô verbs (stored as a data frame named verbs.corpus) observed in the corpus (Table¬†11.4) to calculate the normalized frequency (see Table¬†11.7 below).",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#normalized-frequency",
    "href": "CS_Poppy.html#normalized-frequency",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.5 Normalized frequency",
    "text": "11.5 Normalized frequency\nA normalized frequency is an occurrence rate adjusted to a common base, such as per million words (pmw), to allow comparisons across datasets of different sizes.\nVan Hulle & Enghels (2024a) analyse Spanish corpora from different centuries, using the Corpus Diacr√≥nico del Espa√±ol (CORDE) for the 13th to 20th centuries and the esTenTen18 corpus for the 21st century, accessed via the Sketch Engine platform. To compare frequencies from these varying-sized corpora, we need to normalize them to ensure that large frequencies are not simply due to the corpus being larger. Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227, footnote 2) explain:\n\nThe normalised token frequencies are calculated dividing the absolute token frequency by these total amounts of words, multiplied by 1 million. This number then shows how many times each micro-construction occurs per 1 million words, per century.\n\nThe formula for normalized frequency is as follows:\n\\[\nnormalized frequency = \\frac{token frequency}{total words *1000000}\n\\]\n\n11.5.1 Visualising normalized frequencies in a tabular format\nWe will now attempt to reproduce the ‚ÄúNormalized Token Frequency‚Äù sections of Tables 5 and 8 from the published paper using the authors‚Äô original data. For later comparison, the normalized frequencies as reported in Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227, 232)2 are reproduced in this chapter as Table¬†11.5.\n\n\n\n\nTable¬†11.5: Normalized frequency (pmw) as reported in the published paper (Van Hulle & Enghels 2024a: Tables 5 and 8)\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\narrojar\n-\n-\n-\n0.4\n1.23\n1.11\n0.89\n0.19\n0.01\n\n\ndisparar\n-\n-\n-\n-\n0.08\n-\n0.02\n0.02\n0.0008\n\n\nechar\n4.09\n2\n4.43\n3.07\n2.49\n2.76\n18.7\n27.96\n2.91\n\n\nlanzar\n-\n-\n-\n-\n-\n-\n1.31\n2.15\n2.14\n\n\ntirar\n-\n-\n-\n-\n-\n-\n-\n0.12\n0.02\n\n\n\n\n\n\n\n\nThe sizes of the corpora for each century are provided in Van Hulle & Enghels (Van Hulle & Enghels 2024a: 227, footnote 12)3. We create a table of word counts for each century (Table¬†11.6) using the tibble() function from the tidyverse, by concatenating (using the c() function) the values of Words and Century and storing these as a new data frame named corpus_sizes.\n\ncorpus_sizes &lt;- tibble(Century = c(13, 14, 15, 16, 17, 18, 19, 20, 21),\n                       Words = c(7829566, 7483952, 22796824, 49912675, 38083322, 14466748, 42726881, 58686214, 3554986755))\n\n\n\n\nTable¬†11.6: Total numbers of words in the corpora\n\n\ncorpus_sizes\n\n# A tibble: 9 √ó 2\n  Century      Words\n    &lt;dbl&gt;      &lt;dbl&gt;\n1      13    7829566\n2      14    7483952\n3      15   22796824\n4      16   49912675\n5      17   38083322\n6      18   14466748\n7      19   42726881\n8      20   58686214\n9      21 3554986755\n\n\n\n\nWe will apply this formula to each verb for every century. First, we use the left_join function from {dplyr} to combine two data frames, i.e.¬†verbs.corpus, which we used to create Table¬†11.4, and corpus_sizes, which we just created (Table¬†11.6), based on the common Century column.\n\nleft_join(verbs.corpus, corpus_sizes, by = \"Century\")\n\n        AUX Century     n      Words\n1     echar      13    32    7829566\n2     echar      14    15    7483952\n3     echar      15   101   22796824\n4     tirar      15     2   22796824\n5   arrojar      16    20   49912675\n6     echar      16   153   49912675\n7   arrojar      17    47   38083322\n8  disparar      17     3   38083322\n9     echar      17    95   38083322\n10  arrojar      18    16   14466748\n11    echar      18    40   14466748\n12    tirar      18     8   14466748\n13  arrojar      19    38   42726881\n14 disparar      19     1   42726881\n15    echar      19   799   42726881\n16   lanzar      19    55   42726881\n17  arrojar      20    11   58686214\n18 disparar      20     1   58686214\n19    echar      20  1641   58686214\n20   lanzar      20   125   58686214\n21    tirar      20     7   58686214\n22  arrojar      21    28 3554986755\n23 disparar      21     3 3554986755\n24    echar      21 10347 3554986755\n25   lanzar      21  7625 3554986755\n26    tirar      21    81 3554986755\n\n\nNext, we pipe the combined data frames into a mutate() function to add a new column named normalized and apply the formula (n / Words) * 1000000 to normalize the frequency. In a second step, we round the result to two decimal places.\n\nleft_join(verbs.corpus, corpus_sizes, by = \"Century\") |&gt;\n  mutate(normalized = (n / Words) * 1000000) |&gt;\n  mutate(normalized = round(normalized,\n                            digits = 2))\n\n        AUX Century     n      Words normalized\n1     echar      13    32    7829566       4.09\n2     echar      14    15    7483952       2.00\n3     echar      15   101   22796824       4.43\n4     tirar      15     2   22796824       0.09\n5   arrojar      16    20   49912675       0.40\n6     echar      16   153   49912675       3.07\n7   arrojar      17    47   38083322       1.23\n8  disparar      17     3   38083322       0.08\n9     echar      17    95   38083322       2.49\n10  arrojar      18    16   14466748       1.11\n11    echar      18    40   14466748       2.76\n12    tirar      18     8   14466748       0.55\n13  arrojar      19    38   42726881       0.89\n14 disparar      19     1   42726881       0.02\n15    echar      19   799   42726881      18.70\n16   lanzar      19    55   42726881       1.29\n17  arrojar      20    11   58686214       0.19\n18 disparar      20     1   58686214       0.02\n19    echar      20  1641   58686214      27.96\n20   lanzar      20   125   58686214       2.13\n21    tirar      20     7   58686214       0.12\n22  arrojar      21    28 3554986755       0.01\n23 disparar      21     3 3554986755       0.00\n24    echar      21 10347 3554986755       2.91\n25   lanzar      21  7625 3554986755       2.14\n26    tirar      21    81 3554986755       0.02\n\n\nNext, we remove the n and Words columns that we no longer need here by combining the minus operator - and the select() function to ‚Äúunselect‚Äù these columns.\n\nverb.normalized &lt;- left_join(verbs.corpus,\n                           corpus_sizes,\n                           by = \"Century\") |&gt;\n  mutate(normalized = (n / Words) * 1000000) |&gt;\n  mutate(normalized = round(normalized,\n                          digits = 2)) |&gt; \n  select(-c(n, Words))\n\nverb.normalized\n\n        AUX Century normalized\n1     echar      13       4.09\n2     echar      14       2.00\n3     echar      15       4.43\n4     tirar      15       0.09\n5   arrojar      16       0.40\n6     echar      16       3.07\n7   arrojar      17       1.23\n8  disparar      17       0.08\n9     echar      17       2.49\n10  arrojar      18       1.11\n11    echar      18       2.76\n12    tirar      18       0.55\n13  arrojar      19       0.89\n14 disparar      19       0.02\n15    echar      19      18.70\n16   lanzar      19       1.29\n17  arrojar      20       0.19\n18 disparar      20       0.02\n19    echar      20      27.96\n20   lanzar      20       2.13\n21    tirar      20       0.12\n22  arrojar      21       0.01\n23 disparar      21       0.00\n24    echar      21       2.91\n25   lanzar      21       2.14\n26    tirar      21       0.02\n\n\nWe reshape the data frame verb.normalized from long format to wide format by replicating the pivot_wider() function, which we used to create Table¬†11.2 and Table¬†11.4. The new column names will be taken from Century. The values in the new column will come from normalized. As earlier, we sort the rows of the data frame according to the alphabetical order of AUX using arrange(). We convert the output into a data frame format with the as.data.frame() command and assign the output to normalized.wide.\n\nnormalized.wide &lt;- verb.normalized |&gt;\n  pivot_wider(names_from = Century, values_from = normalized) |&gt;\n  arrange(AUX) |&gt; \n  as.data.frame()\n\nnormalized.wide\n\n       AUX   13 14   15   16   17   18    19    20   21\n1  arrojar   NA NA   NA 0.40 1.23 1.11  0.89  0.19 0.01\n2 disparar   NA NA   NA   NA 0.08   NA  0.02  0.02 0.00\n3    echar 4.09  2 4.43 3.07 2.49 2.76 18.70 27.96 2.91\n4   lanzar   NA NA   NA   NA   NA   NA  1.29  2.13 2.14\n5    tirar   NA NA 0.09   NA   NA 0.55    NA  0.12 0.02\n\n\nNext, we use the is.na() function to find all missing values (NA) in the data frame normalized.wide. We replace all these NA values with a dash (\"-\") using the &lt;- operator.\n\nnormalized.wide[is.na(normalized.wide)] &lt;- \"-\"\n\nThe result can be seen in Table¬†11.7.\n\nShow the R code to generate the wide table below.\n# Use left_join to merge the dataframes\nverb.normalized &lt;- left_join(verbs.corpus,\n                             corpus_sizes,\n                             by = \"Century\") |&gt;\n# Use mutate to create a new column with n divided by words\n  mutate(normalized = (n / Words) * 1000000) |&gt;\n  mutate(normalized = round(normalized,\n                            digits = 2)) |&gt; \n# Remove raw frequencies (n) and corpus sizes (Words)\n  select(-c(n, Words))\n\n# Pivot to wide format and replace NAs with 0\nnormalized.wide &lt;- verb.normalized |&gt;\n  pivot_wider(names_from = Century, values_from = normalized) |&gt;\n  arrange(AUX) |&gt; \n  as.data.frame()\n\n# replace NA with \"-\"\nnormalized.wide[is.na(normalized.wide)] &lt;- \"-\"\n\nnormalized.wide\n\n\n\n\nTable¬†11.7: Normalized frequency of Spanish ‚Äòthrow‚Äô verbs (pmw) based on TROLLing data\n\n\n\n       AUX   13 14   15   16   17   18   19    20   21\n1  arrojar    -  -    -  0.4 1.23 1.11 0.89  0.19 0.01\n2 disparar    -  -    -    - 0.08    - 0.02  0.02 0.00\n3    echar 4.09  2 4.43 3.07 2.49 2.76 18.7 27.96 2.91\n4   lanzar    -  -    -    -    -    - 1.29  2.13 2.14\n5    tirar    -  - 0.09    -    - 0.55    -  0.12 0.02\n\n\n\n\nAt this stage, it is important to note some differences between Table¬†11.7 and Table¬†11.5. Van Hulle & Enghels (2024a) provided normalized frequencies for the three verbs arrojar, disparar and tirar only from the 16th until the 21st centuries, with no data for the 13th to 15th centuries. However, Table¬†11.7 shows the normalized frequency of tirar at 0.09 for the 15th century and 0.55 for the 18th century, filling in some missing data found in the dataset (Table¬†11.3). Additionally, there are slight differences in the normalized frequencies of lanzar for the 19th and 20th centuries, calculated as 1.29 and 2.13 based on TROLLing data and displayed in Table¬†11.7, compared to 1.31 and 2.15 in reported by Van Hulle & Enghels (2024a) and displayed in Table¬†11.5.\nAnother point to note is the apparent discrepancy in the normalized frequency of the verb disparar. In Table¬†11.5, it is reported in the original paper as 0.0008 for the 21st century, while Table¬†11.7 displays it as 0.00. However, this difference is due to Table¬†11.7 using a two-digit format; when rounded to four digits, the value would indeed be 0.0008. Thus, this is not a true discrepancy.\n\n\n11.5.2 Visualisation of the normalized frequencies as a line graph\nWe now visualize how the usage of Spanish ‚Äòthrow‚Äô verbs in inchoative constructions has evolved from the 13th to the 21st century. Although such a visualization is not provided in Van Hulle & Enghels (2024a), it is mentioned in the dataset description Van Hulle & Enghels (2024b), and it can facilitate the interpretation of the changes in normalized frequencies documented in Table¬†11.7.\nFor a diachronic study based on corpus data, it is reasonable to choose a connected scatterplot, which is essentially a combination of a scatterplot and a line plot. Using the {ggplot2} package, this entails combining a geom_point() layer on top of a geom_line() layer. The connected scatterplot provides a visualisation that helps to identify the usage of the five ‚Äòthrow‚Äô verbs in inchoative constructions over time.\nFigure¬†11.2 is created using a ggplot() function that takes the data frame verb.normalized as its first argument and the aesthetics (aes) as its second argument. For the aes argument, we choose the Century column for the x-axis and the column normalized for the y-axis. Additionally, we specify two more optional aesthetics mappings in aes: ‚Äúcolor‚Äù and ‚Äúgroup‚Äù. Both will be mapped onto the AUX variable, meaning that each verb will be displayed in a different color, and the line will be grouped by each verb over time. We also add a scale_x_continuous() layer ensures that the x-axis is labelled from the 13th to 21st century.\n\nggplot(verb.normalized, \n       aes(x = Century, \n           y = normalized, \n           color = AUX, \n           group = AUX)) +\n  geom_point() +  # Scatterplot points\n  geom_line() +   # Connect points with lines\n  scale_x_continuous(breaks = 13:21) + \n  labs(title = \"Normalized frequency of Spanish 'throw' verbs over time\",\n       x = \"Century\",\n       y = \"Normalized frequency (pmw)\",\n       color = \"Verbs\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure¬†11.2: Normalized frequency of Spanish ‚Äòthrow‚Äô verbs over time\n\n\n\n\n\nFigure¬†11.2 shows that the verb echar is the most frequently used verb as an inchoative auxiliary, appearing in the corpus since the 13th century, while the other verbs only began to appear from the 15th century (tirar), the 16th century (arrojar), the 17th century (disparar), and the 19th century (lanzar). According to Van Hulle & Enghels (2024a: 223), the verb echar ‚Äúcan be considered as the exemplary verb which opened the pathway for other ‚Äòthrow‚Äô verbs towards the aspectual inchoative domain‚Äù. They further state, &gt; ‚ÄúThe relative token frequency increases remarkably in the 19th (n=18,70) and 20th (n=27,96) centuries, which can thus be defined as the time frames in which the micro-construction with echar was most frequently used. In the 21st century data, both micro-constructions appear with a comparable normalized token frequency in the corpus‚Äù (Van Hulle & Enghels (2024a)).\nThe normalized frequency graphic of Spanish ‚Äòthrow‚Äô verbs in Figure¬†11.2 effectively illustrates the authors‚Äô statement, providing a clear visual representation of how these verbs have evolved in usage over time.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#type-frequency",
    "href": "CS_Poppy.html#type-frequency",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.6 Type frequency",
    "text": "11.6 Type frequency\nType frequency refers to the number of unique words that can appear in a specific position, or ‚Äúslot,‚Äù within a particular grammatical construction. In the context of an inchoative construction, a specific slot refers to the position within the construction where an infinitive verb can occur.\nFor example, let‚Äôs look at the data in spanish.data (as shown in View(spanish.data) in the imported data, (see Figure¬†11.1). Here, we see a list of verb usages, with each row representing a token, or instance, of a verb in a sentence or construction. There are 15 rows, each representing a token of a verb in specific sentences.\nIf we focus on the verb lanzar, we can count a total of 7 tokens, meaning that lanzar appears 7 times in Figure¬†11.1 (in the 1st, 3rd, 5th, 6th, 7th, 8th, and 15th rows). However, among these tokens, lanzar pairs twice with hacer in an inchoative construction. Because hacer is repeated, this combination with lanzar is counted as only one type. Therefore, although we have 7 tokens (occurrences) of lanzar, we have only 6 unique types (distinct pairings) involving lanzar in the inchoative slot.\nVan Hulle & Enghels (2024a: 226) state that one may generally assume ‚Äúthat a higher type frequency indicates a higher degree of semantic productivity. As such, it is likely that a construction with a high number of different infinitives will accept even more types in the future‚Äù. Thus, type frequency is an important measure of how productive and adaptable a pattern is.\n\n11.6.1 Visualising type frequencies in a tabular format\nWe will now attempt to reproduce the type frequencies of Spanish ‚Äòthrow‚Äô verbs as displayed in the two subtables (both labelled ‚Äútype frequency‚Äù) of the original publication: one for echar and lanzar (Van Hulle & Enghels 2024a: Table 5) and the other for arrojar, disparar and tirar (Van Hulle & Enghels 2024a: Table 8). The values from these two subtables are reproduced in this chapter as Table¬†11.8.\n\n\n\n\nTable¬†11.8: Type frequency of Spanish ‚Äòthrow‚Äô verbs based on the published paper\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\narrojar\n-\n-\n-\n16\n34\n13\n32\n10\n27\n\n\ndisparar\n-\n-\n-\n-\n2\n-\n1\n1\n3\n\n\nechar\n8\n3\n15\n12\n12\n17\n19\n20\n20\n\n\nlanzar\n-\n-\n-\n-\n-\n-\n45\n95\n215\n\n\ntirar\n-\n-\n-\n-\n-\n-\n-\n7\n46\n\n\n\n\n\n\n\n\nBased on the object spanish.data (see Figure¬†11.1), which we created from on the TROLLing dataset Spanish_ThrowVerbs_Inchoatives_20230413.csv, we can calculate the type frequency of each ‚Äòthrow‚Äô verbs in inchoative construction (see Table¬†11.9). To achieve this, we first select the first three columns of spanish.data, i.e.¬†AUX, Century, INF. The result is a long table with the three columns and 2,882 rows. We check the first six lines of the table using the head() function.\n\ntype.token &lt;- select(spanish.data, 1:3)\nhead(type.token)\n\n     AUX Century      INF\n1 lanzar      21   llevar\n2  echar      21   dormir\n3 lanzar      21   probar\n4  tirar      21   atacar\n5 lanzar      21    hacer\n6 lanzar      21 estudiar\n\n\nWe then calculate the number of unique combinations among these variables using the distinct() function. We pipe the output into a group_by() function, which allows us to group all the corpus occurences according to Century and AUX. Then, using the summarize() function, we create a new column called Types with the number (n) of types corresponding to each combination of Century and AUX. We convert the output into a data frame format using as.data.frame() and assign it to a new R object called verb.types.\n\nverb.types &lt;- type.token |&gt; \n  distinct(Century, AUX, INF) |&gt; \n  group_by(Century, AUX) |&gt; \n  summarize(Types = n()) |&gt; \n  as.data.frame()\n\nWe reshape the object verb.types from long format to wide format using the pivot_wider() function. The new column names will be taken from Century. The values in the new column will come from Types. We use mutate(across(everything()) to modify all columns at once. The modification entails replacing all missing values (NA) with 0 using the replace_na function. Next, we sort the rows of the data frame in the alphabetical order of the AUX column using the arrange() function. We assign the output to types.wide.\n\ntypes.wide &lt;- verb.types |&gt;\n  pivot_wider(names_from = Century, values_from = Types) |&gt;\n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX)\n\nThe result is displayed as Table¬†11.9.\n\nShow the R code to generate the table below.\n# Selecting the first three columns of spanish.data\n# Creating a type frequency table labelled as type.token\ntype.token &lt;- select(spanish.data, 1:3)\n\n# Calculating distinct combinations of Century, AUX, and INF using the distinct() function\n# Grouping data by Century and AUX using the group_by() function\n# Creating a new column Types with the summarize() function,\n# Returning the count (n) for each group\n\nverb.types &lt;- type.token |&gt; \n  distinct(Century, AUX, INF) |&gt; \n  group_by(Century, AUX) |&gt; \n  summarize(Types = n())\n\n# Converting verb.types to a data frame using the as.data.frame() function\nverb.types &lt;- as.data.frame(verb.types)\n\n# Using the pivot_wider() function to create a contigency table\ntypes.wide &lt;- verb.types |&gt;\n  pivot_wider(names_from = Century, values_from = Types) |&gt;\n  mutate(across(everything(), ~ replace_na(., 0))) |&gt;\n  arrange(AUX)\n\n# Printing the table in elegantly formatted HTML format\ntypes.wide\n\n\n\n\nTable¬†11.9: Type frequency of Spanish ‚Äòthrow‚Äô verbs based on data\n\n\n\n# A tibble: 5 √ó 10\n  AUX       `13`  `14`  `15`  `16`  `17`  `18`  `19`  `20`  `21`\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 arrojar      0     0     0    16    34    13    32    10    27\n2 disparar     0     0     0     0     2     0     1     1     3\n3 echar        8     3    15    12    12    17    18    22    20\n4 lanzar       0     0     0     0     0     0    44    95   215\n5 tirar        0     0     2     0     0     7     0     7    46\n\n\n\n\nHere, too, we observe several discrepancies between Table¬†11.9 and Table¬†11.8. The discrepancies involve the type frequencies of echar for the 19th and 20th centuries, reported as 19 and 20 in the original paper, and of lanzar for the 19th century, originally reported as 45 (Table¬†11.8). Other discrepancies include the type frequencies of the verb tirar in the 15th and 18th centuries, which are two and seven according to the TROLLing data, but both reported as zero in the published study (see also Table¬†11.2).\n\n\n11.6.2 Visualisation of the type frequency as a line graph\nUsing the type frequency data that we calculated above (see Table¬†11.9), we can largely recycle the ggplot() code that we used to create Figure¬†11.2.\n\n\nShow the R code to generate the graph below.\n# Using the ggplot() function with the dataframe verb.types\n# The y-axis represents the type frequency\nggplot(verb.types, \n       aes(x = Century, \n           y = Types, \n           color = AUX, \n           group = AUX)) +\n  geom_point() +  # Scatterplot points\n  geom_line() +   # Connect points with lines\n  scale_x_continuous(breaks = 13:21) + \n  labs(title = \"Productivity of Spanish 'throw' verbs over time\",\n       x = \"Century\",\n       y = \"Type frequency\",\n       color = \"Verbs\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†11.3: Type frequency of Spanish ‚Äòthrow‚Äô verbs over time\n\n\n\n\n\nThe connected scatterplot displayed in Figure¬†11.3 provides a visualisation that helps identify the productivity of the five ‚Äòthrow‚Äô verbs in inchoative constructions with respect to their type frequency.\nAs Van Hulle & Enghels (2024a: 226) state:\n\n‚ÄúIn general, it is assumed that a higher type frequency indicates a higher degree of semantic productivity. As such, it is likely that a construction with a high number of different infinitives will accept even more types in the future. In this sense, type frequency constitutes an important parameter to measure the extending productivity of a construction‚Äù.\n\nHowever, we should interpret this graphic carefully, keeping in mind that absence of evidence is not evidence of absence. Notably, there is almost no data for disparar, which raises the question: in the real world, is this verb rarely used in an inchoative construction, or are there simply no examples in the corpus?\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ7. Which line of code can you add in the ggplot() code above to change the color scheme of the line graph in Figure¬†11.3 to a color-blind friendly one? Click on ‚ÄúShow the R code to generate the graph below.‚Äù to see the code for Figure¬†11.3.\n\n\n+ scale_color_viridis_b()\n\n\n+ scale_color_continuous()\n\n\n+ scale_color_viridis_d()\n\n\n+ scale_color_viridis_c()\n\n\n+ scale_color_blind()\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n Q8. Alternatively, we could opt for a black-and-white solution like below. How can we adapt the ggplot() code from Figure¬†11.3 to achieve this?\n\n\n\n\n\n\n\n\n\n\n\nChange `aes(color = AUX)` to `aes(linetype = AUX)`\n\n\nChange `aes(color = AUX)` to `aes(line = AUX)`\n\n\nChange `geom_line()` to `geom_line(linetype = AUX)`\n\n\nChange `geom_line()` to `geom_dotted()`",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#typetoken-ratio-ttr",
    "href": "CS_Poppy.html#typetoken-ratio-ttr",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.7 Type/token ratio (TTR)",
    "text": "11.7 Type/token ratio (TTR)\nAs stated by Van Hulle & Enghels (2024a: 226), the ‚Äútype/token ratio measures the realized productivity‚Äù of each verb. Furthermore,\n\nsince type frequency depends to some extent on token frequency (the more tokens, the more opportunities for different types to occur), the two must be put into some kind of relationship. The simplest measure suggested in the literature is the type/token ratio [‚Ä¶] (Stefanowitsch & Flach 2017: 118)\n\nType/token ratios (TTR) can range from zero and one. A TTR of zero indicates that there are no examples of the type in the given occurrences, while a TTR of one signifies that all types are unique to those given occurrences.\n\\[\nTTR = \\frac{types}{tokens}\n\\]\nAs type/token ratios depend on corpus size, Van Hulle & Enghels (2024a: 227) explain that:\n\n‚ÄúTo be representative, the measures of type/token and hapax/token ratio are calculated on a maximum of 500 tokens per auxiliary. Specifically, for echar in the 19th, 20th and 21st century and for lanzar in the 21st century, token frequency is reduced to 500.‚Äù\n\n\n11.7.1 Visualising type/token ratios in a tabular format\nWe will now attempt to reproduce the type/token ratio of Spanish throw verbs based on two subtables (both labelled ‚Äútype/token ratio‚Äù): one for echar and lanzar (Table 5 from Van Hulle & Enghels 2024a: 227) and the other for arrojar, disparar and tirar (Table 8 from Van Hulle & Enghels 2024a: 232), which are reproduced in this chapter as Table¬†11.10.\n\n\n\n\nTable¬†11.10: Type/token ratio of Spanish ‚Äòthrow‚Äô verbs over time based on Van Hulle & Enghels (2024a)\n\n\n\n\n\n\nAUX\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\narrojar\n-\n-\n-\n0.8\n0.72\n0.81\n0.84\n0.91\n0.96\n\n\ndisparar\n-\n-\n-\n-\n0.67\n-\n1\n1.00\n1.00\n\n\nechar\n0.25\n0.2\n0.15\n0.08\n0.13\n0.43\n0.04\n0.04\n0.04\n\n\nlanzar\n-\n-\n-\n-\n-\n-\n0.8\n0.75\n0.43\n\n\ntirar\n-\n-\n-\n-\n-\n-\n-\n1.00\n0.57\n\n\n\n\n\n\n\n\nTo calculate type/token ratios, we first create two matching wide tables, one for the token frequencies, and another for the type frequencies. We can use the wide table labelled token.data from Table¬†11.3, and the wide table labelled types.wide from Table¬†11.9 as they are ordered in exactly the same way (you can check this by comparing their structures using the str() function).\nFirst, we create a new data frame using the data.frame() function. This data frame will take its first column from token.data, which contains the auxiliary verbs (AUX). We access this column using token.data[, 1].4\nNext, we calculate the type/token ratio. This is done by dividing the numeric values in types.wide (i.e., all columns except the first) by the corresponding values in token.data. To this end, we use the notation types.wide[, -1] / token.data[, -1]. The [, -1] indicates that we take all columns except the first one. We exclude the first column because it contains non-numeric values (the AUX column).\nFinally, we combine these components into our new data frame. We include the AUX column as the first column by selecting it with the command token.data[, 1]. To ensure that the column names remain unchanged, we set check.names = FALSE in the data.frame() function. This prevents R from altering the original column names, keeping them exactly as they are in token.data. We also round() the values of all numeric columns to just two decimals.\n\ndata.frame(token.data[, 1], \n           types.wide[, -1] / token.data[, -1],\n           check.names = FALSE) |&gt; \n    mutate(across(where(is.numeric), round, digits = 2))\n\n       AUX   13  14   15   16   17   18   19   20   21\n1  arrojar  NaN NaN  NaN 0.80 0.72 0.81 0.84 0.91 0.96\n2 disparar  NaN NaN  NaN  NaN 0.67  NaN 1.00 1.00 1.00\n3    echar 0.25 0.2 0.15 0.08 0.13 0.42 0.04 0.04 0.04\n4   lanzar  NaN NaN  NaN  NaN  NaN  NaN 0.80 0.76 0.43\n5    tirar  NaN NaN 1.00  NaN  NaN 0.88  NaN 1.00 0.57\n\n\nOur table contains a lot NaN values. In these cells of the table, the number of tokens was zero and, as a consequence, the number of types was also zero. As it is mathematically impossible to divide zero by zero, R returns NaN values instead. To replace these NaN values to dashes (\"-\") to match the formatting of the published tables, we use the base R function is.na().\n\ntype.token1 &lt;- data.frame(token.data[, 1], \n\n                          types.wide[, -1] / token.data[, -1],\n\n                          check.names = FALSE) |&gt; \n    mutate(across(where(is.numeric), round, digits = 2))\n\ntype.token1[is.na(type.token1)] &lt;- \"-\"\n\nThe result is saved as type.token1 and is displayed below as Table¬†11.11.\n\n\n\nTable¬†11.11: Type/token ratio of Spanish ‚Äòthrow‚Äô verbs over time based on data\n\n\n\n       AUX   13  14   15   16   17   18   19   20   21\n1  arrojar    -   -    -  0.8 0.72 0.81 0.84 0.91 0.96\n2 disparar    -   -    -    - 0.67    -    1 1.00 1.00\n3    echar 0.25 0.2 0.15 0.08 0.13 0.42 0.04 0.04 0.04\n4   lanzar    -   -    -    -    -    -  0.8 0.76 0.43\n5    tirar    -   -    1    -    - 0.88    - 1.00 0.57\n\n\n\n\nComparing Table¬†11.10 and Table¬†11.11, we find some minor discrepancies between the type/token ratios presented in the published paper and those calculated on the basis of the TROLLing data. The type/token ratio of the verb tirar in the 15th and 18th centuries, are reported as 0 and 0 in the published paper (see also Table¬†11.10), but as 1 and 0.88 in Table¬†11.11. These differences correspond to the discrepancies already identified when calculating the token frequencies (see Section 11.4.1).\nThe other (very minor) discrepancies involve the type/token ratio of lanzar for the 20th century, reported as 0.75 in Table¬†11.10 but as 0.76 in Table¬†11.11 and echar in the 18th century, reported as 0.43 (see Table¬†11.10), while Table¬†11.11 displays it as 0.42. These differences arise from the fact that Van Hulle & Enghels (2024a) presumably did not use R for their calculations. The type/token ratio of echar in the 18th century is actually 0.4250, which is rounded as 0.43 by Van Hulle & Enghels (2024a), but as 0.42 by R (see Table¬†11.11). This somewhat confusing rounding behaviour is explained in the help file of the round() function:\n\n‚ÄúNote that for rounding off a 5, the IEC 60559 standard (see also ‚ÄòIEEE 754‚Äô) is expected to be used, ‚Äògo to the even digit‚Äô. Therefore round(0.5) is 0 and round(-1.5) is -2. However, this is dependent on OS services and on representation error (since e.g.¬†0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2).‚Äù\n\n\n\n11.7.2 Visualising the type/token ratios as a line graph\nFor the visualisation of the type/token ratios (Figure¬†11.4), we create a new data frame. We start by merging the data frames verbs.investigated and verb.types using the left_join() function, ensuring that the Century and AUX variables are aligned. This results in a single data frame, which we save as type_token.\n\ntype_token &lt;- left_join(verbs.investigated, verb.types, \n                        by = c(\"Century\", \"AUX\"))\n\nNext, we calculate the type/token ratios by adding a new column, TypeTokenRatio, to the type_token data frame using the mutate() function, which applies the type/token ratio formula to each row. Finally, we use the arrange() function to sort the data by Century and AUX organizing the results chronologically by Century and alphabetically by the verb type (AUX).\n\ntype.token.ratio &lt;- type_token  |&gt; \n  mutate(TypeTokenRatio = Types / n) |&gt; \n  arrange(Century, AUX)\n\nThe output is a table with 26 rows and five columns: AUX, Century, n, and Types and TypeTokenRatio. We can display the first six rows of the table using the head() function.\n\nhead(type.token.ratio)\n\n      AUX Century   n Types TypeTokenRatio\n1   echar      13  32     8     0.25000000\n2   echar      14  15     3     0.20000000\n3   echar      15 101    15     0.14851485\n4   tirar      15   2     2     1.00000000\n5 arrojar      16  20    16     0.80000000\n6   echar      16 153    12     0.07843137\n\n\nNow we can use the type.token.ratio data frame with the same ggplot code that we used to create Figure¬†11.2 and Figure¬†11.3, allowing us to visualize the type/token ratios of Spanish ‚Äòthrow‚Äô verbs over time as Figure¬†11.4.\n\n\nShow the R code to generate the table below.\nggplot(type.token.ratio, \n       aes(x = Century, \n           y = TypeTokenRatio, \n           color = AUX, \n           group = AUX)) +\n  geom_point() +  # Scatterplot points\n  geom_line() +   # Connect points with lines\n  scale_x_continuous(breaks = 13:21) + \n  labs(title = \"The productivity of Spanish 'throw' verbs over time\",\n       x = \"Century\",\n       y = \"type/token ratio\",\n       color = \"Verbs\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure¬†11.4: type/token ratio of Spanish ‚Äòthrow‚Äô Verbs Over Time\n\n\n\n\n\nAccording to Van Hulle & Enghels (2024a), the verb lanzar is considered the ‚Äúmost productive auxiliary‚Äù due to its high type/token ratio values, despite only appearing from the 19th century onward, and because it ‚Äúwas able to incorporate a more varied set of infinitives‚Äù (Van Hulle & Enghels 2024a: 228). In contrast, the type/token ratio for echar is comparably low. However, as Van Hulle & Enghels (2024a: 228) state:\n\n‚Äú[‚Ä¶] the type/token ratio for the micro-construction with echar is quite stable until it considerably drops from the 19th century on (n=0.04). This means that, although speakers used the construction more frequently, this was mainly done with a limited group of infinitives [‚Ä¶]‚Äù\n\nAdditionally, the verbs disparar and tirar have a type/token ratio of one for the 19th and 21st centuries, and the 15th and 20th centuries, respectively. This is due to the high hapax value, i.e., ‚Äúthe number of types that appear only once in a text or corpus‚Äù for the respective verbs (Van Hulle & Enghels 2024a: 226). In the cases of disparar and tirar, each hapax refers to only one occurrence.\nThe above plot (Figure¬†11.4) shows more clearly that arrojar, not lanzar, is actually the most semantically productive verb. When compared with echar, as the authors of the published paper have done, lanzar does indeed appear more semantically productive. However, as Van Hulle & Enghels (2024a) note, ‚Äútype/token ratio measures the realized productivity.‚Äù Based on this measure, arrojar is even more productive than lanzar, as this graphic (Figure¬†11.4) clearly illustrates. Van Hulle & Enghels (2024a) do not provide such a visualization, but this chapter has shown that it can aid in interpreting the realized productivity measured by the type/token ratio.\n\n\n\n\n\n\nQuiz time!\n\n\n\nQ9. What issue arises when interpreting the productivity of Spanish ‚Äòthrow‚Äô verbs over time on the basis of Figure¬†11.4?\n\n\n\n\nSome lines suggest a linear increase or decrease in productivity over several centuries when there were, in fact, zero occurrences of that verb in one of these centuries.\n\n\nThe graph incorrectly suggests that type/token ratios cannot go beyond 1.00.\n\n\nThe graph makes it difficult to compare the productivity of verbs across each century.\n\n\nThe graph incorrectly suggests that there were zero occurrences of some verbs in earlier centuries.\n\n\nThe graph incorrectly suggests a decrease in productivity for all verbs.\n\n\n\n\n\n\n\nüê≠ Click on the mouse for a hint.\n\n\n\n\n Q10. Based on the type/token ratios displayed in Figure¬†11.4, which ‚Äòthrow‚Äô verb appears to be the least productive one?\n\n\n\n\nlanzar\n\n\ntirar\n\n\ndisparar\n\n\nechar\n\n\narrojar\n\n\n\n\n\n\n\n Q11. Based on the type/token ratios displayed in Figure¬†11.4, which ‚Äòthrow‚Äô verb appears to be the most productive one since the 19th century?\n\n\n\n\narrojar\n\n\ntirar\n\n\nechar\n\n\nlanzar\n\n\ndisparar",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#conclusion",
    "href": "CS_Poppy.html#conclusion",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "11.8 Conclusion",
    "text": "11.8 Conclusion\nYou have successfully completed 0 out of 11 quiz questions in this chapter.\nThis chapter attempted to reproduce the results of a corpus linguistics study that explores the evolution of five throw verbs in Peninsular Spanish (echar, lanzar, disparar, tirar, and arrojar) into aspectual auxiliaries in inchoative constructions that express the beginning of an event. The authors of the original study, Van Hulle & Enghels (2024a), used historical and contemporary data to analyse the development and usage of these verbs, making their research data openly accessible. As part of this chapter, we identified some discrepancies between the results we obtained on the basis of the authors‚Äô data (Van Hulle & Enghels 2024b) and those published in the 2024 study, indicating that the version of the dataset uploaded onto TROLLing does not exactly match the one used for the published results.\nWe have also created some new data visualizations based on the authors‚Äô uploaded data, which uncover patterns in the evolution of Spanish inchoative constructions that might not be immediately apparent through the examination of the tabular results alone. These visualizations underscore the effectiveness of graphical representation as a tool for understanding linguistic shifts over time‚Äîan approach not employed in the original study.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#references",
    "href": "CS_Poppy.html#references",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "References",
    "text": "References\n[1] S. T. Gries and N. C. Ellis. ‚ÄúStatistical Measures for Usage-Based Linguistics‚Äù. In: Language Learning 65.S1 (2015). _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12119, p.¬†228‚Äì255. ISSN: 1467-9922. DOI: 10.1111/lang.12119. https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12119.\n[2] K. Pfadenhauer and E. Wiesinger, ed.¬†Romance motion verbs in language change: Grammar, lexicon, discourse. De Gruyter, Jul.¬†2024. ISBN: 978-3-11-124814-1. DOI: 10.1515/9783111248141. https://www.degruyter.com/document/doi/10.1515/9783111248141/html.\n[3] A. Stefanowitsch and S. Flach. ‚ÄúThe corpus-based perspective on entrenchment‚Äù. In: Entrenchment and the psychology of language learning: How we reorganize and adapt linguistic knowledge. Ed. by H. Schmid. De Gruyter, 2017, p.¬†101‚Äì127. ISBN: 978-3-11-034130-0 978-3-11-034142-3. DOI: 10.1037/15969-006. https://content.apa.org/books/15969-006.\n[4] S. Van Hulle and R. Enghels. Replication Data for: ‚ÄúThe category of throw verbs as productive source of the Spanish inchoative construction. DataverseNO, V1.‚Äù. 2024. DOI: 10.18710/TR2PWJ. https://dataverse.no/dataset.xhtml?persistentId=doi:10.18710/TR2PWJ.\n[5] S. Van Hulle and R. Enghels. ‚ÄúThe category of throw verbs as productive source of the Spanish inchoative construction‚Äù. In: Romance motion verbs in language change. Ed. by K. Pfadenhauer and E. Wiesinger. De Gruyter, Jul.¬†2024, p.¬†213‚Äì240. ISBN: 978-3-11-124814-1. DOI: 10.1515/9783111248141-009. https://www.degruyter.com/document/doi/10.1515/9783111248141-009/html.\n\nPackages used in this chapter\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Brussels\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] knitcitations_1.0.12 xfun_0.45            lubridate_1.9.3     \n [4] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4         \n [7] purrr_1.0.2          readr_2.1.5          tidyr_1.3.1         \n[10] tibble_3.2.1         ggplot2_3.5.1        tidyverse_2.0.0     \n[13] here_1.0.1           kableExtra_1.4.0     checkdown_0.0.12    \n[16] webexercises_1.1.0  \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4        generics_0.1.3    xml2_1.3.6        stringi_1.8.4    \n [5] hms_1.1.3         digest_0.6.36     magrittr_2.0.3    timechange_0.3.0 \n [9] evaluate_0.24.0   grid_4.4.1        fastmap_1.2.0     plyr_1.8.9       \n[13] rprojroot_2.0.4   jsonlite_1.8.8    backports_1.5.0   httr_1.4.7       \n[17] fansi_1.0.6       viridisLite_0.4.2 scales_1.3.0      bibtex_0.5.1     \n[21] codetools_0.2-20  cli_3.6.3         rlang_1.1.4       munsell_0.5.1    \n[25] commonmark_1.9.1  withr_3.0.1       yaml_2.3.8        tools_4.4.1      \n[29] tzdb_0.4.0        colorspace_2.1-0  vctrs_0.6.5       R6_2.5.1         \n[33] lifecycle_1.0.4   RefManageR_1.4.0  htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[37] pillar_1.9.0      gtable_0.3.5      Rcpp_1.0.12       glue_1.7.0       \n[41] systemfonts_1.1.0 tidyselect_1.2.1  rstudioapi_0.16.0 knitr_1.47       \n[45] farver_2.1.2      htmltools_0.5.8.1 labeling_0.4.3    rmarkdown_2.27   \n[49] svglite_2.1.3     compiler_4.4.1    markdown_1.13    \n\n\n\n\nPackage references\n[1] G. Grolemund and H. Wickham. ‚ÄúDates and Times Made Easy with lubridate‚Äù. In: Journal of Statistical Software 40.3 (2011), pp. 1-25. https://www.jstatsoft.org/v40/i03/.\n[2] G. Moroz. checkdown: Check-Fields and Check-Boxes for rmarkdown. R package version 0.0.12. 2023. https://agricolamz.github.io/checkdown/.\n[3] G. Moroz. Create check-fields and check-boxes with checkdown. 2020. https://CRAN.R-project.org/package=checkdown.\n[4] K. M√ºller. here: A Simpler Way to Find Your Files. R package version 1.0.1. 2020. https://here.r-lib.org/.\n[5] K. M√ºller and H. Wickham. tibble: Simple Data Frames. R package version 3.2.1. 2023. https://tibble.tidyverse.org/.\n[6] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria, 2024. https://www.R-project.org/.\n[7] V. Spinu, G. Grolemund, and H. Wickham. lubridate: Make Dealing with Dates a Little Easier. R package version 1.9.3. 2023. https://lubridate.tidyverse.org.\n[8] H. Wickham. forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0. 2023. https://forcats.tidyverse.org/.\n[9] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. ISBN: 978-3-319-24277-4. https://ggplot2.tidyverse.org.\n[10] H. Wickham. stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.1. 2023. https://stringr.tidyverse.org.\n[11] H. Wickham. tidyverse: Easily Install and Load the Tidyverse. R package version 2.0.0. 2023. https://tidyverse.tidyverse.org.\n[12] H. Wickham, M. Averick, J. Bryan, et al.¬†‚ÄúWelcome to the tidyverse‚Äù. In: Journal of Open Source Software 4.43 (2019), p.¬†1686. DOI: 10.21105/joss.01686.\n[13] H. Wickham, W. Chang, L. Henry, et al.¬†ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.5.1. 2024. https://ggplot2.tidyverse.org.\n[14] H. Wickham, R. Fran√ßois, L. Henry, et al.¬†dplyr: A Grammar of Data Manipulation. R package version 1.1.4. 2023. https://dplyr.tidyverse.org.\n[15] H. Wickham and L. Henry. purrr: Functional Programming Tools. R package version 1.0.2. 2023. https://purrr.tidyverse.org/.\n[16] H. Wickham, J. Hester, and J. Bryan. readr: Read Rectangular Text Data. R package version 2.1.5. 2024. https://readr.tidyverse.org.\n[17] H. Wickham, D. Vaughan, and M. Girlich. tidyr: Tidy Messy Data. R package version 1.3.1. 2024. https://tidyr.tidyverse.org.\n[18] Y. Xie. Dynamic Documents with R and knitr. 2nd. ISBN 978-1498716963. Boca Raton, Florida: Chapman and Hall/CRC, 2015. https://yihui.org/knitr/.\n[19] Y. Xie. ‚Äúknitr: A Comprehensive Tool for Reproducible Research in R‚Äù. In: Implementing Reproducible Computational Research. Ed. by V. Stodden, F. Leisch and R. D. Peng. ISBN 978-1466561595. Chapman and Hall/CRC, 2014.\n[20] Y. Xie. knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.47. 2024. https://yihui.org/knitr/.\n[21] Y. Xie. xfun: Supporting Functions for Packages Maintained by Yihui Xie. R package version 0.45. 2024. https://github.com/yihui/xfun.\n[22] H. Zhu. kableExtra: Construct Complex Table with kable and Pipe Syntax. R package version 1.4.0. 2024. http://haozhu233.github.io/kableExtra/.\n\n\n\n\nGries, Stefan Th. & Nick C. Ellis. 2015. Statistical measures for usage-based linguistics. Language Learning 65(S1). 228‚Äì255. https://doi.org/10.1111/lang.12119.\n\n\nPfadenhauer, Katrin & Evelyn Wiesinger (eds.). 2024. Romance motion verbs in language change: Grammar, lexicon, discourse. De Gruyter. https://doi.org/10.1515/9783111248141.\n\n\nStefanowitsch, Anatol & Susanne Flach. 2017. The corpus-based perspective on entrenchment. In Hans-J√∂rg Schmid (ed.), Entrenchment and the psychology of language learning: How we reorganize and adapt linguistic knowledge, 101‚Äì127. De Gruyter. https://doi.org/10.1037/15969-006.\n\n\nVan Hulle, Sven & Renata Enghels. 2024a. The category of throw verbs as productive source of the spanish inchoative construction. In Katrin Pfadenhauer & Evelyn Wiesinger (eds.), Romance motion verbs in language change, 213‚Äì240. De Gruyter. https://doi.org/10.1515/9783111248141-009.\n\n\nVan Hulle, Sven & Renata Enghels. 2024b. TROLLing replication data for: ‚ÄúThe category of throw verbs as productive source of the spanish inchoative construction. DataverseNO, V1.‚Äù https://doi.org/10.18710/TR2PWJ.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "CS_Poppy.html#footnotes",
    "href": "CS_Poppy.html#footnotes",
    "title": "11¬† ‚ÄòThrow‚Äô verbs in Spanish: RepRoducing the results of a corpus linguistics study",
    "section": "",
    "text": "We contacted the first and corresponding author of the paper. They responded and confirmed that these discrepancies were likely due to small changes that were made to the dataset that was ultimately used in the analyses published in Van Hulle & Enghels (2024a). These changes were deemed necessary when either additional occurrences of inchoative constructions were found in the corpora, or false positives (i.e.¬†occurrences of ‚Äòthrow‚Äô verbs that did not enter such constructions) were later identified in the dataset. The author did not provide us with the final dataset that was used in the reported analyses.‚Ü©Ô∏é\nThe normalized frequencies of echar and lanzar are found in Table 5 (Van Hulle & Enghels 2024a: 227), whilst those for arrojar, disparar and tirar are displayed in Table 8 (Van Hulle & Enghels 2024a: 232). Note that in Tables 5 and 8 (Van Hulle & Enghels 2024a: 232) all values are rounded off to two decimal places except the normalized frequency of disparar in the 21st which is reported as ‚Äú0.0008‚Äù.‚Ü©Ô∏é\nNote that we cannot copy the word counts directly from the paper, as the authors use the continental European format with the dot (.) as the thousand-separator and the comma (,) as a decimal point (e.g., 7.829.566 for the 13th century). In R, however, the dot is interpreted as a decimal separator so entering 7.829.566 will generate an error:\n\n7.829.566\n\nunexpected numeric constant in \"7.829.566\"\n‚Ü©Ô∏é\nRemember that, in base R, the notation [x, y] allows us to specify rows and columns in a data frame, where x refers to the row and y refers to the column (see Section 7.3). For example, token.data[, 1] means we are selecting all rows from the first column of token.data.‚Ü©Ô∏é",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>'Throw' verbs in Spanish: Rep`R`oducing the results of a corpus linguistics study</span>"
    ]
  },
  {
    "objectID": "A_FurtherResources.html",
    "href": "A_FurtherResources.html",
    "title": "Appendix A ‚Äî Next-step resources",
    "section": "",
    "text": "A.1 Recommended resources specific to the language sciences (in alphabetical order)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Next-step resources</span>"
    ]
  },
  {
    "objectID": "A_FurtherResources.html#recommended-resources-specific-to-the-language-sciences-in-alphabetical-order",
    "href": "A_FurtherResources.html#recommended-resources-specific-to-the-language-sciences-in-alphabetical-order",
    "title": "Appendix A ‚Äî Next-step resources",
    "section": "",
    "text": "Brezina, Vaclav. 2018. Statistics in Corpus Linguistics: A Practical Guide. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781316410899.\nDesagulier, Guillaume. 2017. Corpus Linguistics and Statistics with R: Introduction to Quantitative Methods in Linguistics (Quantitative Methods in the Humanities and Social Sciences). Cham: Springer International Publishing.\nGries, Stefan Thomas. 2021. Statistics for linguistics with R: a practical introduction (De Gruyter Mouton Textbook). 3rd revised edition. Berlin Boston: de Gruyter Mouton.\nLADAL contributors. Tutorials of the Language Technology and Data Analysis Laboratory. https://ladal.edu.au/tutorials.html Open Educational Resource.\nLevshina, Natalia. 2015. How to do linguistics with R: Data exploration and statistical analysis. Amsterdam: John Benjamins.\nSchneider, Dr Gerold & Max Lauber. 2020. Statistics for Linguists. https://dlf.uzh.ch/openbooks/statisticsforlinguists/ Open Educational Resource.\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using R. New York: Routledge. https://doi.org/10.4324/9781315165547.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Next-step resources</span>"
    ]
  },
  {
    "objectID": "A_FurtherResources.html#further-open-educational-resources-currently-in-no-particular-order",
    "href": "A_FurtherResources.html#further-open-educational-resources-currently-in-no-particular-order",
    "title": "Appendix A ‚Äî Next-step resources",
    "section": "A.2 Further Open Educational Resources (currently in no particular order)",
    "text": "A.2 Further Open Educational Resources (currently in no particular order)\n\nMine √áetinkaya-Rundel and Johanna Hardin: Introduction to Modern Statistics (2024), Second Edition. https://openintro-ims.netlify.app/\nGuide to Effect Sizes and Confidence Intervals: https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals/\nHappy Git and GitHub for the useR: https://happygitwithr.com/\nQuarto & reproducibility: https://ucsbcarpentry.github.io/Reproducible-Publications-with-RStudio-Quarto/index.html\nModern Data Visualization with R: https://rkabacoff.github.io/datavis\nBuilding reproducible analytical pipelines with R: https://raps-with-r.dev/\nModern Plain Text Computing: https://mptc.io/content/01-content.html\nhttps://www.data-to-viz.com/\nInterpreting data visualisation: https://pressbooks.library.torontomu.ca/criticaldataliteracy/\nImprove your statistical inferences: https://lakens.github.io/statistical_inferences/\nWhat they forgot to teach you about R: https://rstats.wtf/\nIntroduction to Data Science: https://florian-huber.github.io/data_science_course/book/cover.html\nData Science in Education Using R: https://datascienceineducation.com/\nModels Demystified: A Practical Guide from t-tests to Deep Learning https://m-clark.github.io/book-of-models/\nData Visualization in R https://datavizf23.classes.andrewheiss.com/\nR for Data Science https://r4ds.hadley.nz/intro\nDauber, Daniel. 2024. R for Non-Programmers: A Guide for Social Scientists. https://bookdown.org/daniel_dauber_io/r4np_book/.\nBayes Rules! An Introduction to Applied Bayesian Modeling https://www.bayesrulesbook.com/\nFundamentals of Data Visualization by Claus O. Wilke https://clauswilke.com/dataviz/\nLearning statistics with R https://learningstatisticswithr.com\nQuarto for scientists https://qmd4sci.njtierney.com/\nThe Version Control Book: Track, organize and share your work: An introduction to Git for research https://lennartwittkuhn.com/version-control-book/\nIntermediate Statistics with R https://greenwood-stat.github.io/GreenwoodBookHTML/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Next-step resources</span>"
    ]
  }
]